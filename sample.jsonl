{"text": "Tell us about your PDF experience."}
{"text": "Operator Nexus\nOperator Nexus runs network-intensive workloads under stringent security, performance and manageability. Learn Operator Nexus with these quickstarts and how-to guides.\nOperator Nexus overview\nOVERVIEW"}
{"text": "What is Operator Nexus?\nCONCEPT"}
{"text": "Resource types\nCompute\nStorage\nNexus Kubernetes overview\nObservability\nFrequently asked questions\nQuickStart guides\nQUICKSTART"}
{"text": "Before you start NFC/CM deployment\nBefore you start platform deployment\nQUICKSTART"}
{"text": "Deploy Nexus Kubernetes cluster\nDeploy Nexus virtual machine\nObservability\nCONCEPT"}
{"text": "Observability concepts\nHOW-TO GUIDE"}
{"text": "Monitor Nexus Kubernetes cluster\nMonitor VMs for VNF\nCluster metrics configuration management\nReferences\nREFERENCE"}
{"text": "List of metrics collected\nNear-edge Compute\nNear-edge storage\nLimits & quotas"}
{"text": "What is Azure Operator Nexus?\nArticle  03/14/2023\nAzure Operator Nexus is a carrier-grade, next-generation hybrid cloud platform for telecommunication operators. Operator Nexus is purpose-built for operators' network- intensive workloads and mission-critical applications. Operator Nexus supports both Microsoft and a wide variety of partner virtualized and containerized network functions. The platform automates lifecycle management of the infrastructure, including: network fabric, bare metal hosts, and storage appliances, as well as tenant workloads for Container Network Functions and Virtualized Network Functions. Operator Nexus meets operators' security, resiliency, observability, and performance requirements to achieve meaningful business results. The platform seamlessly integrates compute, network, and storage. Operator Nexus is self service and uses the Azure portal, CLI, SDKs, and other tools to interact with the platform.\nFigure: Operator Nexus Overview"}
{"text": "Key benefits\nOperator Nexus includes the following benefits for operating secure carrier-grade network functions at scale:\nReduced operational complexity and costs   Operators have the ability to manage their Operator Nexus infrastructure and tenants from Azure. Automation can be built to streamline deployment, allowing for operators to have faster time to market and innovate to provide value add services to their customers. Integrated platform for compute, network, and storage   Operators no longer need to provision compute, network, and storage separately as Operator Nexus\nprovides an end-to-end (E2E) platform from the infrastructure to the tenant for applications. For example, the networks associated to the compute infrastructure can automatically be provisioned across the compute and network infrastructure without requiring additional teams. Expanding Network Function (NF) ecosystem   Operator Nexus supports a wide variety of Microsoft's own NFs and partners NFs via the Operator Nexus Ready program. These NFs are tested for deployment and lifecycle management on Operator Nexus before they're made available in Azure Marketplace. Access to key Azure services   Operator Nexus being connected to Azure, operators can seamlessly access most Azure services through the same connection as the on-premises network. Operators can monitor logs and metrics via Azure Monitor, and analyze telemetry data using Log Analytics or Azure AI/Machine Learning framework. Unified governance and compliance   Operator Nexus extends Azure management and services to operator's premises. Operators can unify data governance and enforce security and compliance policies by  Azure Role based Access Control  and  Azure Policy ."}
{"text": "How Operator Nexus works\nOperator Nexus utilizes a curated and certified hardware Bill of Materials (BOM). It is composed of commercially available off-the-shelf servers, network switches, and storage arrays. The infrastructure is deployed in operator's on-premises data center. Operators or System Integrators must make sure they  meet the prerequisites and follow the guidance .\nThe service that manages the Operator Nexus infrastructure is hosted in Azure. Operators can choose an Azure region that supports Operator Nexus for any on- premises Operator Nexus instance. The diagram illustrates the architecture of the Operator Nexus service.\nFigure: How Operator Nexus works\n1. The management layer of Operator Nexus is built on Azure Resource Manager (ARM), that provides consistent user experience in the Azure portal and Azure APIs 2. Azure Resource Providers provide modeling and lifecycle management of  Operator Nexus resources  such as bare metal machines, clusters, network devices, etc. 3. Operator Nexus controllers: Cluster Manager and Network Fabric Controller, are deployed in a managed Virtual Network (VNet) connected to operator's on- premises network. The controllers enable functionalities such as infrastructure bootstrapping, configurations, service upgrades etc. 4. Operator Nexus is integrated with many Azure services such as Azure Monitor, Azure Container Registry, and Azure Kubernetes Services. 5. ExpressRoute is a network connectivity service that bridges Azure regions and operators' locations."}
{"text": "Key features\nHere are some of the key features of Operator Nexus."}
{"text": "CBL-Mariner\nOperator Nexus runs Microsoft's own Linux distribution "CBL-Mariner" on the bare metal hosts in the operator's facilities. The same Linux distribution supports Azure cloud infrastructure and edge services. It includes a small set of core packages by default.  CBL- Mariner  is a lightweight OS and consumes limited system resources and is engineered to be efficient. For example, it has a fast boot time with a small footprint with locked-\ndown packages, resulting in the reduction of the threat landscape. On identifying a security vulnerability, Microsoft makes the latest security patches and fixes available with the goal of fast turn-around time. Running the infrastructure on Linux aligns with Network Function needs, telecommunication industry trends, and relevant open-source communications. Operator Nexus supports both virtualized network functions (VNFs) and containerized network functions (CNFs)."}
{"text": "Bare metal and cluster management\nOperator Nexus includes capabilities to manage the bare metal hosts in operators' premises. Operators can provision the bare metal hosts using Operator Nexus and can interact to restart, shutdown, or re-image, for example. One important component of the service is Cluster Manager.  Cluster Manager  provides the lifecycle management of Kubernetes clusters that are made of the bare metal hosts."}
{"text": "Network Fabric Automation\nOperator Nexus includes  Network Fabric Automation (NFA)  which enables operators to build, operate and manage carrier grade network fabrics. The reliable and distributed cloud services model supports the operators' telco network functions. Operators have the ability to interact with Operator Nexus to provision the network fabric via Zero- Touch Provisioning (ZTP), as well as perform complex network implementations via a workflow driven, API model."}
{"text": "Network Packet Broker\nNetwork Packet Broker (NPB) is an integral part of the network fabric in Operator Nexus. NPB enables multiple scenarios from network performance monitoring to security intrusion detection. Operators can monitor every single packet in Operator Nexus and replicate it. They can apply packet filters dynamically and send filtered packets to multiple destinations for further processing."}
{"text": "Azure Hybrid Kubernetes Service\nAzure Kubernetes Service (AKS) is a managed Kubernetes service on Azure. It lets users focus on developing and deploying their workloads while letting Azure handle the management and operational overheads. In  AKS-Hybrid  the Kubernetes cluster is deployed on-premises. Azure still performs the traditional operational management activities such as updates, certificate rotations, etc."}
{"text": "Network functions virtualization infrastructure"}
{"text": "capabilities\nAs a platform, Operator Nexus is designed for telco network functions and optimized for carrier-grade performance and resiliency. It has many built-in Network Functions Virtualization Infrastructure (NFVI) capabilities:\nCompute: NUMA aligned VMs with dedicated cores (both SMT siblings), backed by huge pages ensures consistent performance. There's no impact from other workloads running on the same hypervisor host. Networking: SR-IOV & DPDK for low latency and high throughput. Highly available VFs to VMs with redundant physical paths provide links to all workloads. APIs are used to control access and trunk port consumption in both VNFs and CNFs. Storage: Filesystem storage for CNFs backed by high performance storage arrays"}
{"text": "Azure Operator Service Manager\nAzure Operator Service Manager is a service that allows Network Equipment Providers (NEP) to publish their NFs in Azure Marketplace. Operators can deploy them using familiar Azure APIs. Operator Service Manager provides a framework for NEPs and Microsoft to test and validate the basic functionality of the NFs. The validation includes lifecycle management of an NF on Operator Nexus."}
{"text": "Observability\nOperator Nexus automatically streams the metrics and logs from the operator's premises to Azure Monitor and Log Analytics workspace of:\nInfrastructure (compute, network and storage) Tenant Infrastructure (ex. VNF VMs).\nLog Analytics has a rich analytical tool-set that operators can use for troubleshooting or correlating for operational insights. And, Azure Monitor lets operators specify alerts."}
{"text": "Next steps\nLearn more about Operator Nexus  resource models Review  Operator Nexus deployment prerequisites and steps"}
{"text": "Azure Operator Nexus resource types\nArticle  07/12/2023\nThis article introduces you to the Operator Nexus components represented as Azure resources in Azure Resource Manager.\nFigure: Resource model"}
{"text": "Platform components\nThe Operator Nexus Cluster (or Instance) platform components include the infrastructure and the platform components used to manage these infrastructure resources."}
{"text": "Network Fabric Controller\nThe Network Fabric Controller (NFC) is a resource that automates the life cycle management of all network devices deployed in an Operator Nexus instance. NFC is hosted in a  Microsoft Azure Virtual Network  in an Azure region. The region should be connected to your on-premises network via  Microsoft Azure ExpressRoute . An NFC can manage the Network Fabric of many (subject to limits) Operator Nexus instances."}
{"text": "Network Fabric\nThe Network Fabric resource models a collection of network devices, compute servers, and storage appliances, and their interconnections. The Network Fabric resource also includes the networking required for your network functions and workloads. Each Operator Nexus instance has one Network Fabric.\nThe Network Fabric Controller (NFC) performs the lifecycle management of the Network Fabric. It configures and bootstraps the Network Fabric resources."}
{"text": "Cluster manager\nThe Cluster Manager (CM) is hosted on Azure and manages the lifecycle of all on- premises clusters. Like NFC, a CM can manage multiple Operator Nexus instances. The CM and the NFC are hosted in the same Azure subscription."}
{"text": "Cluster\nThe Cluster (or Compute Cluster) resource models a collection of racks, bare metal machines, storage, and networking. Each cluster is mapped to the on-premises Network Fabric. A cluster provides a holistic view of the deployed compute capacity. Cluster capacity examples include the number of vCPUs, the amount of memory, and the amount of storage space. A cluster is also the basic unit for compute and storage upgrades."}
{"text": "Network Rack\nThe Network Rack consists of Consumer Edge (CE) routers, Top of Rack switches (ToRs), storage appliance, Network Packet Broker (NPB), and the Terminal Server (TS). The Rack also models the connectivity to the operator's Physical Edge switches (PEs) and the ToRs on the other Racks."}
{"text": "Rack\nThe Rack (or a compute rack) resource represents the compute servers (Bare Metal Machines), management servers, management switch and ToRs. The Rack is created, updated or deleted as part of the Cluster lifecycle management."}
{"text": "Storage appliance\nStorage Appliances represent storage arrays used for persistent data storage in the Operator Nexus instance. All user and consumer data is stored in these appliances local to your premises. This local storage complies with some of the most stringent local data storage requirements."}
{"text": "Bare Metal Machine\nBare Metal Machines represent the physical servers in a rack. They're lifecycle managed by the Cluster Manager. Bare Metal Machines are used by workloads to host Virtual Machines and Kubetnetes clusters."}
{"text": "Workload components\nWorkload components are resources that you use in hosting your workloads."}
{"text": "Network resources\nThe Network resources represent the virtual networking in support of your workloads hosted on VMs or Kubernetes clusters. There are five Network resource types that represent a network attachment to an underlying isolation-domain.\nCloud Services Network Resource : provides VMs/Kubernetes clusters access to cloud services such as DNS, NTP, and user-specified Azure PaaS services. You must create at least one Cloud Services Network (CSN) in each of your Operator Nexus instances. Each CSN can be reused by many VMs and/or tenant clusters.\nLayer 2 Network Resource : enables "East-West" communication between VMs or tenant clusters.\nLayer 3 Network Resource : facilitate "North-South" communication between your VMs/tenant clusters and the external network.\nTrunked Network Resource : provides a VM or an tenant cluster access to multiple layer 3 networks and/or multiple layer 2 networks."}
{"text": "Virtual machine\nYou can use VMs to host your Virtualized Network Function (VNF) workloads."}
{"text": "Nexus Kubernetes cluster\nNexus Kubernetes cluster is Azure Kubernetes Service cluster modified to run on your on-premises Operator Nexus instance. The Nexus Kubernetes cluster is designed to host your Containerized Network Function (CNF) workloads."}
{"text": "Azure Operator Nexus compute\nArticle  05/24/2023\nAzure Operator Nexus is built on some basic constructs like compute servers, storage appliance, and network fabric devices. These compute servers, also referred to as BareMetal Machines (BMMs), represent the physical machines in the rack. They run the CBL-Mariner operating system and provide closed integration support for high- performance workloads.\nThese BareMetal Machine gets deployed as part of the Azure Operator Nexus automation suite and live as nodes in a Kubernetes cluster to serve various virtualized and containerized workloads in the ecosystem.\nEach BareMetal Machine within an Azure Operator Nexus instance is represented as an Azure resource and Operators (end users) get access to perform various operations to manage its lifecycle like any other Azure resource."}
{"text": "Key capabilities offered in Azure Operator"}
{"text": "Nexus compute\nNUMA Alignment  Nonuniform memory access (NUMA) alignment is a technique to optimize performance and resource utilization in multi-socket servers. It involves aligning memory and compute resources to reduce latency and improve data access within a server system. The strategic placement of software components and workloads in a NUMA-aware manner, Operators can enhance the performance of network functions, such as virtualized routers and firewalls. This placement leads to improved service delivery and responsiveness in their Telco cloud environments. By default, all the workloads deployed in an Azure Operator Nexus instance are NUMA-aligned. CPU Pinning  CPU pinning is a technique to allocate specific CPU cores to dedicated tasks or workloads, ensuring consistent performance and resource isolation. Pinning critical network functions or real-time applications to specific CPU cores allows Operators to minimize latency and improve predictability in their infrastructure. This approach is useful in scenarios where strict quality-of-service requirements exist, ensuring that these tasks receive dedicated processing power for optimal performance. All of the virtual machines created for Virtual Network Function (VNF) or Containerized Network Function (CNF) workloads on Nexus compute are pinned to specific virtual cores. This pinning provides better performance and avoids CPU stealing.\nCPU Isolation  CPU isolation provides a clear separation between the CPUs allocated for workloads from the CPUs allocated for control plane and platform activities. CPU isolation prevents interference and limits the performance predictability for critical workloads. By isolating CPU cores or groups of cores, we can mitigate the effect of noisy neighbors. It guarantees the required processing power for latency-sensitive applications. Azure Operator Nexus reserves a small set of CPUs for the host operating system and other platform applications. The remaining CPUs are available for running actual workloads. Huge Page Support  Huge page usage in Telco workloads refers to the utilization of large memory pages, typically 2 MB or 1 GB in size, instead of the standard 4-KB pages. This approach helps reduce memory overhead and improves the overall system performance. It reduces the translation look-aside buffer (TLB) miss rate and improves memory access efficiency. Telco workloads that involve large data sets or intensive memory operations, such as network packet processing can benefit from huge page usage as it enhances memory performance and reduces memory-related bottlenecks. As a result, users see improved throughput and reduced latency. All virtual machines created on Azure Operator Nexus can make use of either 2 MB or 1-GB huge pages depending on the flavor of the virtual machine. Dual Stack Support  Dual stack support refers to the ability of networking equipment and protocols to simultaneously handle both IPv4 and IPv6 traffic. With the depletion of available IPv4 addresses and the growing adoption of IPv6, dual stack support is crucial for seamless transition and coexistence between the two protocols. Telco operators utilize dual stack support to ensure compatibility, interoperability, and future-proofing of their networks, allowing them to accommodate both IPv4 and IPv6 devices and services while gradually transitioning towards full IPv6 deployment. Dual stack support ensures uninterrupted connectivity and smooth service delivery to customers regardless of their network addressing protocols. Azure Operator Nexus provides support for both IPV4 and IPV6 configuration across all layers of the stack. Network Interface Cards  Computes in Azure Operator Nexus are designed to meet the requirements for running critical applications that are Telco-grade and can perform fast and efficient data transfer between servers and networks. Workloads can make use of SR-IOV (Single Root I/O Virtualization) that enables the direct assignment of physical I/O resources, such as network interfaces, to virtual machines. This direct assignment bypasses the hypervisor's virtual switch layer. This direct hardware access improves network throughput, reduces latency, and enables more efficient utilization of resources. It makes it an ideal choice for Operators running virtualized and containerized network functions."}
{"text": "BareMetal machine status\nThere are multiple properties, which reflects the operational state of BareMetal Machines. Some of these include:\nPower state Ready state Cordon status Detailed status\nPower state\n field indicates the state as derived from BareMetal Controller (BMC). The\nstate can be either 'On' or 'Off'.\nThe \nReady State\n field provides an overall assessment of the BareMetal Machine\nreadiness. It looks at a combination of Detailed Status, Power State and provisioning state of the resource to determine whether the BareMetal Machine is ready or not. When  Ready State  is 'True', the BareMetal Machine is powered on, the  Detailed Status  is 'Provisioned' and the node representing the BareMetal Machine has successfully joined the Undercloud Kubernetes cluster. If any of those conditions aren't met, the  Ready State is set to 'False'.\nThe \nCordon State\n reflects the ability to run any workloads on machine. Valid values\ninclude 'Cordoned' and 'Uncordoned'. "Cordoned' seizes creation of any new workloads on the machine, whereas "Uncordoned' ensures that workloads can now run on this BareMetal Machine.\nThe BareMetal Machine \nDetailed Status\n field reflects the current status of the machine.\nPreparing - Preparing for provisioning of the machine Provisioning - Provisioning in progress Provisioned  - The OS is provisioned to the machine Available  - Available to participate in the cluster Error  - Unable to provision the machine\nBold indicates an end state status.  Preparing  and  Provisioning  are transitory states. Available  indicates the machine has successfully provisioned but is currently powered off."}
{"text": "BareMetal machine operations\nUpdate/Patch BareMetal Machine  Update the bare metal machine resource properties.\nList/Show BareMetal Machine  Retrieve bare metal machine information. Reimage BareMetal Machine  Reprovision a bare metal machine matching the image version being used across the Cluster. Replace BareMetal Machine  Replace a bare metal machine as part of an effort to service the machine. Restart BareMetal Machine  Reboots a bare metal machine. Power Off BareMetal Machine  Power off a bare metal machine. Start BareMetal Machine  Power on a bare metal machine. Cordon BareMetal Machine  Prevents scheduling of workloads on the specified bare metal machine's Kubernetes node. Optionally allows for evacuation of the workloads from the node. Uncordon BareMetal Machine  Allows scheduling of workloads on the specified bare metal machine's Kubernetes node. BareMetalMachine Validate  Triggers hardware validation of a bare metal machine. BareMetalMachine Run  Allows the customer to run a script specified directly in the input on the targeted bare metal machine. BareMetalMachine Run Data Extract  Allows the customer to run one or more data extractions against a bare metal machine. BareMetalMachine Run Read-only  Allows the customer to run one or more read- only commands against a bare metal machine.\n  Note\nCustomers cannot explicitly create or delete BareMetal Machines directly.\nThese machines are only created as the realization of the Cluster lifecycle.\nImplementation will block any creation or delete requests from any user, and\nonly allow internal/application driven creates or deletes."}
{"text": "Form-factor specific information\nAzure Operator Nexus offers a group of on-premises cloud solutions catering to both Near Edge  and Far-Edge environments. For more information about the compute offerings and the respective configurations, see the following reference links for more details."}
{"text": "Azure Operator Nexus storage"}
{"text": "appliance\nArticle  07/03/2023\nOperator Nexus is built on some basic constructs like compute servers, storage appliance, and network fabric devices. These storage appliances, also referred to as Nexus storage appliances, represent the persistent storage appliance in the rack. In each Nexus storage appliance, there are multiple storage devices, which are aggregated to provide a single storage pool. This storage pool is then carved out into multiple volumes, which are then presented to the compute servers as block storage devices. The compute servers can then use these block storage devices as persistent storage for their workloads. Each Nexus cluster is provisioned with a single storage appliance that is shared across all the tenant workloads.\nThe storage appliance within an Operator Nexus instance is represented as an Azure resource and operators (end users) get access to view its attributes like any other Azure resource."}
{"text": "Key capabilities offered in Azure Operator"}
{"text": "Nexus Storage software stack"}
{"text": "Kubernetes storage classes\nThe Nexus Software Kubernetes stack offers two types of storage, selectable using the Kubernetes StorageClass mechanism."}
{"text": "StorageClass: nexus-volume\nThe default storage mechanism, known as "nexus-volume," is the preferred choice for most users. It provides the highest levels of performance and availability. However, it's important to note that volumes can't be simultaneously shared across multiple worker nodes. These volumes can be accessed and managed using the Azure API and Portal through the Volume Resource."}
{"text": "StorageClass: nexus-shared\nIn situations where a "shared filesystem" is required, the "nexus-shared" storage class is available. This storage class enables multiple pods to concurrently access and share the\nsame volume, providing a shared storage solution. While the performance and availability of "nexus-shared" are sufficient for most applications, it's recommended that workloads with heavy IO (input/output) requirements utilize the "nexus-volume" option mentioned earlier for optimal performance."}
{"text": "Storage appliance status\nThere are multiple properties, which reflect the operational state of storage appliance. Some of these include:\nStatus Provisioning state Capacity total / used Remote Vendor Management\nStatus\n field indicates the state as derived from the storage appliance. The state can be\nAvailable, Error or Provisioning.\nThe \nProvisioning State\n field provides the current provisioning state of the storage\nappliance. The provisioning state can be Succeeded, Failed, or InProgress.\nThe \nCapacity\n field provides the total and used capacity of the storage appliance.\nThe \nRemote Vendor Management\n field indicates whether the remote vendor management\nis enabled or disabled for the storage appliance."}
{"text": "Storage appliance operations\nList Storage Appliances  List storage appliances in the provided resource group or subscription. Show Storage Appliance  Get properties of the provided storage appliance. Update Storage Appliance  Update properties or provided tags of the provided storage appliance. Enable/Disable Remote Vendor Management for Storage Appliance Enable/Disable remote vendor management for the provided storage appliance.\n  Note\nCustomers cannot explicitly create or delete storage appliances directly. These resources are only created as the realization of the Cluster lifecycle. Implementation\nwill block any creation or delete requests from any user, and only allow internal/application driven creates or deletes."}
{"text": "Nexus Kubernetes cluster overview\nArticle  07/05/2023\nThis article introduces you to Nexus Kubernetes cluster."}
{"text": "What is Kubernetes?\nKubernetes is a rapidly evolving platform that manages container-based applications and their associated networking and storage components. Kubernetes focuses on the application workloads, not the underlying infrastructure components. It provides a declarative approach to deployments, backed by a robust set of APIs for management operations. See  What is Kubernetes  to learn about Kubernetes."}
{"text": "Nexus Kubernetes cluster\nNexus Kubernetes cluster (NAKS) is an Operator Nexus version of AKS for on-premises use. It is optimized to automate creation of containers to run tenant network function workloads.\nLike any Kubernetes cluster, Nexus Kubernetes cluster has two components:\n Control plane: provides core Kubernetes services and orchestration of application workloads.\n Nodes: There are two difference node pools in Nexus Kubernetes Clusters - System node pools and user node pools. System node pools host critical system pods. User node pools host application pods. However, application pods can be scheduled on system node pools if user wants only one pool in their cluster. Every Nexus Kubernetes Cluster must contain at least one system node pool with at least one node."}
{"text": "Failure domain\nOperator Nexus ensures that the Nexus Kubernetes Cluster VMs are distributed across nodes and failure domains (physical racks). This distribution is done in a way that improves the resilience and availability of the cluster. Operator Nexus uses Kubernetes affinity rules to schedule clusters in specific zones. This ensures that VMs aren't placed on the same node or in the same failure domain, improving the cluster's fault tolerance. The utilization of the failure domains is especially advantageous when operators have diverse performance requirements for racks. Or when they aim to guarantee that certain workloads remain isolated to specific racks."}
{"text": "Next steps\nGuide to deploy Nexus kubernetes cluster"}
{"text": "Azure Operator Nexus observability\nArticle  03/14/2023\nThe Operator Nexus observability framework provides operational insights into your on- premises instances. The framework supports logging, monitoring, and alerting (LMA), analytics, and visualization of operational (platform and workloads) data and metrics.\nFigure: Operator Nexus Logging, Monitoring and Alerting (LMA) Framework\nThe key highlights of Operator Nexus observability framework are:\nCentralized data collection : Operator Nexus observability solution is based on a collection of all the data in a central place. In this place, you can observe the monitoring data from all of your on-premises instances. Well-defined and tested tooling : The solution relies on Azure Monitor that collects, analyzes, and acts on telemetry data from your cloud and on-premises instances. Easy to learn and use : The solution makes it easy for you to analyze and debug problems with the ability to search the data from within or across all of your cloud and on-premises instances. Visualization tools : You create customized dashboards and workbooks per your needs. Integrated Alert tooling : You create alerts based on custom thresholds. You can create and reuse alert templates across all of your instances.\nThis article helps you understand Operator Nexus observability framework that consists of a stack of components:\nAzure Monitor collects and aggregates logging data from the Operator Nexus components Azure Log Analytics Workspace (LAW) collects and aggregates logging data from multiple Azure subscriptions and tenants Analysis, visualization, and alerting are performed on the aggregated log data."}
{"text": "Platform Monitoring\nOperator Nexus gives you visibility into the performance of your deployments that consist of  infrastructure resources . You need the logs and metrics to be collected and analyzed from these platform resources. You gain valuable insights from the centralized collection and aggregation of data from all sources, compared with from dis- aggregated data.\nThese logs and metrics are used to observe the state of the platform. You can see the performance and analyze what's wrong. You can analyze what caused the situation. Visualization helps you configure the required alerts and under what conditions. For example, you can configure the alerts to be generated when resources are behaving abnormally, or when thresholds have been reached. You can use the collected logs and analytics to debug any problems in the environment."}
{"text": "Monitoring Data\nOperator Nexus observability allows you to collect the same kind of data as other Azure resources. The data collected from each of your instances can be viewed in your LAW.\nYou can learn about monitoring Azure resources  here ."}
{"text": "Collection and Routing\nOperator Nexus observability allows you to collect data for each infrastructure resource. The set of infrastructure components includes:\nNetwork fabric that includes CEs, TORs, NPBs, management switches, and the terminal server. Compute that includes Bare Metal Servers. Undercloud Control Plane (Kubernetes cluster responsible for deployment and managing lifecycle of overall Platform).\nCollection of log data from these layers is enabled by default during the creation of your Operator Nexus instance. These collected logs are routed to your Azure Monitor LAW.\nYou can also collect data from the tenant layers created for running Containerized and Virtualized Network Functions. The log data that can be collected includes:\nCollection of syslog from Virtual Machines (used for either VNFs or CNF workloads). Collection of logs from AKS-Hybrid clusters and the applications deployed on top.\nYou'll need to enable the collection of the logs from the tenant AKS-Hybrid clusters and Virtual Machines. You should follow the steps to deploy the  Azure monitoring agents . The data would be collected in your Azure LAW."}
{"text": "Operator Nexus Logs storage\nData in Azure Monitor Logs is stored in tables where each table has its own set of unique properties.\nAll resource logs in Azure Monitor have the same fields followed by service-specific fields; see the  common schema .\nThe logs from Operator Nexus platform are stored in the following tables:\nTable Description\nSyslog Syslog events on Linux computers using the Log Analytics agent\nContainerInventory Details and current state of each container.\nContainerLog Log lines collected from stdout and stderr streams for containers\nContainerNodeInventory Details of nodes that serve as container hosts.\nInsightMetrics Metrics collected from Server, K8s, Containers.\nKubeEvents Kubernetes events and their properties.\nKubeMonAgentEvents Events logged by Azure Monitor Kubernetes agent for errors and warnings.\nKubeNodeInventory Details for nodes that are part of Kubernetes cluster\nKubePodInventory Kubernetes pods and their properties\nKubePVInventory Kubernetes persistent volumes and their properties.\nKubeServices Kubernetes services and their properties\nHeartbeat Records logged by Log Analytics agents once per minute to report on agent health"}
{"text": "Operator nexus metrics\nThe 'InsightMetrics' table in the Logs section contains the metrics collected from Bare Metal Machines and the undercloud Kubernetes cluster. In addition, a few selected metrics collected from the undercloud can be observed by opening the Metrics tab from the Azure Monitor menu.\nFigure: Azure Monitor Metrics Selection\nSee  Getting Started with Azure Metrics Explorer  for details on using this tool."}
{"text": "Workbooks\nWorkbooks combine text,log queries, metrics, and parameters for data analysis and the creation of multiple kinds of rich visualizations. You can use the sample Azure Resource Manager workbook templates for  Operator Nexus Logging and Monitoring  to deploy Azure Workbooks within your Azure LAW."}
{"text": "Alerts\nYou can use the sample Azure Resource Manager alarm templates for  Operator Nexus alerting rules . You should specify thresholds and conditions for the alerts. You can then deploy these alert templates on your on-premises environment."}
{"text": "Log Analytic Workspace\nA  LAW  is a unique environment to log data from Azure Monitor and other Azure services. Each workspace has its own data repository and configuration but may combine data from multiple services. Each workspace consists of multiple data tables.\nA single LAW can be created to collect all relevant data or multiple workspaces based on operator requirements."}
{"text": "Prerequisites for deploying tenant"}
{"text": "workloads\nArticle  07/12/2023\nThis guide explains prerequisites for creating:\nVirtual machines (VMs) for virtual network function (VNF) workloads. Nexus Kubernetes cluster deployments for cloud-native network function (CNF) workloads."}
{"text": "Network prerequisites\nYou need to create various networks based on your workload needs. The following list of considerations isn't exhaustive. Consult with the appropriate support teams for help.\nDetermine the types of networks that you need to support your workloads: A layer 3 (L3) network requires a VLAN and subnet assignment. The subnet must be large enough to support IP assignment to each of the VMs. The platform reserves the first three usable IP addresses for internal use. For instance, to support six VMs, the minimum CIDR for your subnet is /28 (14 usable addresses  3 reserved = 11 addresses available). A layer 2 (L2) network requires only a single VLAN assignment. A trunked network requires the assignment of multiple VLANs. Determine how many networks of each type you need. Determine the MTU size of each of your networks (maximum is 9,000).\nDetermine the BGP peering info for each network, and whether the networks need to talk to each other. You should group networks that need to talk to each other into the same L3 isolation domain, because each L3 isolation domain can support multiple L3 networks. The platform provides a proxy to allow your VM to reach other external endpoints. Creating a \ncloudservicesnetwork\n  instance requires the endpoints to be proxied, so\ngather the list of endpoints. You can modify the list of endpoints after the network creation."}
{"text": "Create networks for tenant workloads\nThe following sections explain the steps to create networks for tenant workloads (VMs and Kubernetes clusters)."}
{"text": "Create isolation domains\nIsolation domains enable creation of layer 2 (L2) and layer 3 (L3) connectivity between network functions running on Azure Operator Nexus. This connectivity enables inter- rack and intra-rack communication between the workloads. You can create as many L2 and L3 isolation domains as needed.\nYou should have the following information already:\nThe network fabric resource ID to create isolation domains. VLAN and subnet info for each L3 network. Which networks need to talk to each other. (Remember to put VLANs and subnets that need to talk to each other into the same L3 isolation domain.) BGP peering and network policy information for your L3 isolation domains. VLANs for all your L2 networks. VLANs for all your trunked networks. MTU values for your networks."}
{"text": "L2 isolation domain\nCreate an L2 isolation domain:\nAzure CLI\n  az networkfabric l2domain create  --resource-name   " <YourL2IsolationDomainName>"  \      --resource-group   "<YourResourceGroupName>"  \      --subscription   "<YourSubscription>"  \\n     --nf-id   "<NetworkFabricResourceId>"  \      --location   "<ClusterAzureRegion>"  \      --vlan   <YourNetworkVlan>  \      --mtu   "<MtuOfThisNetwork>\nEnable the L2 isolation domain that you created:\nAzure CLI\n  az networkfabric l2domain update -administrative-state  \      --name   "<YourL2IsolationDomainName>"  \      --resource-group   "<YourResourceGroupName>"  \      --subscription   "<YourSubscription>"  \      --state  Enable\nRepeat, as needed, to create other L2 isolation domains."}
{"text": "L3 isolation domain\nCreate an L3 isolation domain:\nAzure CLI\n  az networkfabric l3domain create \      --resource-name   "<YourL3IsolationDomainName>"  \      --resource-group   "<YourResourceGroupName>"  \      --subscription   "<YourSubscription>"  \      --nf-id   "<NetworkFabricResourceId >"  \      --location   "<ClusterAzureRegion>"\nCreate an \ninternalnetwork\n  resource for every VLAN or subnet that you need to include\nin your L3 isolation domain.\n  Note\nThe following example uses the minimal configuration for creating a valid internal network. It doesn't show optional parameters.\nAzure CLI\n  az networkfabric internalnetwork create \      --resource-name   "<L3IsolationDomainInternalNetworkName>"  \      --resource-group   "<YourResourceGroupName>"  \      --subscription   "<YourSubscription>"  \      --l 3 -isolation-domain-name   "<YourL3IsolationDomainName>"  \      --vlan-id   <YourNetworkVlan>  \\n     --mtu   <MtuOfThisNetwork>  \      --connected-ipv 4 -subnets   '[{prefix="<YourSubnetCIDR> \         "gateway="<YourGatewayIp>}]'      --bgp-configuration   '{"fabric-asn" <FabricAsNumber>, \         "defaultRouteOriginate":<boolean>, "peerASN": <PeerAsNumber>, \         "ipv4NeighborAddress":[{"address": "<YourSubetInfoHere> "}]}'\nRepeat, as needed, for any other \ninternalnetwork\n  resources that you have to add to this\nL3 isolation domain.\nEnable the L3 isolation domain after you've created all \ninternalnetwork\n  resources.\nAzure CLI\n  az networkfabric l3domain update -admin-state  \      --resource-name   "<YourL3IsolationDomainName>"  \      --resource-group   "<YourResourceGroupName>"  \      --subscription   "<YourSubscription>"  \      --state  Enable\nRepeat to create more L3 isolation domains."}
{"text": "Create networks for tenant workloads\nThe following sections describe how to create these networks:\nLayer 2 network Layer 3 network Trunked network Cloud services network"}
{"text": "Create an L2 network\nCreate an L2 network, if necessary, for your workloads. You can repeat the instructions for each required L2 network.\nGather the resource ID of the L2 isolation domain that you  created  to configure the VLAN for this network.\nHere's an example Azure CLI command:\nAzure CLI\n  az networkcloud l2network create  --name   "<YourL2NetworkName>"  \      --resource-group   "<YourResourceGroupName>"  \      --subscription   "<YourSubscription>"  \\n     --extended-location   name= "<ClusterCustomLocationId>"   type= "CustomLocation"  \      --location   "<ClusterAzureRegion>"  \      --l 2 -isolation-domain-id   "<YourL2IsolationDomainId>""}
{"text": "Create an L3 network\nCreate an L3 network, if necessary, for your workloads. Repeat the instructions for each required L3 network.\nYou need:\nThe  created  to configure the\nresourceID\n  value of the L3 isolation domain that you \nVLAN for this network. The \nipv4-connected-prefix\n  value, which must match the \ni-pv4-connected-prefix\nvalue that's in the L3 isolation domain.\nipv6-connected-prefix\n  value, which must match the \ni-pv6-connected-prefix\nThe \nvalue that's in the L3 isolation domain.\nip-allocation-type\n  value, which can be \nIPv4\n , \nIPv6\n , or \nDualStack\n  (default). The \nThe \nvlan\n  value, which must match what's in the L3 isolation domain.\nAzure CLI\n  az networkcloud l3network create  --name   "<YourL3NetworkName>"  \      --resource-group   "<YourResourceGroupName>"  \      --subscription   "<YourSubscription>"  \      --extended-location   name= "<ClusterCustomLocationId>"   type= "CustomLocation"  \      --location   "<ClusterAzureRegion>"  \      --ip-allocation-type   "<YourNetworkIpAllocation>"  \      --ipv 4 -connected-prefix   "<YourNetworkIpv4Prefix>"  \      --ipv 6 -connected-prefix   "<YourNetworkIpv6Prefix>"  \      --l 3 -isolation-domain-id   "<YourL3IsolationDomainId>"  \      --vlan   <YourNetworkVlan>"}
{"text": "Create a trunked network\nCreate a trunked network, if necessary, for your VM. Repeat the instructions for each required trunked network.\nGather the \nresourceId\n  values of the L2 and L3 isolation domains that you created earlier\nto configure the VLANs for this network. You can include as many L2 and L3 isolation domains as needed.\nAzure CLI\n  az networkcloud trunkednetwork create  --name   "<YourTrunkedNetworkName>"  \      --resource-group   "<YourResourceGroupName>"  \      --subscription   "<YourSubscription>"  \      --extended-location   name= "<ClusterCustomLocationId>"   type= "CustomLocation"  \      --location   "<ClusterAzureRegion>"  \      --interface-name   "<YourNetworkInterfaceName>"  \      --isolation-domain-ids  \        "<YourL3IsolationDomainId1>"  \        "<YourL3IsolationDomainId2>"  \        "<YourL2IsolationDomainId1>"  \        "<YourL2IsolationDomainId2>"  \        "<YourL3IsolationDomainId3>"  \      --vlans   <YourVlanList>"}
{"text": "Create a cloud services network\nYour VM requires at least one cloud services network. You need the egress endpoints that you want to add to the proxy for your VM to access. This list should include any domains needed to pull images or access data, such as \n.azurecr.io\n  or \n.docker.io\n .\nAzure CLI\n  az networkcloud cloudservicesnetwork create  --name   " <YourCloudServicesNetworkName>"  \      --resource-group   "<YourResourceGroupName >"  \      --subscription   "<YourSubscription>"  \      --extended-location   name= "<ClusterCustomLocationId >"   type= "CustomLocation"  \      --location   "<ClusterAzureRegion>"  \      --additional-egress-endpoints   "[{\"category\":\"<YourCategory  >\",\"endpoints\":[{\"<domainName1 >\":\"< endpoint1 >\",\"port\": <portnumber1 >}]}]""}
{"text": "Using the proxy to reach outside of the virtual machine\nOnce you have created your VM or Kubernetes cluster with this cloud services network, you can use the proxy to reach outside of the virtual machine. Proxy is useful if you need to access resources outside of the virtual machine, such as pulling images or accessing data.\nTo use the proxy, you need to set the following environment variables:\nBash\nexport  HTTP_PROXY=http://169.254.0.11:3128 export  http_proxy=http://169.254.0.11:3128 export  HTTPS_PROXY=http://169.254.0.11:3128 export  https_proxy=http://169.254.0.11:3128\nOnce you have set the environment variables, your virtual machine should be able to reach outside of the virtual network using the proxy.\nIn order to reach the desired endpoints, you need to add the required egress endpoints\n--additional-\nto the cloud services network. Egress endpoints can be added using the \negress-endpoints\n  parameter when creating the network. Be sure to include any domains\nneeded to pull images or access data, such as \n.azurecr.io\n  or \n.docker.io\n .\n  Important\nWhen using a proxy, it's also important to set the \nno_proxy\n  environment variable\nproperly. This variable can be used to specify domains or IP addresses that shouldn't be accessed through the proxy. If not set properly, it can cause issues while accessing services, such as the Kubernetes API server or cluster IP. Make sure to include the IP address or domain name of the Kubernetes API server and any cluster IP addresses in the \nno_proxy\n  variable."}
{"text": "Nexus Kubernetes cluster availability zone\nWhen you're creating a Nexus Kubernetes cluster, you can schedule the cluster onto specific racks or distribute it evenly across multiple racks. This technique can improve resource utilization and fault tolerance.\nIf you don't specify a zone when you're creating a Nexus Kubernetes cluster, the Azure Operator Nexus platform automatically implements a default anti-affinity rule. This rule aims to prevent scheduling the cluster VM on a node that already has a VM from the same cluster, but it's a best-effort approach and can't make guarantees.\nTo get the list of available zones in the Azure Operator Nexus instance, you can use the following command:\nAzure CLI\n    az networkcloud cluster show \        --resource-group   <Azure Operator Nexus on-premises cluster resource  group>  \\n       --name   <Azure Operator Nexus on-premises cluster name>  \        --query  computeRackDefinitions[* ] .availabilityZone"}
{"text": "Quickstart: Create an Azure Nexus"}
{"text": "Kubernetes cluster by using Azure CLI\nArticle  06/26/2023\nDeploy an Azure Nexus Kubernetes cluster using Azure CLI."}
{"text": "Before you begin\nIf you don't have an  Azure subscription , create an  Azure free account  before you begin.\nUse the Bash environment in  Azure Cloud Shell . For more information, see Quickstart for Bash in Azure Cloud Shell .\nIf you prefer to run CLI reference commands locally,  install  the Azure CLI. If you're running on Windows or macOS, consider running Azure CLI in a Docker container. For more information, see  How to run the Azure CLI in a Docker container .\nIf you're using a local installation, sign in to the Azure CLI by using the  az login command. To finish the authentication process, follow the steps displayed in your terminal. For other sign-in options, see  Sign in with the Azure CLI .\nWhen you're prompted, install the Azure CLI extension on first use. For more information about extensions, see  Use extensions with the Azure CLI .\nRun  az version  to find the version and dependent libraries that are installed. To upgrade to the latest version, run  az upgrade .\nInstall the latest version of the  necessary Azure CLI extensions .\nThis article requires version 2.49.0 or later of the Azure CLI. If using Azure Cloud Shell, the latest version is already installed.\nIf you have multiple Azure subscriptions, select the appropriate subscription ID in which the resources should be billed using the  az account  command.\nCreate a resource group using the  az group create  command. An  Azure resource group  is a logical group in which Azure resources are deployed and managed. When you create a resource group, you're prompted to specify a location. This location is the storage location of your resource group metadata and where your\nresources run in Azure if you don't specify another region during resource creation. The following example creates a resource group named  myResourceGroup  in the eastus  location.\nAzure CLI\naz group create  --name  myResourceGroup  --location  eastus \nThe following output example resembles successful creation of the resource group:\nJSON\n{     "id" :  "/subscriptions/<guid>/resourceGroups/myResourceGroup" ,     "location" :  "eastus" ,     "managedBy" :  null ,     "name" :  "myResourceGroup" ,     "properties" : {       "provisioningState" :  "Succeeded"     },     "tags" :  null   } \nTo deploy a Bicep file or ARM template, you need write access on the resources you're deploying and access to all operations on the Microsoft.Resources/deployments resource type. For example, to deploy a cluster, you need Microsoft.NetworkCloud/kubernetesclusters/write and Microsoft.Resources/deployments/* permissions. For a list of roles and permissions, see  Azure built-in roles .\nYou need the \ncustom location\n resource ID of your Azure Operator Nexus cluster.\nYou need to create various networks according to your specific workload requirements, and it's essential to have the appropriate IP addresses available for your workloads. To ensure a smooth implementation, it's advisable to consult the relevant support teams for assistance.\nThis quickstart assumes a basic understanding of Kubernetes concepts. For more information, see  Kubernetes core concepts for Azure Kubernetes Service (AKS) ."}
{"text": "Create an Azure Nexus Kubernetes cluster\nThe following example creates a cluster named  myNexusAKSCluster  in resource group myResourceGroup  in the  eastus  location.\nBefore you run the commands, you need to set several variables to define the configuration for your cluster. Here are the variables you need to set, along with some default values you can use for certain variables:\nVariable Description\nLOCATION The Azure region where you want to create your cluster.\nRESOURCE_GROUP The name of the Azure resource group where you want to create the cluster.\nSUBSCRIPTION_ID The ID of your Azure subscription.\nCUSTOM_LOCATION This argument specifies a custom location of the Nexus instance.\nCSN_ARM_ID CSN ID is the unique identifier for the cloud services network you want to use.\nCNI_ARM_ID CNI ID is the unique identifier for the network interface to be used by the container runtime.\nAAD_ADMIN_GROUP_OBJECT_ID The object ID of the Azure Active Directory group that should have admin privileges on the cluster.\nCLUSTER_NAME The name you want to give to your Nexus Kubernetes cluster.\nK8S_VERSION The version of Kubernetes you want to use.\nADMIN_USERNAME The username for the cluster administrator.\nSSH_PUBLIC_KEY The SSH public key that is used for secure communication with the cluster.\nCONTROL_PLANE_COUNT The number of control plane nodes for the cluster.\nCONTROL_PLANE_VM_SIZE The size of the virtual machine for the control plane nodes.\nINITIAL_AGENT_POOL_NAME The name of the initial agent pool.\nINITIAL_AGENT_POOL_COUNT The number of nodes in the initial agent pool.\nINITIAL_AGENT_POOL_VM_SIZE The size of the virtual machine for the initial agent pool.\nPOD_CIDR The network range for the Kubernetes pods in the cluster, in CIDR notation.\nSERVICE_CIDR The network range for the Kubernetes services in the cluster, in CIDR notation.\nDNS_SERVICE_IP The IP address for the Kubernetes DNS service.\nOnce you've defined these variables, you can run the Azure CLI command to create the cluster. The \n--debug\n flag at the end is used to provide more detailed output for\ntroubleshooting purposes.\nTo define these variables, use the following set commands and replace the example values with your preferred values. You can also use the default values for some of the variables, as shown in the following example:\nBash\nRESOURCE_GROUP= "myResourceGroup"   LOCATION= "$(az group show --name $RESOURCE_GROUP --query location | tr -d  '\"')"   SUBSCRIPTION_ID= "$(az account show -o tsv --query id)"   CUSTOM_LOCATION= "/subscriptions/<subscription_id>/resourceGroups/<managed_re source_group>/providers/microsoft.extendedlocation/customlocations/<custom- location-name>"   CSN_ARM_ID= "/subscriptions/<subscription_id>/resourceGroups/<resource_group> /providers/Microsoft.NetworkCloud/cloudServicesNetworks/<csn-name>"   CNI_ARM_ID= "/subscriptions/<subscription_id>/resourceGroups/<resource_group> /providers/Microsoft.NetworkCloud/l3Networks/<l3Network-name>"   AAD_ADMIN_GROUP_OBJECT_ID= "00000000-0000-0000-0000-000000000000"   CLUSTER_NAME= "myNexusAKSCluster"   K8S_VERSION= "v1.24.9"   ADMIN_USERNAME= "azureuser"   SSH_PUBLIC_KEY= "$(cat ~/.ssh/id_rsa.pub)"   CONTROL_PLANE_COUNT= "1"   CONTROL_PLANE_VM_SIZE= "NC_G2_v1"   INITIAL_AGENT_POOL_NAME= "${CLUSTER_NAME}-nodepool-1"   INITIAL_AGENT_POOL_COUNT= "1"   INITIAL_AGENT_POOL_VM_SIZE= "NC_M4_v1"   POD_CIDR= "10.244.0.0/16"   SERVICE_CIDR= "10.96.0.0/16"   DNS_SERVICE_IP= "10.96.0.10"  \n  Note\nIt is essential that you replace the placeholders for CUSTOM_LOCATION, CSN_ARM_ID, CNI_ARM_ID, and AAD_ADMIN_GROUP_OBJECT_ID with your actual values before running these commands.\nAfter defining these variables, you can create the Kubernetes cluster by executing the following Azure CLI command:\nAzure CLI\naz networkcloud kubernetescluster create  \  --name   "${CLUSTER_NAME}"  \ \n--resource-group   "${RESOURCE_GROUP}"  \  --subscription   "${SUBSCRIPTION_ID}"  \  --extended-location   name= "${CUSTOM_LOCATION}"   type= CustomLocation \  --location   "${LOCATION}"  \  --kubernetes-version   "${K8S_VERSION}"  \  --aad-configuration  admin -group-object-ids = "[${AAD_ADMIN_GROUP_OBJECT_ID}]"   \  --admin-username   "${ADMIN_USERNAME}"  \  --ssh-key-values   "${SSH_PUBLIC_KEY}"  \  --control-plane-node-configuration  \       count= "${CONTROL_PLANE_COUNT}"  \      vm -sku-name = "${CONTROL_PLANE_VM_SIZE}"  \  --initial-agent-pool-configurations   " [{count:${INITIAL_AGENT_POOL_COUNT},mode:System,name:${INITIAL_AGENT_POOL_NA ME},vm-sku-name:${INITIAL_AGENT_POOL_VM_SIZE}}]"  \  --network-configuration  \      cloud -services-network-id = "${CSN_ARM_ID}"  \      cni -network-id = "${CNI_ARM_ID}"  \       pod-cidrs= "[${POD_CIDR}]"  \       service-cidrs= "[${SERVICE_CIDR}]"  \      dns -service-ip = "${DNS_SERVICE_IP}"  \nAfter a few minutes, the command completes and returns information about the cluster. For more advanced options, see  Quickstart: Deploy an Azure Nexus Kubernetes cluster using Bicep ."}
{"text": "Review deployed resources\nAfter the deployment finishes, you can view the resources using the CLI or the Azure portal.\nTo view the details of the \nmyNexusAKSCluster\n cluster in the \nmyResourceGroup\n resource\ngroup, execute the following Azure CLI command:\nAzure CLI\naz networkcloud kubernetescluster show  \     --name  myNexusAKSCluster \     --resource-group  myResourceGroup \nAdditionally, to get a list of agent pool names associated with the \nmyNexusAKSCluster\ncluster in the \nmyResourceGroup\n resource group, you can use the following Azure CLI\ncommand.\nAzure CLI\naz networkcloud kubernetescluster agentpool list  \     --kubernetes-cluster-name  myNexusAKSCluster \     --resource-group  myResourceGroup \     --output  table"}
{"text": "Connect to the cluster\nNow that the Nexus Kubernetes cluster has been successfully created and connected to Azure Arc, you can easily connect to it using the cluster connect feature. Cluster connect allows you to securely access and manage your cluster from anywhere, making it convenient for interactive development, debugging, and cluster administration tasks.\n  Note\nWhen you create a Nexus Kubernetes cluster, Nexus automatically creates a managed resource group dedicated to storing the cluster resources, within this group, the Arc connected cluster resource is established. Retrieve the Arc connected resource id before proceeding with  how to connect to an Azure Arc- enabled Kubernetes cluster .\nAzure CLI\naz networkcloud kubernetescluster show  \     --name  myNexusAKSCluster \     --resource-group  myResourceGroup \     --output  tsv \     --query  connectedClusterId \nFor the specific steps to connect to an Azure Arc-connected Nexus Kubernetes cluster, refer  how to connect to an Azure Arc-enabled Kubernetes cluster . This documentation provides a detailed, step-by-step guide to help you connect to your cluster."}
{"text": "Add an agent pool\nThe cluster created in the previous step has a single node pool. Let's add a second agent pool using the \naz networkcloud kubernetescluster agentpool create\n command. The\nfollowing example creates an agent pool named \nmyNexusAKSCluster-nodepool-2\n:\nYou can also use the default values for some of the variables, as shown in the following example:\nBash\nRESOURCE_GROUP= "myResourceGroup"   CUSTOM_LOCATION= "/subscriptions/<subscription_id>/resourceGroups/<managed_re source_group>/providers/microsoft.extendedlocation/customlocations/<custom- location-name>"   CLUSTER_NAME= "myNexusAKSCluster"   AGENT_POOL_NAME= "${CLUSTER_NAME}-nodepool-2"   AGENT_POOL_VM_SIZE= "NC_M4_v1"   AGENT_POOL_COUNT= "1"   AGENT_POOL_MODE= "User"  \nAfter defining these variables, you can add an agent pool by executing the following Azure CLI command:\nAzure CLI\naz networkcloud kubernetescluster agentpool create  \     --name   "${AGENT_POOL_NAME}"  \     --kubernetes-cluster-name   "${CLUSTER_NAME}"  \     --resource-group   "${RESOURCE_GROUP}"  \     --extended-location   name= "${CUSTOM_LOCATION}"   type= CustomLocation \     --count   "${AGENT_POOL_COUNT}"  \     --mode   "${AGENT_POOL_MODE}"  \     --vm-sku-name   "${AGENT_POOL_VM_SIZE}"  \nAfter a few minutes, the command completes and returns information about the agent pool. For more advanced options, see  Quickstart: Deploy an Azure Nexus Kubernetes cluster using Bicep .\n  Note\nYou can add multiple agent pools during the initial creation of your cluster itself by using the initial agent pool configurations. However, if you want to add agent pools after the initial creation, you can utilize the above command to create additional agent pools for your Nexus Kubernetes cluster.\nThe following output example resembles successful creation of the agent pool.\nBash\n$ az networkcloud kubernetescluster agentpool list --kubernetes-cluster-name  myNexusAKSCluster --resource-group myResourceGroup --output table  This  command  is experimental and under development. Reference and support  levels: https://aka.ms/CLI_refstatus  Count    Location    Mode    Name                          ProvisioningState    ResourceGroup    VmSkuName \n-------  ----------  ------  ----------------------------  ----------------- --  ---------------  -----------  1        eastus      System  myNexusAKSCluster-nodepool-1  Succeeded            myResourceGroup  NC_M4_v1  1        eastus      User    myNexusAKSCluster-nodepool-2  Succeeded            myResourceGroup  NC_M4_v1"}
{"text": "Clean up resources\nWhen no longer needed, delete the resource group. The resource group and all the resources in the resource group are deleted.\nAzure CLI\nUse the  az group delete  command to remove the resource group, Kubernetes cluster, and all related resources except the Operator Nexus network resources.\nAzure CLI\naz group delete  --name  myResourceGroup  --yes   --no-wait"}
{"text": "Next steps\nYou can now deploy the CNFs either directly via  cluster connect  or via Azure Operator Service Manager (AOSM)."}
{"text": "Quickstart: Deploy an Azure Nexus"}
{"text": "Kubernetes cluster by using Azure"}
{"text": "Resource Manager template (ARM"}
{"text": "template)\nArticle  06/26/2023\nDeploy an Azure Nexus Kubernetes cluster using an Azure Resource Manager template.\nThis quickstart describes how to use an Azure Resource Manager template (ARM template) to create Azure Nexus Kubernetes cluster.\nA  resource manager template  is a JavaScript Object Notation (JSON) file that defines the infrastructure and configuration for your project. The template uses declarative syntax. In declarative syntax, you describe your intended deployment without writing the sequence of programming commands to create the deployment."}
{"text": "Prerequisites\nIf you don't have an  Azure subscription , create an  Azure free account  before you begin.\nUse the Bash environment in  Azure Cloud Shell . For more information, see Quickstart for Bash in Azure Cloud Shell .\nIf you prefer to run CLI reference commands locally,  install  the Azure CLI. If you're running on Windows or macOS, consider running Azure CLI in a Docker container. For more information, see  How to run the Azure CLI in a Docker container .\nIf you're using a local installation, sign in to the Azure CLI by using the  az login command. To finish the authentication process, follow the steps displayed in your terminal. For other sign-in options, see  Sign in with the Azure CLI .\nWhen you're prompted, install the Azure CLI extension on first use. For more information about extensions, see  Use extensions with the Azure CLI .\nRun  az version  to find the version and dependent libraries that are installed. To upgrade to the latest version, run  az upgrade .\nInstall the latest version of the  necessary Azure CLI extensions .\nThis article requires version 2.49.0 or later of the Azure CLI. If using Azure Cloud Shell, the latest version is already installed.\nIf you have multiple Azure subscriptions, select the appropriate subscription ID in which the resources should be billed using the  az account  command.\nCreate a resource group using the  az group create  command. An  Azure resource group  is a logical group in which Azure resources are deployed and managed. When you create a resource group, you're prompted to specify a location. This location is the storage location of your resource group metadata and where your resources run in Azure if you don't specify another region during resource creation. The following example creates a resource group named  myResourceGroup  in the eastus  location.\nAzure CLI\naz group create  --name  myResourceGroup  --location  eastus\nThe following output example resembles successful creation of the resource group:\nJSON\n{    "id" :  "/subscriptions/<guid>/resourceGroups/myResourceGroup" ,    "location" :  "eastus" ,    "managedBy" :  null ,    "name" :  "myResourceGroup" ,    "properties" : {      "provisioningState" :  "Succeeded"   },    "tags" :  null }\nTo deploy a Bicep file or ARM template, you need write access on the resources you're deploying and access to all operations on the Microsoft.Resources/deployments resource type. For example, to deploy a cluster, you need Microsoft.NetworkCloud/kubernetesclusters/write and Microsoft.Resources/deployments/* permissions. For a list of roles and permissions, see  Azure built-in roles .\nYou need the \ncustom location\n  resource ID of your Azure Operator Nexus cluster.\nYou need to create  various networks  according to your specific workload requirements, and it's essential to have the appropriate IP addresses available for your workloads. To ensure a smooth implementation, it's advisable to consult the relevant support teams for assistance.\nThis quickstart assumes a basic understanding of Kubernetes concepts. For more information, see  Kubernetes core concepts for Azure Kubernetes Service (AKS) ."}
{"text": "Review the template\nBefore deploying the Kubernetes template, let's review the content to understand its structure.\nJSON\n{      "$schema" :  "https://schema.management.azure.com/schemas/2019-04- 01/deploymentTemplate.json#" ,      "contentVersion" :  "1.0.0.0" ,      "parameters" : {        "kubernetesClusterName" : {          "type" :  "string" ,          "metadata" : {            "description" :  "The name of Nexus Kubernetes cluster"         }       },        "location" : {          "type" :  "string" ,          "metadata" : {            "description" :  "The Azure region where the cluster is to be  deployed"         },          "defaultValue" :  "[resourceGroup().location]"       },        "extendedLocation" : {          "type" :  "string" ,          "metadata" : {            "description" :  "The custom location of the Nexus instance"         },          "defaultValue" :  ""       },        "tags" : {          "type" :  "object" ,          "metadata" : {            "description" :  "The metadata tags to be associated with the  cluster resource"         },          "defaultValue" : {}       },        "adminUsername" : {\n         "type" :  "string" ,          "metadata" : {            "description" :  "The username for the administrative account on the  cluster"         },          "defaultValue" :  "azureuser"       },        "adminGroupObjectIds" : {          "type" :  "array" ,          "metadata" : {            "description" :  "The object IDs of Azure Active Directory (AAD)  groups that will have administrative access to the cluster"         },          "defaultValue" : []       },        "cniNetworkId" : {          "type" :  "string" ,          "metadata" : {            "description" :  "The Azure Resource Manager (ARM) id of the network  to be used as the Container Networking Interface (CNI) network"         }       },        "cloudServicesNetworkId" : {          "type" :  "string" ,          "metadata" : {            "description" :  "The ARM id of the network to be used for cloud  services network"         }       },        "podCidrs" : {          "type" :  "array" ,          "metadata" : {            "description" :  "The CIDR blocks used for Nexus Kubernetes PODs in  the cluster"         },          "defaultValue" : [ "10.244.0.0/16" ]       },        "serviceCidrs" : {          "type" :  "array" ,          "metadata" : {            "description" :  "The CIDR blocks used for k8s service in the  cluster"         },          "defaultValue" : [ "10.96.0.0/16" ]       },        "dnsServiceIp" : {          "type" :  "string" ,          "metadata" : {            "description" :  "The IP address of the DNS service in the cluster"         },          "defaultValue" :  "10.96.0.10"       },        "agentPoolL2Networks" : {          "type" :  "array" ,          "metadata" : {\n           "description" :  "The Layer 2 networks associated with the initial  agent pool"         },          "defaultValue" : []         /*           {              "networkId" :  "string" ,              "pluginType" :  "SRIOV|DPDK|OSDevice|MACVLAN|IPVLAN"           }         */       },        "agentPoolL3Networks" : {          "type" :  "array" ,          "metadata" : {            "description" :  "The Layer 3 networks associated with the initial  agent pool"         },          "defaultValue" : []         /*           {              "ipamEnabled" :  "True/False" ,              "networkId" :  "string" ,              "pluginType" :  "SRIOV|DPDK|OSDevice|MACVLAN|IPVLAN"           }         */       },        "agentPoolTrunkedNetworks" : {          "type" :  "array" ,          "metadata" : {            "description" :  "The trunked networks associated with the initial  agent pool"         },          "defaultValue" : []         /*           {              "networkId" :  "string" ,              "pluginType" :  "SRIOV|DPDK|OSDevice|MACVLAN|IPVLAN"           }         */       },        "l2Networks" : {          "type" :  "array" ,          "metadata" : {            "description" :  "The Layer 2 networks associated with the cluster"         },          "defaultValue" : []         /*           {              "networkId" :  "string" ,              "pluginType" :  "SRIOV|DPDK|OSDevice|MACVLAN|IPVLAN"           }         */       },        "l3Networks" : {          "type" :  "array" ,\n         "metadata" : {            "description" :  "The Layer 3 networks associated with the cluster"         },          "defaultValue" : []         /*           {              "ipamEnabled" :  "True/False" ,              "networkId" :  "string" ,              "pluginType" :  "SRIOV|DPDK|OSDevice|MACVLAN|IPVLAN"           }         */       },        "trunkedNetworks" : {          "type" :  "array" ,          "metadata" : {            "description" :  "The trunked networks associated with the cluster"         },          "defaultValue" : []         /*           {              "networkId" :  "string" ,              "pluginType" :  "SRIOV|DPDK|OSDevice|MACVLAN|IPVLAN"           }         */       },        "ipAddressPools" : {          "type" :  "array" ,          "metadata" : {            "description" :  "The LoadBalancer IP address pools associated with  the cluster"         },          "defaultValue" : []         /*           {              "addresses" : [                "string"             ],              "autoAssign" :  "True/False" ,              "name" :  "sting" ,              "onlyUseHostIps" :  "True/False"           }         */       },        "kubernetesVersion" : {          "type" :  "string" ,          "metadata" : {            "description" :  "The version of Kubernetes to be used in the Nexus  Kubernetes cluster"         },          "defaultValue" :  "v1.24.9"       },        "controlPlaneCount" : {          "type" :  "int" ,          "metadata" : {            "description" :  "The number of control plane nodes to be deployed \nin the cluster"         },          "defaultValue" : 1       },        "controlPlaneZones" : {          "type" :  "array" ,          "metadata" : {            "description" :  "The zones/racks used for placement of the control  plane nodes"         },          "defaultValue" : []         /* array of strings Example: [ "1" ,  "2" ,  "3" ] */       },        "agentPoolZones" : {          "type" :  "array" ,          "metadata" : {            "description" :  "The zones/racks used for placement of the agent  pool nodes"         },          "defaultValue" : []         /* array of strings Example: [ "1" ,  "2" ,  "3" ] */       },        "controlPlaneVmSkuName" : {          "type" :  "string" ,          "metadata" : {            "description" :  "The size of the control plane nodes"         },          "defaultValue" :  "NC_G2_v1"       },        "systemPoolNodeCount" : {          "type" :  "int" ,          "metadata" : {            "description" :  "The number of worker nodes to be deployed in the  initial agent pool"         },          "defaultValue" : 1       },        "workerVmSkuName" : {          "type" :  "string" ,          "metadata" : {            "description" :  "The size of the worker nodes"         },          "defaultValue" :  "NC_M4_v1"       },        "initialPoolAgentOptions" : {          "type" :  "object" ,          "metadata" : {            "description" :  "The configurations for the initial agent pool"         },          "defaultValue" : {}         /*            "hugepagesCount" : int,            "hugepagesSize" :  "2M/1G"         */       },\n       "sshPublicKey" : {          "type" :  "string" ,          "metadata" : {            "description" :  "The SSH public key that will be associated with  the 'azureuser' user for secure remote login"         }       },        "labels" : {          "type" :  "array" ,          "metadata" : {            "description" :  "The labels to assign to the nodes in the cluster  for identification and organization"         },          "defaultValue" : []         /*           {              "key" :  "string" ,              "value" :  "string"           }         */       },        "taints" : {          "type" :  "array" ,          "metadata" : {            "description" :  "The taints to apply to the nodes in the cluster to  restrict which pods can be scheduled on them"         },          "defaultValue" : []         /*           {              "key" :  "string" ,              "value" :  "string:NoSchedule|PreferNoSchedule|NoExecute"           }         */       }     },      "resources" : [       {          "type" :  "Microsoft.NetworkCloud/kubernetesClusters" ,          "apiVersion" :  "2023-05-01-preview" ,          "name" :  "[parameters('kubernetesClusterName')]" ,          "location" :  "[parameters('location')]" ,          "tags" :  "[parameters('tags')]" ,          "extendedLocation" : {            "name" :  "[parameters('extendedLocation')]" ,            "type" :  "CustomLocation"         },          "properties" : {            "kubernetesVersion" :  "[parameters('kubernetesVersion')]" ,            "managedResourceGroupConfiguration" : {              "name" :  "[concat(uniqueString(resourceGroup().name), '-',  parameters('kubernetesClusterName'))]" ,              "location" :  "[parameters('location')]"           },            "aadConfiguration" : {"}
{"text": ""adminGroupObjectIds" :  "[parameters('adminGroupObjectIds')]"           },            "administratorConfiguration" : {              "adminUsername" :  "[parameters('adminUsername')]" ,              "sshPublicKeys" : [               {                  "keyData" :  "[parameters('sshPublicKey')]"               }             ]           },            "initialAgentPoolConfigurations" : [             {                "name" :  "[concat(parameters('kubernetesClusterName'), '- nodepool-1')]" ,                "count" :  "[parameters('systemPoolNodeCount')]" ,                "vmSkuName" :  "[parameters('workerVmSkuName')]" ,                "mode" :  "System" ,                "labels" :  "[if(empty(parameters('labels')), json('null'),  parameters('labels'))]" ,                "taints" :  "[if(empty(parameters('taints')), json('null'),  parameters('taints'))]" ,                "agentOptions" :  " [if(empty(parameters('initialPoolAgentOptions')), json('null'),  parameters('initialPoolAgentOptions'))]" ,                "attachedNetworkConfiguration" : {                  "l2Networks" :  "[if(empty(parameters('agentPoolL2Networks')),  json('null'), parameters('agentPoolL2Networks'))]" ,                  "l3Networks" :  "[if(empty(parameters('agentPoolL3Networks')),  json('null'), parameters('agentPoolL3Networks'))]" ,                  "trunkedNetworks" :  " [if(empty(parameters('agentPoolTrunkedNetworks')), json('null'),  parameters('agentPoolTrunkedNetworks'))]"               },                "availabilityZones" :  "[if(empty(parameters('agentPoolZones')),  json('null'), parameters('agentPoolZones'))]" ,                "upgradeSettings" : {                  "maxSurge" :  "1"               }             }           ],            "controlPlaneNodeConfiguration" : {              "count" :  "[parameters('controlPlaneCount')]" ,              "vmSkuName" :  "[parameters('controlPlaneVmSkuName')]" ,              "availabilityZones" :  " [if(empty(parameters('controlPlaneZones')), json('null'),  parameters('controlPlaneZones'))]"           },            "networkConfiguration" : {              "cniNetworkId" :  "[parameters('cniNetworkId')]" ,              "cloudServicesNetworkId" :  " [parameters('cloudServicesNetworkId')]" ,              "dnsServiceIp" :  "[parameters('dnsServiceIp')]" ,              "podCidrs" :  "[parameters('podCidrs')]" ,              "serviceCidrs" :  "[parameters('serviceCidrs')]" ,              "attachedNetworkConfiguration" : {\n               "l2Networks" :  "[if(empty(parameters('l2Networks')),  json('null'), parameters('l2Networks'))]" ,                "l3Networks" :  "[if(empty(parameters('l3Networks')),  json('null'), parameters('l3Networks'))]" ,                "trunkedNetworks" :  "[if(empty(parameters('trunkedNetworks')),  json('null'), parameters('trunkedNetworks'))]"             },              "bgpServiceLoadBalancerConfiguration" : {                "ipAddressPools" :  "[if(empty(parameters('ipAddressPools')),  json('null'), parameters('ipAddressPools'))]"             }           }         }       }     ]   }\nkubernetes-deploy.json\n , Once you have reviewed and saved the template file named \nproceed to the next section to deploy the template."}
{"text": "Deploy the template\n1. Create a file named \nkubernetes-deploy-parameters.json\n  and add the required\nparameters in JSON format. You can use the following example as a starting point. Replace the values with your own.\nJSON\n{    "$schema" :  "https://schema.management.azure.com/schemas/2019-04- 01/deploymentParameters.json#" ,    "contentVersion" :  "1.0.0.0" ,    "parameters" : {      "kubernetesClusterName" :{        "value" :  "myNexusAKSCluster"     },      "adminGroupObjectIds" : {        "value" : [          "00000000-0000-0000-0000-000000000000"       ]     },      "cniNetworkId" : {        "value" :  "/subscriptions/<subscription_id>/resourceGroups/<resource_group>/providers/ Microsoft.NetworkCloud/l3Networks/<l3Network-name>"     },      "cloudServicesNetworkId" : {        "value" :  "/subscriptions/<subscription_id>/resourceGroups/<resource_group>/providers/ Microsoft.NetworkCloud/cloudServicesNetworks/<csn-name>"\n    },      "extendedLocation" : {        "value" :  "/subscriptions/<subscription_id>/resourceGroups/<managed_resource_group>/pr oviders/microsoft.extendedlocation/customlocations/<custom-location-name>"     },      "location" : {        "value" :  "eastus"     },      "sshPublicKey" : {        "value" :  "ssh-rsa AAAAB...."     }   } }\n2. Deploy the template.\nAzure CLI\n    az deployment group create \        --resource-group  myResourceGroup \        --template-file  kubernetes -deploy .json \        --parameters  @kubernetes -deploy-parameters .json"}
{"text": "Review deployed resources\nAfter the deployment finishes, you can view the resources using the CLI or the Azure portal.\nTo view the details of the \nmyNexusAKSCluster\n  cluster in the \nmyResourceGroup\n  resource\ngroup, execute the following Azure CLI command:\nAzure CLI\naz networkcloud kubernetescluster show  \    --name  myNexusAKSCluster \    --resource-group  myResourceGroup\nAdditionally, to get a list of agent pool names associated with the \nmyNexusAKSCluster\nmyResourceGroup\n  resource group, you can use the following Azure CLI cluster in the \ncommand.\nAzure CLI\naz networkcloud kubernetescluster agentpool list  \    --kubernetes-cluster-name  myNexusAKSCluster \\n   --resource-group  myResourceGroup \    --output  table"}
{"text": "Connect to the cluster\nNow that the Nexus Kubernetes cluster has been successfully created and connected to Azure Arc, you can easily connect to it using the cluster connect feature. Cluster connect allows you to securely access and manage your cluster from anywhere, making it convenient for interactive development, debugging, and cluster administration tasks.\n  Note\nWhen you create a Nexus Kubernetes cluster, Nexus automatically creates a managed resource group dedicated to storing the cluster resources, within this group, the Arc connected cluster resource is established.\nTo access your cluster, you need to set up the cluster connect \nkubeconfig\n . After logging\nkubeconfig\ninto Azure CLI with the relevant Azure AD entity, you can obtain the \nnecessary to communicate with the cluster from anywhere, even outside the firewall that surrounds it.\nCLUSTER_NAME\n , \nRESOURCE_GROUP\n  and \nSUBSCRIPTION_ID\n  variables. 1. Set \nBash\nCLUSTER_NAME= "myNexusAKSCluster" RESOURCE_GROUP= "myResourceGroup" SUBSCRIPTION_ID=< set  the correct subscription_id>\n2. Query managed resource group with \naz\n  and store in \nMANAGED_RESOURCE_GROUP\nAzure CLI\n az account set  -s  $SUBSCRIPTION_ID  MANAGED_RESOURCE_GROUP=$(az networkcloud kubernetescluster show  -n   $CLUSTER_NAME  -g  $RESOURCE_GROUP  --output  tsv  --query   managedResourceGroupConfiguration.name)\n3. The following command starts a connectedk8s proxy that allows you to connect to the Kubernetes API server for the specified Nexus Kubernetes cluster.\nAzure CLI\naz connectedk8s proxy  -n  $CLUSTER_NAME   -g  $MANAGED_RESOURCE_GROUP &\n4. Use \nkubectl\n  to send requests to the cluster:\nConsole\nkubectl get nodes\nYou should now see a response from the cluster containing the list of all nodes.\n  Note\nIf you see the error message "Failed to post access token to client proxyFailed to connect to MSI", you may need to perform an \naz login\n  to re-authenticate with\nAzure."}
{"text": "Add an agent pool\nThe cluster created in the previous step has a single node pool. Let's add a second agent pool using the ARM template. The following example creates an agent pool named\nmyNexusAKSCluster-nodepool-2\n :\n1. Review the template.\nBefore adding the agent pool template, let's review the content to understand its structure.\nJSON\n{      "$schema" :  "https://schema.management.azure.com/schemas/2019-04- 01/deploymentTemplate.json#" ,      "contentVersion" :  "1.0.0.0" ,      "parameters" : {        "kubernetesClusterName" : {          "type" :  "string" ,          "metadata" : {            "description" :  "The name of Nexus Kubernetes cluster"         }       },        "location" : {          "type" :  "string" ,          "defaultValue" :  "[resourceGroup().location]" ,          "metadata" : {\n           "description" :  "The Azure region where the cluster is to be  deployed"         }       },        "extendedLocation" : {          "type" :  "string" ,          "metadata" : {            "description" :  "The custom location of the Nexus instance"         }       },        "tags" : {          "type" :  "object" ,          "defaultValue" : {},          "metadata" : {            "description" :  "Tags to be associated with the resource"         }       },        "adminUsername" : {          "type" :  "string" ,          "defaultValue" :  "azureuser" ,          "metadata" : {            "description" :  "The username for the administrative account on the  cluster"         }       },        "sshPublicKey" : {          "type" :  "string" ,          "defaultValue" :  "" ,          "metadata" : {            "description" :  "The SSH public key that will be associated with  the 'azureuser' user for secure remote login"         }       },        "agentPoolNodeCount" : {          "type" :  "int" ,          "defaultValue" : 1,          "metadata" : {            "description" :  "Number of nodes in the agent pool"         }       },        "agentPoolName" : {          "type" :  "string" ,          "defaultValue" :  "nodepool-2" ,          "metadata" : {            "description" :  "Agent pool name"         }       },        "agentVmSku" : {          "type" :  "string" ,          "defaultValue" :  "NC_M4_v1" ,          "metadata" : {            "description" :  "VM size of the agent nodes"         }       },        "agentPoolZones" : {\n         "type" :  "array" ,          "defaultValue" : [],          "metadata" : {            "description" :  "The zones/racks used for placement of the agent  pool nodes"         }         /* array of strings Example: [ "1" ,  "2" ,  "3" ] */       },        "agentPoolMode" : {          "type" :  "string" ,          "defaultValue" :  "User" ,          "metadata" : {            "description" :  "Agent pool mode"         }       },        "agentOptions" : {          "type" :  "object" ,          "defaultValue" : {},          "metadata" : {            "description" :  "The configurations for the initial agent pool"         }         /*            "hugepagesCount" : int,            "hugepagesSize" :  "2M/1G"         */       },        "labels" : {          "type" :  "array" ,          "defaultValue" : [],          "metadata" : {            "description" :  "The labels to assign to the nodes in the cluster  for identification and organization"         }         /*           {              "key" :  "string" ,              "value" :  "string"           }         */       },        "taints" : {          "type" :  "array" ,          "defaultValue" : [],          "metadata" : {            "description" :  "The taints to apply to the nodes in the cluster to  restrict which pods can be scheduled on them"         }         /*           {              "key" :  "string" ,              "value" :  "string:NoSchedule|PreferNoSchedule|NoExecute"           }         */       },        "l2Networks" : {\n         "type" :  "array" ,          "defaultValue" : [],          "metadata" : {            "description" :  "The Layer 2 networks to connect to the agent pool"         }         /*           {              "networkId" :  "string" ,              "pluginType" :  "SRIOV|DPDK|OSDevice|MACVLAN|IPVLAN"           }         */       },        "l3Networks" : {          "type" :  "array" ,          "defaultValue" : [],          "metadata" : {            "description" :  "The Layer 3 networks to connect to the agent pool"         }         /*           {              "ipamEnabled" :  "True/False" ,              "networkId" :  "string" ,              "pluginType" :  "SRIOV|DPDK|OSDevice|MACVLAN|IPVLAN"           }         */       },        "trunkedNetworks" : {          "type" :  "array" ,          "defaultValue" : [],          "metadata" : {            "description" :  "The trunked networks to connect to the agent pool"         }         /*           {              "networkId" :  "string" ,              "pluginType" :  "SRIOV|DPDK|OSDevice|MACVLAN|IPVLAN"           }         */       }     },      "resources" : [       {          "type" :  "Microsoft.NetworkCloud/kubernetesClusters/agentpools" ,          "apiVersion" :  "2023-05-01-preview" ,          "name" :  "[concat(parameters('kubernetesClusterName'), '/',  parameters('kubernetesClusterName'), '-', parameters('agentPoolName'))]" ,          "location" :  "[parameters('location')]" ,          "extendedLocation" : {            "name" :  "[parameters('extendedLocation')]" ,            "type" :  "CustomLocation"         },          "properties" : {            "count" :  "[parameters('agentPoolNodeCount')]" ,            "mode" :  "[parameters('agentPoolMode')]" ,            "vmSkuName" :  "[parameters('agentVmSku')]" ,\n           "labels" :  "[if(empty(parameters('labels')), json('null'),  parameters('labels'))]" ,            "taints" :  "[if(empty(parameters('taints')), json('null'),  parameters('taints'))]" ,            "agentOptions" :  "[if(empty(parameters('agentOptions')),  json('null'), parameters('agentOptions'))]" ,            "attachedNetworkConfiguration" : {              "l2Networks" :  "[if(empty(parameters('l2Networks')),  json('null'), parameters('l2Networks'))]" ,              "l3Networks" :  "[if(empty(parameters('l3Networks')),  json('null'), parameters('l3Networks'))]" ,              "trunkedNetworks" :  "[if(empty(parameters('trunkedNetworks')),  json('null'), parameters('trunkedNetworks'))]"           },            "availabilityZones" :  "[if(empty(parameters('agentPoolZones')),  json('null'), parameters('agentPoolZones'))]" ,            "upgradeSettings" : {              "maxSurge" :  "1"           }         },          "dependsOn" : []       }     ] }\nkubernetes-add-\nOnce you have reviewed and saved the template file named \nagentpool.json\n , proceed to the next section to deploy the template.\nkubernetes-nodepool-parameters.json\n  and add the required 1. Create a file named \nparameters in JSON format. You can use the following example as a starting point. Replace the values with your own.\nJSON\n{      "$schema" :  "https://schema.management.azure.com/schemas/2019-04- 01/deploymentParameters.json#" ,      "contentVersion" :  "1.0.0.0" ,      "parameters" : {        "kubernetesClusterName" :{          "value" :  "myNexusAKSCluster"       },        "extendedLocation" : {          "value" :  "/subscriptions/<subscription_id>/resourceGroups/<resource_group>/providers/ microsoft.extendedlocation/customlocations/<custom-location-name>"       }     }   }\n2. Deploy the template.\nAzure CLI\n    az deployment group create \        --resource-group  myResourceGroup \        --template-file  kubernetes -add-agentpool .json \        --parameters  @kubernetes -nodepool-parameters .json\n  Note\nYou can add multiple agent pools during the initial creation of your cluster itself by using the initial agent pool configurations. However, if you want to add agent pools after the initial creation, you can utilize the above command to create additional agent pools for your Nexus Kubernetes cluster.\nThe following output example resembles successful creation of the agent pool.\nBash\n$ az networkcloud kubernetescluster agentpool list --kubernetes-cluster-name  myNexusAKSCluster --resource-group myResourceGroup --output table This  command  is experimental and under development. Reference and support  levels: https://aka.ms/CLI_refstatus Count    Location    Mode    Name                          ProvisioningState    ResourceGroup    VmSkuName -------  ----------  ------  ----------------------------  ----------------- --  ---------------  ----------- 1        eastus      System  myNexusAKSCluster-nodepool-1  Succeeded            myResourceGroup  NC_M4_v1 1        eastus      User    myNexusAKSCluster-nodepool-2  Succeeded            myResourceGroup  NC_M4_v1"}
{"text": "Clean up resources\nWhen no longer needed, delete the resource group. The resource group and all the resources in the resource group are deleted.\nAzure CLI\nUse the  az group delete  command to remove the resource group, Kubernetes cluster, and all related resources except the Operator Nexus network resources.\nAzure CLI\naz group delete  --name  myResourceGroup  --yes   --no-wait"}
{"text": "Next steps\nYou can now deploy the CNFs either directly via  cluster connect  or via Azure Operator Service Manager (AOSM)."}
{"text": "Quickstart: Deploy an Azure Nexus"}
{"text": "Kubernetes cluster using Bicep\nArticle  06/26/2023\nDeploy an Azure Nexus Kubernetes cluster using Bicep.\nBicep  is a domain-specific language (DSL) that uses declarative syntax to deploy Azure resources. It provides concise syntax, reliable type safety, and support for code reuse. Bicep offers the best authoring experience for your infrastructure-as-code solutions in Azure."}
{"text": "Prerequisites\nIf you don't have an  Azure subscription , create an  Azure free account  before you begin.\nUse the Bash environment in  Azure Cloud Shell . For more information, see Quickstart for Bash in Azure Cloud Shell .\nIf you prefer to run CLI reference commands locally,  install  the Azure CLI. If you're running on Windows or macOS, consider running Azure CLI in a Docker container. For more information, see  How to run the Azure CLI in a Docker container .\nIf you're using a local installation, sign in to the Azure CLI by using the  az login command. To finish the authentication process, follow the steps displayed in your terminal. For other sign-in options, see  Sign in with the Azure CLI .\nWhen you're prompted, install the Azure CLI extension on first use. For more information about extensions, see  Use extensions with the Azure CLI .\nRun  az version  to find the version and dependent libraries that are installed. To upgrade to the latest version, run  az upgrade .\nInstall the latest version of the  necessary Azure CLI extensions .\nThis article requires version 2.49.0 or later of the Azure CLI. If using Azure Cloud Shell, the latest version is already installed.\nIf you have multiple Azure subscriptions, select the appropriate subscription ID in which the resources should be billed using the  az account  command.\nCreate a resource group using the  az group create  command. An  Azure resource group  is a logical group in which Azure resources are deployed and managed. When you create a resource group, you're prompted to specify a location. This location is the storage location of your resource group metadata and where your resources run in Azure if you don't specify another region during resource creation. The following example creates a resource group named  myResourceGroup  in the eastus  location.\nAzure CLI\naz group create  --name  myResourceGroup  --location  eastus\nThe following output example resembles successful creation of the resource group:\nJSON\n{    "id" :  "/subscriptions/<guid>/resourceGroups/myResourceGroup" ,    "location" :  "eastus" ,    "managedBy" :  null ,    "name" :  "myResourceGroup" ,    "properties" : {      "provisioningState" :  "Succeeded"   },    "tags" :  null }\nTo deploy a Bicep file or ARM template, you need write access on the resources you're deploying and access to all operations on the Microsoft.Resources/deployments resource type. For example, to deploy a cluster, you need Microsoft.NetworkCloud/kubernetesclusters/write and Microsoft.Resources/deployments/* permissions. For a list of roles and permissions, see  Azure built-in roles .\nYou need the \ncustom location\n  resource ID of your Azure Operator Nexus cluster.\nYou need to create  various networks  according to your specific workload requirements, and it's essential to have the appropriate IP addresses available for your workloads. To ensure a smooth implementation, it's advisable to consult the relevant support teams for assistance.\nThis quickstart assumes a basic understanding of Kubernetes concepts. For more information, see  Kubernetes core concepts for Azure Kubernetes Service (AKS) ."}
{"text": "Review the Bicep file\nBefore deploying the Kubernetes template, let's review the content to understand its structure.\nBicep\n// Azure parameters\n@description( 'The name of Nexus Kubernetes cluster' ) param  kubernetesClusterName string\n@description( 'The Azure region where the cluster is to be deployed' ) param  location string = resourceGroup().location\n@description( 'The custom location of the Nexus instance' ) param  extendedLocation string\n@description( 'The metadata tags to be associated with the cluster  resource ' ) param  tags object = {}\n@description( 'The username  for  the administrative account on the cluster' ) param  adminUsername string =  'azureuser'\n@description( 'The object IDs of Azure Active Directory (AAD) groups that  will have administrative access to the cluster' ) param  adminGroupObjectIds array = []\n// Networking Parameters\n@description( 'The Azure  Resource  Manager (ARM) id of the network to be used  as the Container Networking Interface (CNI) network' ) param  cniNetworkId string\n@description( 'The ARM id of the network to be used  for  cloud services  network' ) param  cloudServicesNetworkId string\n@description( 'The CIDR blocks used  for  Nexus Kubernetes PODs  in  the  cluster' ) param  podCidrs array = [ '10.244.0.0/16' ]\n@description( 'The CIDR blocks used  for  k8s service  in  the cluster' ) param  serviceCidrs array = [ '10.96.0.0/16' ]\n@description( 'The IP address of the DNS service  in  the cluster' ) param  dnsServiceIp string =  '10.96.0.10'\n@description( 'The Layer 2 networks associated with the initial agent pool' ) param  agentPoolL2Networks array = [] // { //   networkId: 'string' //   pluginType: 'SRIOV|DPDK|OSDevice|MACVLAN|IPVLAN'\n// }\n@description( 'The Layer 3 networks associated with the initial agent pool' ) param  agentPoolL3Networks array = [] // { //   ipamEnabled: 'True/False' //   networkId: 'string' //   pluginType: 'SRIOV|DPDK|OSDevice|MACVLAN|IPVLAN' // }\n@description( 'The trunked networks associated with the initial agent pool' ) param  agentPoolTrunkedNetworks array = [] // { //   networkId: 'string' //   pluginType: 'SRIOV|DPDK|OSDevice|MACVLAN|IPVLAN' // }\n@description( 'The Layer 2 networks associated with the cluster' ) param  l2Networks array = [] // { //   networkId: 'string' //   pluginType: 'SRIOV|DPDK|OSDevice|MACVLAN|IPVLAN' // }\n@description( 'The Layer 3 networks associated with the cluster' ) param  l3Networks array = [] // { //   ipamEnabled: 'True/False' //   networkId: 'string' //   pluginType: 'SRIOV|DPDK|OSDevice|MACVLAN|IPVLAN' // }\n@description( 'The trunked networks associated with the cluster' ) param  trunkedNetworks array = [] // { //   networkId: 'string' //   pluginType: 'SRIOV|DPDK|OSDevice|MACVLAN|IPVLAN' // }\n@description( 'The LoadBalancer IP address pools associated with the  cluster' ) param  ipAddressPools array = [] // { //   addresses: [ //     'string' //   ] //   autoAssign: 'True/False' //   name: 'string' //   onlyUseHostIps: 'True/False' // }\n// Cluster Configuration Parameters\n@description( 'The version of Kubernetes to be used  in  the Nexus Kubernetes  cluster' )\nparam  kubernetesVersion string =  'v1.24.9'\n@description( 'The number of control plane nodes to be deployed  in  the  cluster' ) param  controlPlaneCount int = 1\n@description( 'The zones/racks used  for  placement of the control plane  nodes' ) param  controlPlaneZones array = [] // "string" Example: ["1", "2", "3"]\n@description( 'The zones/racks used  for  placement of the agent pool nodes' ) param  agentPoolZones array = [] // "string" Example: ["1", "2", "3"]\n@description( 'The size of the control plane nodes' ) param  controlPlaneVmSkuName string =  'NC_G2_v1'\n@description( 'The number of worker nodes to be deployed  in  the initial agent  pool' ) param  systemPoolNodeCount int = 1\n@description( 'The size of the worker nodes' ) param  workerVmSkuName string =  'NC_M4_v1'\n@description( 'The configurations  for  the initial agent pool' ) param  initialPoolAgentOptions object = {} // { //   "hugepagesCount": integer, //   "hugepagesSize": "2M/1G" // }\n@description( 'The SSH public key that will be associated with the  "azureuser" user  for  secure remote login' ) param  sshPublicKey string =  ''\n@description( 'The labels to assign to the nodes  in  the cluster  for   identification and organization' ) param  labels array = [] // { //   key: 'string' //   value: 'string' // } @description( 'The taints to apply to the nodes  in  the cluster to restrict  which pods can be scheduled on them' ) param  taints array = [] // { //   key: 'string' //   value: 'string:NoSchedule|PreferNoSchedule|NoExecute' // }\nresource  kubernetescluster  'Microsoft.NetworkCloud/kubernetesClusters@2023- 05-01-preview'  = {   name: kubernetesClusterName   location: location\n  tags: tags   extendedLocation: {     name: extendedLocation     type:  'CustomLocation'   }   properties: {     kubernetesVersion: kubernetesVersion     managedResourceGroupConfiguration: {       name:  ' ${uniqueString(resourceGroup().name)} - ${kubernetesClusterName} '       location: location     }     aadConfiguration: {       adminGroupObjectIds: adminGroupObjectIds     }     administratorConfiguration: {       adminUsername: adminUsername       sshPublicKeys: [         {           keyData: sshPublicKey         }       ]     }     initialAgentPoolConfigurations: [       {         name:  ' ${kubernetesClusterName} -nodepool-1'         count: systemPoolNodeCount         vmSkuName: workerVmSkuName         mode:  'System'         labels: empty(labels) ?  null  : labels         taints: empty(taints) ?  null  : taints         agentOptions: empty(initialPoolAgentOptions) ?  null  :  initialPoolAgentOptions         attachedNetworkConfiguration: {           l2Networks: empty(agentPoolL2Networks) ?  null  :  agentPoolL2Networks           l3Networks: empty(agentPoolL3Networks) ?  null  :  agentPoolL3Networks           trunkedNetworks: empty(agentPoolTrunkedNetworks) ?  null  :  agentPoolTrunkedNetworks         }         availabilityZones: empty(agentPoolZones) ?  null  : agentPoolZones         upgradeSettings: {           maxSurge:  '1'         }       }     ]     controlPlaneNodeConfiguration: {       count: controlPlaneCount       vmSkuName: controlPlaneVmSkuName       availabilityZones: empty(controlPlaneZones) ?  null  : controlPlaneZones     }     networkConfiguration: {       cniNetworkId: cniNetworkId       cloudServicesNetworkId: cloudServicesNetworkId       dnsServiceIp: dnsServiceIp\n      podCidrs: podCidrs       serviceCidrs: serviceCidrs       attachedNetworkConfiguration: {         l2Networks: empty(l2Networks) ?  null  : l2Networks         l3Networks: empty(l3Networks) ?  null  : l3Networks         trunkedNetworks: empty(trunkedNetworks) ?  null  : trunkedNetworks       }       bgpServiceLoadBalancerConfiguration: {         ipAddressPools: empty(ipAddressPools) ?  null  : ipAddressPools       }     }   } }\nkubernetes-deploy.bicep\n , Once you have reviewed and saved the template file named \nproceed to the next section to deploy the template."}
{"text": "Deploy the Bicep file\nkubernetes-deploy-parameters.json\n  and add the required 1. Create a file named \nparameters in JSON format. You can use the following example as a starting point. Replace the values with your own.\nJSON\n{    "$schema" :  "https://schema.management.azure.com/schemas/2019-04- 01/deploymentParameters.json#" ,    "contentVersion" :  "1.0.0.0" ,    "parameters" : {      "kubernetesClusterName" :{        "value" :  "myNexusAKSCluster"     },      "adminGroupObjectIds" : {        "value" : [          "00000000-0000-0000-0000-000000000000"       ]     },      "cniNetworkId" : {        "value" :  "/subscriptions/<subscription_id>/resourceGroups/<resource_group>/providers/ Microsoft.NetworkCloud/l3Networks/<l3Network-name>"     },      "cloudServicesNetworkId" : {        "value" :  "/subscriptions/<subscription_id>/resourceGroups/<resource_group>/providers/ Microsoft.NetworkCloud/cloudServicesNetworks/<csn-name>"     },      "extendedLocation" : {        "value" : \n"/subscriptions/<subscription_id>/resourceGroups/<managed_resource_group>/pr oviders/microsoft.extendedlocation/customlocations/<custom-location-name>"     },      "location" : {        "value" :  "eastus"     },      "sshPublicKey" : {        "value" :  "ssh-rsa AAAAB...."     }   } }\n2. Deploy the template.\nAzure CLI\n    az deployment group create \        --resource-group  myResourceGroup \        --template-file  kubernetes -deploy .bicep \        --parameters  @kubernetes -deploy-parameters .json"}
{"text": "Review deployed resources\nAfter the deployment finishes, you can view the resources using the CLI or the Azure portal.\nTo view the details of the \nmyNexusAKSCluster\n  cluster in the \nmyResourceGroup\n  resource\ngroup, execute the following Azure CLI command:\nAzure CLI\naz networkcloud kubernetescluster show  \    --name  myNexusAKSCluster \    --resource-group  myResourceGroup\nAdditionally, to get a list of agent pool names associated with the \nmyNexusAKSCluster\nmyResourceGroup\n  resource group, you can use the following Azure CLI cluster in the \ncommand.\nAzure CLI\naz networkcloud kubernetescluster agentpool list  \    --kubernetes-cluster-name  myNexusAKSCluster \    --resource-group  myResourceGroup \    --output  table"}
{"text": "Connect to the cluster\nNow that the Nexus Kubernetes cluster has been successfully created and connected to Azure Arc, you can easily connect to it using the cluster connect feature. Cluster connect allows you to securely access and manage your cluster from anywhere, making it convenient for interactive development, debugging, and cluster administration tasks.\n  Note\nWhen you create a Nexus Kubernetes cluster, Nexus automatically creates a managed resource group dedicated to storing the cluster resources, within this group, the Arc connected cluster resource is established.\nTo access your cluster, you need to set up the cluster connect \nkubeconfig\n . After logging\ninto Azure CLI with the relevant Azure AD entity, you can obtain the \nkubeconfig\nnecessary to communicate with the cluster from anywhere, even outside the firewall that surrounds it.\nCLUSTER_NAME\n , \nRESOURCE_GROUP\n  and \nSUBSCRIPTION_ID\n  variables. 1. Set \nBash\nCLUSTER_NAME= "myNexusAKSCluster" RESOURCE_GROUP= "myResourceGroup" SUBSCRIPTION_ID=< set  the correct subscription_id>\n2. Query managed resource group with \naz\n  and store in \nMANAGED_RESOURCE_GROUP\nAzure CLI\n az account set  -s  $SUBSCRIPTION_ID  MANAGED_RESOURCE_GROUP=$(az networkcloud kubernetescluster show  -n   $CLUSTER_NAME  -g  $RESOURCE_GROUP  --output  tsv  --query   managedResourceGroupConfiguration.name)\n3. The following command starts a connectedk8s proxy that allows you to connect to the Kubernetes API server for the specified Nexus Kubernetes cluster.\nAzure CLI\naz connectedk8s proxy  -n  $CLUSTER_NAME   -g  $MANAGED_RESOURCE_GROUP &\n4. Use \nkubectl\n  to send requests to the cluster:\nConsole\nkubectl get nodes\nYou should now see a response from the cluster containing the list of all nodes.\n  Note\nIf you see the error message "Failed to post access token to client proxyFailed to\naz login\n  to re-authenticate with connect to MSI", you may need to perform an \nAzure."}
{"text": "Add an agent pool\nThe cluster created in the previous step has a single node pool. Let's add a second agent pool using the Bicep template. The following example creates an agent pool named\nmyNexusAKSCluster-nodepool-2\n :\n1. Review the template.\nBefore adding the agent pool template, let's review the content to understand its structure.\nBicep\n// Azure Parameters @description( 'The name of Nexus Kubernetes cluster' ) param  kubernetesClusterName string\n@description( 'The Azure region where the cluster is to be deployed' ) param  location string = resourceGroup().location\n@description( 'The custom location of the Nexus instance' ) param  extendedLocation string\n@description( 'Tags to be associated with the  resource ' ) param  tags object = {}\n@description( 'The username  for  the administrative account on the cluster' ) param  adminUsername string =  'azureuser'\n@description( 'The SSH public key that will be associated with the  "azureuser" user  for  secure remote login' ) param  sshPublicKey string =  ''\n// Cluster Configuration Parameters\n@description( 'Number of nodes  in  the agent pool' ) param  agentPoolNodeCount int = 1\n@description( 'Agent pool name' ) param  agentPoolName string =  'nodepool-2'\n@description( 'VM size of the agent nodes' ) param  agentVmSku string =  'NC_M4_v1'\n@description( 'The zones/racks used  for  placement of the agent pool nodes' ) param  agentPoolZones array = [] // "string" Example: ["1", "2", "3"]\n@description( 'Agent pool mode' ) param  agentPoolMode string =  'User'\n@description( 'The configurations  for  the initial agent pool' ) param  agentOptions object = {} // { //   "hugepagesCount": integer, //   "hugepagesSize": "2M/1G" // }\n@description( 'The labels to assign to the nodes  in  the cluster  for   identification and organization' ) param  labels array = [] // { //   key: 'string' //   value: 'string' // } @description( 'The taints to apply to the nodes  in  the cluster to restrict  which pods can be scheduled on them' ) param  taints array = [] // { //   key: 'string' //   value: 'string:NoSchedule|PreferNoSchedule|NoExecute' // }\n// Networking Parameters @description( 'The Layer 2 networks to connect to the agent pool' ) param  l2Networks array = [] // { //   networkId: 'string' //   pluginType: 'SRIOV|DPDK|OSDevice|MACVLAN|IPVLAN' // }\n@description( 'The Layer 3 networks to connect to the agent pool' ) param  l3Networks array = [] // { //   ipamEnabled: 'True/False' //   networkId: 'string' //   pluginType: 'SRIOV|DPDK|OSDevice|MACVLAN|IPVLAN' // }\n@description( 'The trunked networks to connect to the agent pool' )\nparam  trunkedNetworks array = [] // { //   networkId: 'string' //   pluginType: 'SRIOV|DPDK|OSDevice|MACVLAN|IPVLAN' // }\nresource  agentPools  'Microsoft.NetworkCloud/kubernetesClusters/agentPools@2023-05-01-preview'  =  {   name:  ' ${kubernetesClusterName} / ${kubernetesClusterName} - ${agentPoolName} '   location: location   tags: tags   extendedLocation: {     name: extendedLocation     type:  'CustomLocation'   }   properties: {     administratorConfiguration: sshPublicKey !=  ''  ? {       adminUsername: adminUsername       sshPublicKeys: [         {           keyData: sshPublicKey         }       ]     }:  null     attachedNetworkConfiguration: {       l2Networks: empty(l2Networks) ?  null  : l2Networks       l3Networks: empty(l3Networks) ?  null  : l3Networks       trunkedNetworks: empty(trunkedNetworks) ?  null  : trunkedNetworks     }     count: agentPoolNodeCount     mode: agentPoolMode     vmSkuName: agentVmSku     labels: empty(labels) ?  null  : labels     taints: empty(taints) ?  null  : taints     agentOptions: empty(agentOptions) ?  null  : agentOptions     availabilityZones: empty(agentPoolZones) ?  null  : agentPoolZones     upgradeSettings: {       maxSurge:  '1'     }   } }\nOnce you have reviewed and saved the template file named \nkubernetes-add-\nagentpool.bicep\n , proceed to the next section to deploy the template.\nkubernetes-nodepool-parameters.json\n  and add the required 1. Create a file named \nparameters in JSON format. You can use the following example as a starting point. Replace the values with your own.\nJSON\n{      "$schema" :  "https://schema.management.azure.com/schemas/2019-04- 01/deploymentParameters.json#" ,      "contentVersion" :  "1.0.0.0" ,      "parameters" : {        "kubernetesClusterName" :{          "value" :  "myNexusAKSCluster"       },        "extendedLocation" : {          "value" :  "/subscriptions/<subscription_id>/resourceGroups/<resource_group>/providers/ microsoft.extendedlocation/customlocations/<custom-location-name>"       }     }   }\n2. Deploy the template.\nAzure CLI\n    az deployment group create \        --resource-group  myResourceGroup \        --template-file  kubernetes -add-agentpool .bicep \        --parameters  @kubernetes -nodepool-parameters .json\n  Note\nYou can add multiple agent pools during the initial creation of your cluster itself by using the initial agent pool configurations. However, if you want to add agent pools after the initial creation, you can utilize the above command to create additional agent pools for your Nexus Kubernetes cluster.\nThe following output example resembles successful creation of the agent pool.\nBash\n$ az networkcloud kubernetescluster agentpool list --kubernetes-cluster-name  myNexusAKSCluster --resource-group myResourceGroup --output table This  command  is experimental and under development. Reference and support  levels: https://aka.ms/CLI_refstatus Count    Location    Mode    Name                          ProvisioningState    ResourceGroup    VmSkuName -------  ----------  ------  ----------------------------  ----------------- --  ---------------  ----------- 1        eastus      System  myNexusAKSCluster-nodepool-1  Succeeded            myResourceGroup  NC_M4_v1\n1        eastus      User    myNexusAKSCluster-nodepool-2  Succeeded            myResourceGroup  NC_M4_v1"}
{"text": "Clean up resources\nWhen no longer needed, delete the resource group. The resource group and all the resources in the resource group are deleted.\nAzure CLI\nUse the  az group delete  command to remove the resource group, Kubernetes cluster, and all related resources except the Operator Nexus network resources.\nAzure CLI\naz group delete  --name  myResourceGroup  --yes   --no-wait"}
{"text": "Next steps\nYou can now deploy the CNFs either directly via  cluster connect  or via Azure Operator Service Manager (AOSM)."}
{"text": "Quickstart: Create an Azure Operator"}
{"text": "Nexus virtual machine by using Azure"}
{"text": "CLI\nArticle  07/12/2023\nDeploy an Azure Nexus virtual machine using Azure CLI\nThis quick-start guide is designed to help you get started with using Nexus virtual machines to host virtual network functions (VNFs). By following the steps outlined in this guide, you're able to quickly and easily create a customized Nexus virtual machine that meets your specific needs and requirements. Whether you're a beginner or an expert in Nexus networking, this guide is here to help. You learn everything you need to know to create and customize Nexus virtual machines for hosting virtual network functions."}
{"text": "Before you begin\nIf you don't have an  Azure subscription , create an  Azure free account  before you begin.\nUse the Bash environment in  Azure Cloud Shell . For more information, see Quickstart for Bash in Azure Cloud Shell .\nIf you prefer to run CLI reference commands locally,  install  the Azure CLI. If you're running on Windows or macOS, consider running Azure CLI in a Docker container. For more information, see  How to run the Azure CLI in a Docker container .\nIf you're using a local installation, sign in to the Azure CLI by using the  az login command. To finish the authentication process, follow the steps displayed in your terminal. For other sign-in options, see  Sign in with the Azure CLI .\nWhen you're prompted, install the Azure CLI extension on first use. For more information about extensions, see  Use extensions with the Azure CLI .\nRun  az version  to find the version and dependent libraries that are installed. To upgrade to the latest version, run  az upgrade .\nInstall the latest version of the  necessary Azure CLI extensions .\nThis article requires version 2.49.0 or later of the Azure CLI. If using Azure Cloud Shell, the latest version is already installed.\nIf you have multiple Azure subscriptions, select the appropriate subscription ID in which the resources should be billed using the  az account  command.\nBefore proceeding with virtual machine creation, ensure that the container image to be used is created according to the  instructions .\nCreate a resource group using the  az group create  command. An  Azure resource group  is a logical group in which Azure resources are deployed and managed. When you create a resource group, you're prompted to specify a location. This location is the storage location of your resource group metadata and where your resources run in Azure if you don't specify another region during resource creation. The following example creates a resource group named  myResourceGroup  in the eastus  location.\nAzure CLI\naz group create  --name  myResourceGroup  --location  eastus\nThe following output example resembles successful creation of the resource group:\nJSON\n{    "id" :  "/subscriptions/<guid>/resourceGroups/myResourceGroup" ,    "location" :  "eastus" ,    "managedBy" :  null ,    "name" :  "myResourceGroup" ,    "properties" : {      "provisioningState" :  "Succeeded"   },    "tags" :  null }\nTo deploy a Bicep file or ARM template, you need write access on the resources you're deploying and access to all operations on the Microsoft.Resources/deployments resource type. For example, to deploy a cluster, you need Microsoft.NetworkCloud/virtualMachines/write and Microsoft.Resources/deployments/* permissions. For a list of roles and permissions, see  Azure built-in roles .\ncustom location\n  resource ID of your Azure Operator Nexus cluster. You need the \nYou need to create  various networks  according to your specific workload requirements, and it's essential to have the appropriate IP addresses available for your workloads. To ensure a smooth implementation, it's advisable to consult the relevant support teams for assistance.\nComplete the  prerequisites  for deploying a Nexus virtual machine."}
{"text": "Create a Nexus virtual machine\nThe following example creates a virtual machine named  myNexusVirtualMachine  in resource group  myResourceGroup  in the  eastus  location.\nBefore you run the commands, you need to set several variables to define the configuration for your virtual machine. Here are the variables you need to set, along with some default values you can use for certain variables:\nVariable Description\nLOCATION The Azure region where you want to create your virtual machine.\nRESOURCE_GROUP The name of the Azure resource group where you want to create the virtual machine.\nSUBSCRIPTION The ID of your Azure subscription.\nCUSTOM_LOCATION This argument specifies a custom location of the Nexus instance.\nCSN_ARM_ID CSN ID is the unique identifier for the cloud services network you want to use.\nL3_NETWORK_ID L3 Network ID is the unique identifier for the network interface to be used by the virtual machine.\nNETWORK_INTERFACE_NAME The name of the L3 network interface for the virtual machine.\nADMIN_USERNAME The username for the virtual machine administrator.\nSSH_PUBLIC_KEY The SSH public key that is used for secure communication with the virtual machine.\nCPU_CORES The number of CPU cores for the virtual machine (even number, max 44 vCPUs)\nMEMORY_SIZE The amount of memory (in GB, max 224 GB) for the virtual machine.\nVM_DISK_SIZE The size (in GB) of the virtual machine disk.\nVM_IMAGE The URL of the virtual machine image.\nVariable Description\nACR_URL The URL of the Azure Container Registry.\nACR_USERNAME The username for the Azure Container Registry.\nACR_PASSWORD The password for the Azure Container Registry.\nOnce you've defined these variables, you can run the Azure CLI command to create the\n--debug\n  flag at the end to provide more detailed output for virtual machine. Add the \ntroubleshooting purposes.\nTo define these variables, use the following set commands and replace the example values with your preferred values. You can also use the default values for some of the variables, as shown in the following example:\nBash\n# Azure parameters RESOURCE_GROUP= "myResourceGroup" SUBSCRIPTION= "$(az account show -o tsv --query id)" CUSTOM_LOCATION= "/subscriptions/<subscription_id>/resourceGroups/<managed_re source_group>/providers/microsoft.extendedlocation/customlocations/<custom- location-name>" LOCATION= "$(az group show --name $RESOURCE_GROUP --query location | tr -d  '\"')"\n# VM parameters VM_NAME= "myNexusVirtualMachine"\n# VM credentials ADMIN_USERNAME= "azureuser" SSH_PUBLIC_KEY= "$(cat ~/.ssh/id_rsa.pub)"\n# Network parameters CSN_ARM_ID= "/subscriptions/<subscription_id>/resourceGroups/<resource_group> /providers/Microsoft.NetworkCloud/cloudServicesNetworks/<csn-name>" L3_NETWORK_ID= "/subscriptions/<subscription_id>/resourceGroups/<resource_gro up>/providers/Microsoft.NetworkCloud/l3Networks/<l3Network-name>" NETWORK_INTERFACE_NAME= "mgmt0"\n# VM Size parameters CPU_CORES=4 MEMORY_SIZE=12 VM_DISK_SIZE= "64"\n# Virtual Machine Image parameters VM_IMAGE= "<VM image, example: myacr.azurecr.io/ubuntu:20.04>" ACR_URL= "<Azure container registry URL, example: myacr.azurecr.io>"\nACR_USERNAME= "<Azure container registry username>" ACR_PASSWORD= "<Azure container registry password>"\n  Important\nIt is essential that you replace the placeholders for CUSTOM_LOCATION, CSN_ARM_ID, L3_NETWORK_ID and ACR parameters with your actual values before running these commands.\nAfter defining these variables, you can create the virtual machine by executing the following Azure CLI command.\nBash\naz networkcloud virtualmachine create \     --name  "$VM_NAME"  \     --resource-group  "$RESOURCE_GROUP"  \     --subscription  "$SUBSCRIPTION"  \     --extended-location name= "$CUSTOM_LOCATION"   type = "CustomLocation"  \     --location  "$LOCATION"  \     --admin-username  "$ADMIN_USERNAME"  \     --csn  "attached-network-id=$CSN_ARM_ID"  \     --cpu-cores $CPU_CORES \     --memory-size $MEMORY_SIZE \     --network-attachments  '[{"attachedNetworkId":"' $L3_NETWORK_ID '","ipAllocationMethod":"Dynamic","de faultGateway":"True","networkAttachmentName":"' $NETWORK_INTERFACE_NAME '"}]' \     --storage-profile create-option= "Ephemeral"  delete-option= "Delete"  disk- size= "$VM_DISK_SIZE"  \     --vm-image  "$VM_IMAGE"  \     --ssh-key-values  "$SSH_PUBLIC_KEY"  \     --vm-image-repository-credentials registry-url= "$ACR_URL"   username= "$ACR_USERNAME"  password= "$ACR_PASSWORD"\nAfter a few minutes, the command completes and returns information about the virtual machine. You've created the virtual machine. You're now ready to use them.\n  Note\nIf each server has two CPU chipsets and each CPU chip has 28 cores, then with hyperthreading enabled (default), the CPU chip supports 56 vCPUs. With 8 vCPUs in each chip reserved for infrastructure (OS and agents), the remaining 48 are available for tenant workloads."}
{"text": "Review deployed resources\nAfter the deployment finishes, you can view the resources using the CLI or the Azure portal.\nmyNexusVirtualMachine\n  cluster in the \nmyResourceGroup\nTo view the details of the \nresource group, execute the following Azure CLI command:\nAzure CLI\naz networkcloud virtualmachine show  \    --name  myNexusVirtualMachine \    --resource-group  myResourceGroup"}
{"text": "Clean up resources\nWhen no longer needed, delete the resource group. The resource group and all the resources in the resource group are deleted.\nAzure CLI\nUse the  az group delete  command to remove the resource group, virtual machine, and all related resources except the Operator Nexus network resources.\nAzure CLI\naz group delete  --name  myResourceGroup  --yes   --no-wait"}
{"text": "Next steps\nYou've successfully created a Nexus virtual machine. You can now use the virtual machine to host virtual network functions (VNFs)."}
{"text": "Operator Nexus Azure resources"}
{"text": "prerequisites\nArticle  07/14/2023\nTo get started with Operator Nexus, you need to create a Network Fabric Controller (NFC) and then a Cluster Manager (CM) in your target Azure region.\nEach NFC is associated with a CM in the same Azure region and your subscription. The NFC/CM pair lifecycle manages up to 32 Azure Operator Nexus instances deployed in your sites connected to this Azure region.\nYou'll need to complete the prerequisites before you can deploy the Operator Nexus first NFC and CM pair. In subsequent deployments of Operator Nexus, you can skip to creating the NFC and CM."}
{"text": "Resource Provider Registration\nEnsure Azure Subscription for Operator Nexus resources has been permitted access to the necessary Azure Resource Providers: Microsoft.NetworkCloud Microsoft.ManagedNetworkFabric Microsoft.HybridContainerService Microsoft.HybridNetwork Microsoft.Storage Microsoft.Keyvault Microsoft.Network Microsoft.ExtendedLocation Microsoft.HybridCompute Microsoft.HybridConnectivity Microsoft.HybridContainerService Microsoft.Insights Microsoft.Kubernetes Microsoft.KubernetesConfiguration Microsoft.OperationalInsights Microsoft.OperationsManagement Microsoft.ResourceConnector Microsoft.Resources"}
{"text": "Dependent Azure resources setup\nEstablish  ExpressRoute  connectivity from your on-premises network to an Azure Region: ExpressRoute circuit  creation and verification  can be performed via the Azure portal In the ExpressRoute blade, ensure Circuit status indicates the status of the circuit on the Microsoft side. Provider status indicates if the circuit has been provisioned or not provisioned on the service-provider side. For an ExpressRoute circuit to be operational, Circuit status must be Enabled, and Provider status must be Provisioned Set up Azure Key Vault to store encryption and security tokens, service principals, passwords, certificates, and API keys Set up Log Analytics WorkSpace (LAW) to store logs and analytics data for Operator Nexus subcomponents (Network Fabric, Cluster, etc.) Set up Azure Storage account to store Operator Nexus data objects: Azure Storage supports blobs and files accessible from anywhere in the world over HTTP or HTTPS this storage isn't for user/consumer data."}
{"text": "Install CLI Extensions and sign-in to your Azure"}
{"text": "subscription\nInstall latest version of the  necessary CLI extensions ."}
{"text": "Azure subscription sign-in\nAzure CLI\n  az login   az account set  --subscription  $SUBSCRIPTION_ID   az account show\n  Note\nThe account must have permissions to read/write/publish in the subscription"}
{"text": "Create steps\nStep 1:  Create Network Fabric Controller Step 2:  Create Cluster Manager"}
{"text": "Create and modify a network fabric controller by using the"}
{"text": "Azure CLI\nArticle  05/24/2023\nThis article describes how to create a network fabric controller (NFC) for Azure Operator Nexus by using the Azure CLI. This article also shows you how to check the status of and delete an NFC."}
{"text": "Prerequisites\nValidate Azure ExpressRoute circuits for correct connectivity (\nCircuitId\n and \nAuthId\n). NFC provisioning will fail if connectivity is incorrect. Make sure that names, such as for resources, don't contain the underscore (_) character."}
{"text": "Parameters for NFC creation\nParameter Description Values Example Required Type\nResource-\nA resource\nNFCResourceGroupName XYZNFCResourceGroupName\nTrue String group is a\nGroup\ncontainer that holds related resources for an Azure solution.\nLocation\nThe Azure\neastus\n, \nwestus3 eastus\nTrue String region is mandatory to provision your deployment.\nResource-\nThe resource\nnfcname XYZnfcname\nTrue String name is the\nName\nname of the fabric.\nNFC IP\nThis block is\n10.0.0.0/19 10.0.0.0/19\nNot String the NFC IP required\nBlock\nsubnet. The default subnet block is 10.0.0.0/19, and it shouldn't overlap with any of the ExpressRoute IPs.\nParameter Description Values Example Required Type\nExpress\nThe\n--workload-er-connections subscriptions/xxxxxx-xxxxxx-xxxx-xxxx-\nTrue String ExpressRoute\nRoute '[{"expressRouteCircuitId": xxxxxx/resourceGroups/ER-Dedicated-WUS2-AFO-\ncircuit is a\nCircuits "xxxxxx-xxxxxx-xxxx-xxxx- Circuits/providers/Microsoft.Network/expressRouteCircuits/MSFT-\ndedicated\nxxxxxx", ER-Dedicated-PvtPeering-WestUS2-AFO-Ckt-01",\n10G link that\n"expressRouteAuthorizationKey": "expressRouteAuthorizationKey": "xxxxxx-xxxxxx-xxxx-xxxx-\nconnects\n"xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx"}]\nAzure and on-\nxxxxxx"}]'\n   premises. You need to know\n--infra-er-connections\nthe\n'[{"expressRouteCircuitId":\nExpressRoute\n"xxxxxx-xxxxxx-xxxx-xxxx-\ncircuit ID and\nxxxxxx",\nauthentication\n"expressRouteAuthorizationKey":\nkey to\n"xxxxxx-xxxxxx-xxxx-xxxx-\nsuccessfully\nxxxxxx"}]'\nprovision an NFC. There are two ExpressRoute circuits: one for the infrastructure services and one for workload (tenant) services."}
{"text": "Create a network fabric controller\nYou must create a resource group before you create your NFC. Create a separate resource group for each NFC.\nYou create a resource group by running the following command:\nAzure CLI\naz group create  -n  NFCResourceGroupName  -l   "East US"  \nHere's an example of how you can create an NFC by using the Azure CLI:\nAzure CLI\naz nf controller create  \     --resource-group   "NFCResourceGroupName"  \     --location   "eastus"   \     --resource-name   "nfcname"  \     --ipv 4 -address-space   "10.0.0.0/19"  \     --infra-er-connections   '[{"expressRouteCircuitId": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/ER-Dedicated-WUS2-AFO-Circuits/providers/Microsoft.Network/expressRouteCircuits/MSFT-ER- Dedicated-PvtPeering-WestUS2-AFO-Ckt-01", "expressRouteAuthorizationKey": "<auth-key>"}]'      --workload-er-connections   '[{"expressRouteCircuitId": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/ER-Dedicated-WUS2-AFO-Circuits/providers/Microsoft.Network/expressRouteCircuits/MSFT-ER- Dedicated-PvtPeering-WestUS2-AFO-Ckt-01"", "expressRouteAuthorizationKey": "<auth-key>"}]'  \nExpected output:\nJSON\n  "annotation" :  null ,     "id" :  "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/NFCResourceGroupName/providers/Microsoft.ManagedNetworkFabric/networkFabricControllers/nfcna me" ,     "infrastructureExpressRouteConnections" : [      {         "expressRouteAuthorizationKey" :  null ,         "expressRouteCircuitId" :  "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx-xxxxxx/resourceGroups/ER-Dedicated-WUS2-\nAFO-Circuits/providers/Microsoft.Network/expressRouteCircuits/MSFT-ER-Dedicated-PvtPeering-WestUS2-AFO-Ckt-01"       }    ],     "infrastructureServices" :  null ,     "ipv4AddressSpace" :  "10.0.0.0/19" ,     "ipv6AddressSpace" :  null ,     "location" :  "eastus" ,     "managedResourceGroupConfiguration" : {       "location" :  "eastus2euap" ,       "name" :  "nfcname-HostedResources-7DE8EEC1"     },     "name" :  "nfcname" ,     "networkFabricIds" :  null ,     "operationalState" :  null ,     "provisioningState" :  "Accepted" ,     "resourceGroup" :  "NFCresourcegroupname" ,     "systemData" : {       "createdAt" :  "2022-10-31T10:47:08.072025+00:00" ,       "createdBy" :  "email@address.com" ,       "createdByType" :  "User" ,       "lastModifiedAt" :  "2022-10-31T10:47:08.072025+00:00" ,       "lastModifiedBy" :  "email@address.com" , \nNFC creation takes 30 to 45 minutes. Use the \nshow\n command to monitor the progress. Provisioning states include \nAccepted\n,\nUpdating\n, \nSucceeded\n, and \nFailed\n. Delete and re-create the NFC if the creation fails (\nFailed\n)."}
{"text": "Get a network fabric controller\nAzure CLI\n  az nf controller show  --resource-group   "NFCResourceGroupName"   --resource-name   "nfcname"\nExpected output:\nJSON\n{     "annotation" :  null ,     "id" :  "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/NFCResourceGroupName/providers/Microsoft.ManagedNetworkFabric/networkFabricControllers/nfcna me" ,     "infrastructureExpressRouteConnections" : [      {         "expressRouteAuthorizationKey" :  null ,         "expressRouteCircuitId" :  "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx-xxxxxx/resourceGroups/ER-Dedicated-WUS2- AFO-Circuits/providers/Microsoft.Network/expressRouteCircuits/MSFT-ER-Dedicated-PvtPeering-WestUS2-AFO-Ckt-02"       }    ],     "infrastructureServices" : {       "ipv4AddressSpaces" : [ "10.0.0.0/21" ],       "ipv6AddressSpaces" : []    },     "ipv4AddressSpace" :  "10.0.0.0/19" ,     "ipv6AddressSpace" :  null ,     "location" :  "eastus" ,     "managedResourceGroupConfiguration" : {       "location" :  "eastus" ,       "name" :  "nfcname-HostedResources-XXXXXXXX"     },     "name" :  "nfcname" ,     "networkFabricIds" : [],     "operationalState" :  null ,     "provisioningState" :  "Succeeded" ,     "resourceGroup" :  "NFCResourceGroupName" ,     "systemData" : {       "createdAt" :  "2022-10-27T16:02:13.618823+00:00" ,       "createdBy" :  "email@address.com" ,       "createdByType" :  "User" ,       "lastModifiedAt" :  "2022-10-27T17:13:18.278423+00:00" ,       "lastModifiedBy" :  "d1bd24c7-b27f-477e-86dd-939e107873d7" ,       "lastModifiedByType" :  "Application"     }, \n   "tags" :  null ,     "type" :  "microsoft.managednetworkfabric/networkfabriccontrollers" ,     "workloadExpressRouteConnections" : [      {         "expressRouteAuthorizationKey" :  null ,         "expressRouteCircuitId" :  "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx-xxxxxx/resourceGroups/ER-Dedicated-WUS2- AFO-Circuits/providers/Microsoft.Network/expressRouteCircuits/MSFT-ER-Dedicated-PvtPeering-WestUS2-AFO-Ckt-03"       }    ],     "workloadManagementNetwork" :  true ,     "workloadServices" : {       "ipv4AddressSpaces" : [ "10.0.28.0/22" ],       "ipv6AddressSpaces" : []    }  }"}
{"text": "Delete a network fabric controller\nYou should delete an NFC only after deleting all associated network fabrics. Use this command to delete an NFC:\nAzure CLI\n  az nf controller delete  --resource-group   "NFCResourceGroupName"   --resource-name   "nfcname"  \nExpected output:\nJSON\n"name" :  "nfcname" ,       "networkFabricIds" : [],       "operationalState" :  null ,       "provisioningState" :  "succeeded" ,       "resourceGroup" :  "NFCResourceGroupName" ,       "systemData" : {         "createdAt" :  "2022-10-31T10:47:08.072025+00:00" , \nIt takes 30 minutes for the deletion to finish. In the Azure portal, verify that the hosted resources are deleted."}
{"text": "Next steps\nAfter you successfully create an NFC, the next step is to create a  cluster manager ."}
{"text": "Cluster Manager: How to manage the"}
{"text": "Cluster Manager in Operator Nexus\nArticle  04/05/2023\nThe Cluster Manager is deployed in the operator's Azure subscription to manage the lifecycle of Operator Nexus Clusters."}
{"text": "Before you begin\nYou'll need:\nAzure Subscription ID  - The Azure subscription ID where Cluster Manager needs to be created (should be the same subscription ID of the Network Fabric Controller). Network Fabric Controller ID  - Network Fabric Controller and Cluster Manager have a 1:1 association. You'll need the resource ID of the Network Fabric Controller associated with the Cluster Manager. Log Analytics Workspace ID  - The resource ID of the Log Analytics Workspace used for the logs collection. Azure Region  - The Cluster Manager should be created in the same Azure region as the Network Fabric Controller. This Azure region should be used in the \nLocation\nfield of the Cluster Manager and all associated Operator Nexus instances."}
{"text": "Global arguments\nSome arguments that are available for every Azure CLI command\n--debug  - prints even more information about CLI operations, used for debugging purposes. If you find a bug, provide output generated with the \n--debug\n flag on\nwhen submitting a bug report. --help -h  - prints CLI reference information about commands and their arguments and lists available subgroups and commands. --only-show-errors  - Only show errors, suppressing warnings. --output -o  - specifies the output format. The available output formats are Json, Jsonc (colorized JSON), tsv (Tab-Separated Values), table (human-readable ASCII tables), and yaml. By default the CLI outputs Json. --query  - uses the JMESPath query language to filter the output returned from Azure services.\n--verbose  - prints information about resources created in Azure during an operation, and other useful information"}
{"text": "Cluster Manager elements\nElements Description\nName, ID, location, tags, type Name: User friendly name   ID: < Resource ID >   Location: Azure region where the Cluster Manager is created. Values from: \naz account list -locations\n.  Tags: Resource tags   Type: Microsoft.NetworkCloud/clusterManagers\nmanagerExtendedLocation The ExtendedLocation associated with the Cluster Manager\nmanagedResourceGroupConfiguration Information about the Managed Resource Group\nfabricControllerId A reference to the Network Fabric Controller that is 1:1 with this Cluster Manager\nanalyticsWorkspaceId This workspace will be where any logs that 's relevant to the customer will be relayed.\nclusterVersions[] List of ClusterAvailableVersions objects.   Cluster versions that the manager supports. Will be used as an input in the cluster clusterVersion property.\nprovisioningState Succeeded, Failed, Canceled, Provisioning, Accepted, Updating\ndetailedStatus Detailed statuses that provide additional information about the status of the Cluster Manager.\ndetailedStatusMessage Descriptive message about the current detailedStatus."}
{"text": "Create a Cluster Manager\nUse the \naz networkcloud clustermanager create\n command to create a Cluster Manager.\nThis command creates a new Cluster Manager or updates the properties of the Cluster Manager if it exists. If you have multiple Azure subscriptions, select the appropriate subscription ID using the  az account set  command.\nAzure CLI\naz networkcloud clustermanager create  \       --name   <Cluster Manager name>  \       --location   <region>  \       --analytics-workspace-id   <log analytics workspace ID>        --fabric-controller-id   <Fabric controller ID associated with this  Cluster Manager>        --managed-resource-group-configuration   < name=<Managed Resource group  Name>   location= <Managed Resource group location>  >       --tags   <key=value key=value>        --resource-group   <Resource Group Name>        --subscription   <subscription ID>  \nArguments --name -n [Required]  - The name of the Cluster Manager. --fabric-controller-id [Required]  - The resource ID of the Network Fabric Controller that is associated with the Cluster Manager. --resource-group -g [Required]  - Name of resource group. You can configure the default resource group using \naz configure --defaults group=<name>\n.\n--analytics-workspace-id  - The resource ID of the Log Analytics Workspace that is used for the logs collection --location -l  - Location. Azure region where the Cluster Manager is created. Values from: \naz account list -locations\n. You can configure the default\nlocation using \naz configure --defaults location=<location>\n.\n--managed-resource-group-configuration  - The configuration of the managed resource group associated with the resource. Usage: --managed-resource-group-configuration location=XX name=XX location: The region of the managed resource group. If not specified, the region of the parent resource is chosen. name: The name for the managed resource group. If not specified, a unique name is automatically generated. wait/--no-wait  - Wait for command to complete or don't wait for the long- running operation to finish. --tags  - Space-separated tags: key[=value] [key[=value]...]. Use '' to clear existing tags --subscription  - Name or ID of subscription. You can configure the default subscription using \naz account set -s NAME_OR_ID\n."}
{"text": "List/show Cluster Manager(s)\nList and show commands are used to get a list of existing Cluster Managers or the properties of a specific Cluster Manager."}
{"text": "List Cluster Managers in resource group\nThis command lists the Cluster Managers in the specified Resource group.\nAzure CLI\naz networkcloud clustermanager list  --resource-group   <Azure Resource group>"}
{"text": "List Cluster Managers in subscription\nThis command lists the Cluster Managers in the specified subscription.\nAzure CLI\naz networkcloud clustermanager list    --subscription   <subscription ID>"}
{"text": "Show Cluster Manager properties\nThis command lists the properties of the specified Cluster Manager.\nAzure CLI\naz networkcloud clustermanager show  \       --name   <Cluster Manager name>  \       --resource-group   <Resource group Name>        --subscription   <subscription ID>"}
{"text": "List/show command arguments\n--name -n  - The name of the Cluster Manager. --IDs  - One or more resource IDs (space-delimited). It should be a complete resource ID containing all information of 'Resource ID' arguments. --resource-group -g  - Name of resource group. You can configure the default group using \naz configure --defaults group=<name>\n.\n--subscription  - Name or ID of subscription. You can configure the default subscription using \naz account set -s NAME_OR_ID\n."}
{"text": "Update Cluster Manager\nThis command is used to patch properties of the provided Cluster Manager, or update the tags assigned to the Cluster Manager. Properties and tag updates can be done\nindependently.\nAzure CLI\naz networkcloud clustermanager update  \       --name   <Cluster Manager name>  \       --tags   < <key1=value1>   <key2=value2> >       --resource-group   <Resource group Name>        --subscription   <subscription ID>  \nArguments --tags  - TSpace-separated tags: key[=value] [key[=value] ...]. Use '' to clear existing tags. --name -n  - The name of the Cluster Manager. --IDs  - One or more resource IDs (space-delimited). It should be a complete resource ID containing all information of 'Resource ID' arguments. --resource-group -g  - Name of resource group. You can configure the default group using \naz configure --defaults group=<name>\n.\n--subscription  - Name or ID of subscription. You can configure the default subscription using \naz account set -s NAME_OR_ID\n."}
{"text": "Delete Cluster Manager\nThis command is used to Delete the provided Cluster Manager.\n  Warning\nA Cluster Manager that has an existing associated Network Fabric Controller, or any Clusters that reference this Cluster Manager may not be deleted.\nAzure CLI\naz networkcloud clustermanager delete  \       --name   <Cluster Manager name>  \       --resource-group   <Resource Group Name>        --subscription   <subscription ID>  \nArguments --no-wait  - Don't wait for the long-running operation to complete. --yes -y  - Don't prompt for confirmation. --name -n  - The name of the Cluster Manager.\n--IDs  - One or more resource IDs (space-delimited). It should be a complete resource ID containing all information of 'Resource ID' arguments. --resource-group -g  - Name of resource group. You can configure the default group using \naz configure --defaults group=<name>\n.\n--subscription  - Name or ID of subscription. You can configure the default subscription using \naz account set -s NAME_OR_ID\n."}
{"text": "Operator Nexus platform prerequisites\nArticle  03/14/2023\nOperators will need to complete the prerequisites before the deploy of the Operator Nexus platform software. Some of these steps may take extended amounts of time, thus, a review of these prerequisites may prove beneficial.\nIn subsequent deployments of Operator Nexus instances, you can skip to creating the on-premises  Network Fabric  and the  Cluster ."}
{"text": "Azure prerequisites\nWhen deploying Operator Nexus for the first time or in a new region, you'll first need to create a Network Fabric Controller and then a Cluster Manager as specified  here . Additionally, the following tasks will need to be accomplished:\nSet up users, policies, permissions, and RBAC Set up Resource Groups to place and group resources in a logical manner that will be created for Operator Nexus platform. Establish ExpressRoute connectivity from your WAN to an Azure Region"}
{"text": "On your premises prerequisites\nWhen deploying Operator Nexus on-premises instance in your datacenter, various teams are likely involved to perform a variety of roles. The following tasks must be performed accurately in order to ensure a successful platform software installation."}
{"text": "Physical hardware setup\nAn operator that wishes to take advantage of the Operator Nexus service will need to purchase, install, configure, and operate hardware resources. This section of the document will describe the necessary components and efforts to purchase and implement the appropriate hardware systems. This section will discuss the bill of materials, the rack elevations diagram and the cabling diagram, as well as the steps required to assemble the hardware."}
{"text": "Using the Bill of Materials (BOM)\nTo ensure a seamless operator experience, Operator Nexus has developed a BOM for the hardware acquisition necessary for the service. This BOM is a comprehensive list of the necessary components and quantities needed to implement the environment for a successful implementation and maintenance of the on-premises instance. The BOM is structured to provide the operator with a series of stock keeping units (SKU) that can be ordered from hardware vendors. SKUs will be discussed later in the document."}
{"text": "Using the elevation diagram\nThe rack elevation diagram is a graphical reference that demonstrates how the servers and other components fit into the assembled and configured racks. The rack elevation diagram is provided as part of the overall build instructions and will help the operators staff to correctly configure and install all of the hardware components necessary for service operation."}
{"text": "Cabling diagram\nCabling diagrams are graphical representations of the cable connections that are required to provide network services to components installed within the racks. Following the cabling diagram ensures proper implementation of the various components in the build."}
{"text": "How to order based on SKU"}
{"text": "SKU definition\nA SKU is an inventory management and tracking method that allows grouping of multiple components into a single designator. A SKU allows an operator to order all needed components with through specify one SKU number. This expedites the operator and vendor interaction while reducing ordering errors due to complex parts lists."}
{"text": "Placing a SKU based order\nOperator Nexus has created a series of SKUs with vendors such as Dell, Pure Storage and Arista that the operator will be able to reference when they place an order. Thus, an operator simply needs to place an order based on the SKU information provided by Operator Nexus to the vendor to receive the correct parts list for the build."}
{"text": "How to build the physical hardware footprint\nThe physical hardware build is executed through a series of steps which will be detailed in this section. There are three prerequisite steps prior to the build execution. This section will also discuss assumptions concerning the skills of the operator's employees to execute the build."}
{"text": "Ordering and receipt of the specific hardware infrastructure SKU\nThe ordering of the appropriate SKU and delivery of hardware to the site must occur before the start of building. Adequate time should be allowed for this step. We recommend the operator communicate with the supplier of the hardware early in the process to ensure and understand delivery timeframes."}
{"text": "Site preparation\nThe installation site must be capable of supporting the hardware infrastructure from a space, power, and network perspective. The specific site requirements will be defined by the SKU purchased for the site. This step can be accomplished after the order is placed and before the receipt of the SKU."}
{"text": "Scheduling resources\nThe build process will require several different staff members to perform the build, such as engineers to provide power, network access and cabling, systems staff to assemble the racks, switches, and servers, to name a few. To ensure that the build is accomplished in a timely manner, we recommend scheduling these team members in advance based on the delivery schedule."}
{"text": "Assumptions regarding build staff skills\nThe staff performing the build should be experienced at assembling systems hardware such as racks, switches, PDUs and servers. The instructions provided will discuss the steps of the process, while referencing rack elevations and cabling diagrams."}
{"text": "Build process overview\nIf the site preparation is complete and validated to support the ordered SKU, the build process occurs in the following steps:\n1. Assemble the racks based on the rack elevations of the SKU. Specific rack assembly instructions will be provided by the rack manufacturer.\n2. After the racks are assembled, install the fabric devices in the racks per the elevation diagram. 3. Cable the fabric devices by connecting the network interfaces per the cabling diagram. 4. Assemble and install the servers per rack elevation diagram. 5. Assemble and install the storage device per rack elevation diagram. 6. Cable the server and storage devices by connecting the network interfaces per the cabling diagram. 7. Cable power from each device. 8. Review/validate the build through the checklists provided by Operator Nexus and other vendors."}
{"text": "How to visually inspect the physical hardware installation\nIt is recommended to label on all cables following ANSI/TIA 606 Standards, or the operator's standards, during the build process. The build process should also create reverse mapping for cabling from a switch port to far end connection. The reverse mapping can be compared to the cabling diagram to validate the installation."}
{"text": "Terminal Server and storage array setup\nNow that the physical installation and validation has completed, the next steps involved configuring up the default settings required before platform software installation."}
{"text": "Set up Terminal Server\nTerminal Server has been deployed and configured as follows:\nTerminal Server is configured for Out-of-Band management Authentication credentials have been set up DHCP client is enabled on the out-of-band management port HTTP access is enabled Terminal Server interface is connected to the operators on-premises Provider Edge routers (PEs) and configured with the IP addresses and credentials Terminal Server is accessible from the management VPN\n1. Setup hostname:  CLI Reference\nBash\nsudo ogcli update system/hostname hostname=\"$TS_HOSTNAME\" \nParameter name Description\nTS_HOSTNAME The terminal server hostname\n2. Setup network:\nBash\nsudo ogcli create conn <<  'END'     description= "PE1 to TS NET1"     mode= "static"     ipv4_static_settings.address= "$TS_NET1_IP"     ipv4_static_settings.netmask= "$TS_NET1_NETMASK"     ipv4_static_settings.gateway= "$TS_NET1_GW"     physif= "net1"     END \nsudo ogcli create conn <<  'END'     description= "PE2 to TS NET2"     mode= "static"     ipv4_static_settings.address= "$TS_NET2_IP"     ipv4_static_settings.netmask= "$TS_NET2_NETMASK"     ipv4_static_settings.gateway= "$TS_NET2_GW"     physif= "net2"     END \nParameter name Description\nTS_NET1_IP The terminal server PE1 to TS NET1 IP\nTS_NET1_NETMASK The terminal server PE1 to TS NET1 netmask\nTS_NET1_GW The terminal server PE1 to TS NET1 gateway\nTS_NET2_IP The terminal server PE2 to TS NET2 IP\nTS_NET2_NETMASK The terminal server PE2 to TS NET2 netmask\nTS_NET2_GW The terminal server PE2 to TS NET2 gateway\n3. Setup support admin user:\nFor each port\nBash\nogcli create user <<  'END'   description= "Support Admin User"   enabled= true   groups[0]= "admin"  \ngroups[1]= "netgrp"   hashed_password= "$HASHED_SUPPORT_PWD"   username= "$SUPPORT_USER"   END \nParameter name Description\nSUPPORT_USER Support admin user\nHASHED_SUPPORT_PWD Encoded support admin user password\n4. Verify settings:\nBash\n ping $PE1_IP -c 3   # ping test to PE1    ping $PE2_IP -c 3  # ping test to PE2    ogcli get conns  # verify NET1, NET2    ogcli get users  # verify support admin user    ogcli get static_routes  # there should be no static routes    ip r  # verify only interface routes    ip a  # verify loopback, NET1, NET2"}
{"text": "Set up storage array\n1. Operator needs to install the storage array hardware as specified by the BOM and rack elevation within the Aggregation Rack. 2. Operator will need to provide the storage array Technician with information, in order for the storage array Technician to arrive on-site to configure the appliance. 3. Required location-specific data that will be shared with storage array technician:\nCustomer Name: Physical Inspection Date: Chassis Serial Number: Storage array Array Hostname: CLLI code (Common Language location identifier): Installation Address: FIC/Rack/Grid Location:\n4. Data provided to the operator and shared with storage array technician, which will be common to all installations:\nPurity Code Level: 6.1.14 Array Time zone: UTC\nDNS Server IP Address: 172.27.255.201 DNS Domain Suffix: not set by operator during setup NTP Server IP Address or FQDN: 172.27.255.212 Syslog Primary: 172.27.255.210 Syslog Secondary: 172.27.255.211 SMTP Gateway IP address or FQDN: not set by operator during setup Email Sender Domain Name: not set by operator during setup Email Address(es) to be alerted: not set by operator during setup Proxy Server and Port: not set by operator during setup Management: Virtual Interface IP Address: 172.27.255.200 Gateway: 172.27.255.1 Subnet Mask: 255.255.255.0 MTU: 1500 Bond: not set by operator during setup Management: Controller 0 IP Address: 172.27.255.254 Gateway: 172.27.255.1 Subnet Mask: 255.255.255.0 MTU: 1500 Bond: not set by operator during setup Management: Controller 1 IP Address: 172.27.255.253 Gateway: 172.27.255.1 Subnet Mask: 255.255.255.0 MTU: 1500 Bond: not set by operator during setup VLAN Number / Prefix: 43 ct0.eth10: not set by operator during setup ct0.eth11: not set by operator during setup ct0.eth18: not set by operator during setup ct0.eth19: not set by operator during setup ct1.eth10: not set by operator during setup ct1.eth11: not set by operator during setup ct1.eth18: not set by operator during setup ct1.eth19: not set by operator during setup"}
{"text": "Default setup for other devices installed\nAll network fabric devices (except for the Terminal Server) are set to \nZTP\n mode\nServers have default factory settings"}
{"text": "Install CLI extensions and sign-in to your Azure"}
{"text": "subscription\nInstall latest version of the  necessary CLI extensions ."}
{"text": "Azure subscription sign-in\nAzure CLI\n  az login    az account set  --subscription  $SUBSCRIPTION_ID    az account show \n  Note\nThe account must have permissions to read/write/publish in the subscription"}
{"text": "Create and provision a network fabric"}
{"text": "by using the Azure CLI\nArticle  05/24/2023\nThis article describes how to create a network fabric for Azure Operator Nexus by using the Azure CLI. This article also shows you how to check the status of, update, and delete a network fabric."}
{"text": "Prerequisites\nAn Azure account with an active subscription. The latest version of the Azure CLI commands (2.0 or later). For more information, see  Install the Azure CLI . A network fabric controller (NFC) that manages multiple network fabrics in the same Azure region. A physical Azure Operator Nexus instance with cabling, as described in the bill of materials (BoM). Azure ExpressRoute connectivity between NFC and Azure Operator Nexus instances. A terminal server  installed and configured  with a username and password. Provider edge (PE) devices preconfigured with necessary VLANs, route targets, and IP addresses.\nSupported SKUs for network fabric instances are:\nM4-A400-A100-C16-aa for up to four compute racks M8-A400-A100-C16-aa for up to eight compute racks"}
{"text": "Steps to provision a fabric and racks\n1. Create a network fabric by providing racks, server count, SKU, and network configuration. 2. Create a network-to-network interconnect (NNI) by providing Layer 2 and Layer 3 parameters. 3. Update the serial number in the network device resource with the actual serial number on the device. The device sends the serial number as part of a DHCP request. 4. Configure the terminal server (which also hosts the DHCP server) with the serial numbers of all the devices.\n5. Provision the network devices via zero-touch provisioning mode. Based on the serial number in the DHCP request, the DHCP server responds with the boot configuration file for the corresponding device."}
{"text": "Configure a network fabric\nThe following table specifies parameters that you use to create a network fabric. In the table, \n$prefix\n is \n/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx-\nxxxxxx/resourceGroups/NFResourceGroupName/providers/Microsoft.ManagedNetworkFabric/\nnetworkFabricControllers\n.\nParameter Description Example Required Type\nresource-group\nName of the resource\nNFResourceGroup\nTrue group.\nlocation\nAzure Operator\neastus\nTrue Nexus region.\nresource-name\nName of the fabric\nNF-ResourceName\nTrue resource.\nnf-sku\nFabric SKU ID, which\nM4-A400-A100-\nTrue String is the SKU of the\nC16-aa\nordered BoM. The two supported SKUs are M4-A400-A100- C16-aa and M8- A400-A100-C16-aa.\nnfc-id\nAzure Resource\n$prefix/NFCName\nTrue Manager resource ID for the network fabric controller.\nrackcount\nNumber of compute\n8\nTrue racks per fabric. Possible values are \n2\nto \n8\n.\nserverCountPerRack\nNumber of compute\n16\nTrue servers per rack. Possible values are \n4\n,\n8\n, \n12\n, and \n16\n.\nParameter Description Example Required Type\nipv4Prefix\nIPv4 prefix of the\n10.246.0.0/19\nTrue management network. This prefix should be unique across all network fabrics in a network fabric controller. Prefix length should be at least 19 (/20 isn't allowed, but /18 and lower are allowed).\nipv6Prefix\nIPv6 prefix of the\n10:5:0:0::/59\nTrue management network. This prefix should be unique across all network fabrics in a network fabric controller.\nmanagement-network-config\nDetails of the True management network.\ninfrastructureVpnConfiguration\nDetails of the True management VPN connection between the network fabric and infrastructure services in the network fabric controller.\noptionBProperties\nDetails of MPLS True Option 10B, which is used for connectivity between the network fabric and the network fabric controller.\nParameter Description Example Required Type\nimportRouteTargets\nValues of import\n65048:10039\nTrue (if route targets to be Option B configured on is customer edges (CEs) enabled) for exchanging routes between a CE and provider edge (PE) via MPLS Option 10B.\nexportRouteTargets\nValues of export route\n65048:10039\nTrue (if targets to be Option B configured on CEs for is exchanging routes enabled) between a CE and a PE via MPLS Option 10B.\nworkloadVpnConfiguration\nDetails of the workload VPN connection between the network fabric and workload services in the network fabric controller.\noptionBProperties\nDetails of MPLS Option 10B, which is used for connectivity between the network fabric and the network fabric controller.\nimportRouteTargets\nValues of import\n65048:10050\nTrue (if route targets to be Option B configured on CEs for is exchanging routes enabled) between a CE and a PE via MPLS Option 10B.\nexportRouteTargets\nValues of export route\n65048:10050\nTrue (if targets to be Option B configured on CEs for is exchanging routes enabled) between a CE and a PE via MPLS Option 10B.\nParameter Description Example Required Type\nts-config\nTerminal server True configuration details.\nprimaryIpv4Prefix\nIPv4 prefix for\n20.0.10.0/30\n; True connectivity between the terminal the terminal server server interface and the primary PE. for the primary The terminal server network is interface for the assigned primary network is\n20.0.10.1\n, and assigned the first the PE interface usable IP from the is assigned prefix. The\n20.0.10.2\n. corresponding interface on the PE is assigned the second usable address.\nsecondaryIpv4Prefix\nIPv4 prefix for\n20.0.0.4/30\n; the True connectivity between terminal server the terminal server interface for the and the secondary PE. secondary The terminal server network is interface for the assigned secondary network is\n20.0.10.5\n, and assigned the first the PE interface usable IP from the is assigned prefix. The\n20.0.10.6\n. corresponding interface on the PE is assigned the second usable address.\nusername\nUsername that the True services use to configure the terminal server.\npassword\nPassword that the True services use to configure the terminal server.\nserialNumber\nSerial number of the terminal server."}
{"text": "Create a network fabric\nYou must create a resource group before you create a network fabric. We recommend that you create a separate resource group for each network fabric. You can create a resource group by using the following command:\nAzure CLI\naz group create  -n  NFResourceGroup  -l   "East US"  \nRun the following command to create the network fabric. The rack count is either \n4\n or\n8\n, depending on your setup.\nAzure CLI\naz nf fabric create  \   --resource-group   "NFResourceGroupName"    --location   "eastus"  \  --resource-name   "NFName"  \  --nf-sku   "NFSKU"  \  --nfc-id   "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/NFResourceGroupName/providers/Microsoft.ManagedNetwork Fabric/networkFabricControllers/NFCName"    --fabric-asn  65048   --ipv 4 -prefix  10.2.0.0/19   --ipv 6 -prefix  fda0:d59c:da02::/59   --rack-count  4  --server-count-per-rack  8  --ts-config   '{"primaryIpv4Prefix":"20.0.1.0/30",  "secondaryIpv4Prefix":"20.0.0.0/30", "username":"****", "password": "****",  "serialNumber":"TerminalServerSerialNumber"}'    --managed-network-config   '{"infrastructureVpnConfiguration": {"peeringOption":"OptionB","optionBProperties":{"importRouteTargets": ["65048:10039"],"exportRouteTargets":["65048:10039"]}},  "workloadVpnConfiguration":{"peeringOption": "OptionB", "optionBProperties":  {"importRouteTargets": ["65048:10050"], "exportRouteTargets":  ["65048:10050"]}}}'  \nExpected output:\nOutput\n{    "annotation": null,    "fabricAsn": 65048,    "id": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/NFResourceGroup/providers/Microsoft.ManagedNetworkFabr ic/networkFabrics/NFName",    "ipv4Prefix": "10.2.0.0/19",    "ipv6Prefix": "fda0:d59c:da02::/59", \n  "l2IsolationDomains": null,    "l3IsolationDomains": null,    "location": "eastus",    "managementNetworkConfiguration": {      "infrastructureVpnConfiguration": {        "administrativeState": "Enabled",        "networkToNetworkInterconnectId": null,        "optionAProperties": null,        "optionBProperties": {          "exportRouteTargets": [            "65048:10039"          ],          "importRouteTargets": [            "65048:10039"          ]        },        "peeringOption": "OptionB"      },      "workloadVpnConfiguration": {       "administrativeState": "Enabled",        "networkToNetworkInterconnectId": null,        "optionAProperties": null,        "optionBProperties": {          "exportRouteTargets": [            "65048:10050"          ],          "importRouteTargets": [            "65048:10050"          ]        },        "peeringOption": "OptionB"      }    },    "name": "NFName",    "networkFabricControllerId": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/NFCResourceGroupName/providers/Microsoft.ManagedNetwor kFabric/networkFabricControllers/NFCName",    "networkFabricSku": "NFSKU",    "operationalState": null,    "provisioningState": "Accepted",    "rackCount": 4,    "racks": null,    "resourceGroup": "NFResourceGroupName",    "routerId": null,    "serverCountPerRack": 8,    "systemData": {      "createdAt": "2023-XX-X-6T12:52:11.769525+00:00",      "createdBy": "email@address.com",      "createdByType": "User",      "lastModifiedAt": "2023-XX-XX-6T12:52:11.769525+00:00",      "lastModifiedBy": "email@address.com",      "lastModifiedByType": "User"    },    "tags": null,    "terminalServerConfiguration": { \n    "networkDeviceId": null,      "password": null,      "primaryIpv4Prefix": "20.0.1.0/30",      "primaryIpv6Prefix": null,      "secondaryIpv4Prefix": "20.0.0.0/30",      "secondaryIpv6Prefix": null,      "serialNumber": "TerminalServerSerialNumber",      "username": "****"    },    "type": "microsoft.managednetworkfabric/networkfabrics"  }"}
{"text": "Show network fabrics\nAzure CLI\naz nf fabric show  --resource-group   "NFResourceGroupName"   --resource-name   "NFName"  \nExpected output:\nOutput\n{    "annotation": null,    "fabricAsn": 65048,    "id": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/NFResourceGroup/providers/Microsoft.ManagedNetworkFabr ic/networkFabrics/NFName",    "ipv4Prefix": "10.2.0.0/19",    "ipv6Prefix": "fda0:d59c:da02::/59",    "l2IsolationDomains": null,    "l3IsolationDomains": null,    "location": "eastus",    "managementNetworkConfiguration": {      "infrastructureVpnConfiguration": {        "administrativeState": "Enabled",        "networkToNetworkInterconnectId": null,        "optionAProperties": null,        "optionBProperties": {          "exportRouteTargets": [            "65048:10039"          ],          "importRouteTargets": [            "65048:10039"          ]        },        "peeringOption": "OptionB"      },      "workloadVpnConfiguration": {\n      "administrativeState": "Enabled",        "networkToNetworkInterconnectId": null,        "optionAProperties": null,        "optionBProperties": {          "exportRouteTargets": [            "65048:10050"          ],          "importRouteTargets": [            "65048:10050"          ]        },        "peeringOption": "OptionB"      }    },    "name": "nffab1031623",    "networkFabricControllerId": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/NFCResourceGroupName/providers/Microsoft.ManagedNetwor kFabric/networkFabricControllers/NFCName",    "networkFabricSku": "NFSKU",    "operationalState": null,    "provisioningState": "Succeeded",    "rackCount": 4,    "racks": [      "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourcegroups/NFResourceGroup/providers/microsoft.managednetworkfabr ic/networkracks/NFName-aggrack",      "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourcegroups/NFResourceGroup/providers/microsoft.managednetworkfabr ic/networkracks/NFName-comprack1",      "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourcegroups/NFResourceGroup/providers/microsoft.managednetworkfabr ic/networkracks/NFName-comprack2"   ],    "resourceGroup": "NFResourceGroup",    "routerId": null,    "serverCountPerRack": 8,    "systemData": {      "createdAt": "2023-XX-XXT12:52:11.769525+00:00",      "createdBy": "email@address.com",      "createdByType": "User",      "lastModifiedAt": "2023-XX-XXT12:53:02.504974+00:00",      "lastModifiedBy": "d1bd24c7-b27f-477e-86dd-939e107873d7",      "lastModifiedByType": "Application"    },    "tags": null,    "terminalServerConfiguration": {      "networkDeviceId": null,      "password": null,      "primaryIpv4Prefix": "20.0.1.0/30",      "primaryIpv6Prefix": null,      "secondaryIpv4Prefix": "20.0.0.0/30",      "secondaryIpv6Prefix": null,      "serialNumber": "TerminalServerSerialNumber",      "username": "****"    }, \n  "type": "microsoft.managednetworkfabric/networkfabrics"  }"}
{"text": "List all network fabrics in a resource group\nAzure CLI\naz nf fabric list  --resource-group   "NFResourceGroup"    \nExpected output:\nOutput\n{      "annotation": null,      "fabricAsn": 65048,      "id": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/NFResourceGroup/providers/Microsoft.ManagedNetworkFabr ic/networkFabrics/NFName",      "ipv4Prefix": "10.2.0.0/19",      "ipv6Prefix": "fda0:d59c:da02::/59",      "l2IsolationDomains": [Null],         "l3IsolationDomains": [Null],     "location": "eastus",      "managementNetworkConfiguration": {        "infrastructureVpnConfiguration": {          "administrativeState": "Enabled",          "networkToNetworkInterconnectId": null,          "optionAProperties": null,          "optionBProperties": {            "exportRouteTargets": [             "65048:10039"            ],            "importRouteTargets": [             "65048:10039"            ]          },          "peeringOption": "OptionB"        },        "workloadVpnConfiguration": {          "administrativeState": "Enabled",          "networkToNetworkInterconnectId": null,          "optionAProperties": null,          "optionBProperties": {            "exportRouteTargets": [             "65048:10050"            ],            "importRouteTargets": [             "65048:10050"            ] \n        },          "peeringOption": "OptionB"        }      },      "name": "NFName",      "networkFabricControllerId": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/NFCResourceGroup/providers/Microsoft.ManagedNetworkFab ric/networkFabricControllers/NFCName",      "networkFabricSku": "NFSKU",      "operationalState": "Provisioned",      "provisioningState": "Succeeded",      "rackCount": 4,      "racks": [        "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourcegroups/NFResourceGroup/providers/microsoft.managednetworkfabr ic/networkracks/NFName-aggrack",        "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourcegroups/NFResourceGroup/providers/microsoft.managednetworkfabr ic/networkracks/NFName-comprack1",        "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourcegroups/NFResourceGroup/providers/microsoft.managednetworkfabr ic/networkracks/NFName-comprack2"     ],      "resourceGroup": "NFResourceGroup",      "routerId": null,      "serverCountPerRack": 8,      "systemData": {        "createdAt": "2023-XX-XXT12:52:11.769525+00:00",        "createdBy": "email@address.com",        "createdByType": "User",        "lastModifiedAt": "2023-XX-XXT02:05:44.043591+00:00",        "lastModifiedBy": "d1bd24c7-b27f-477e-86dd-939e107873d7",        "lastModifiedByType": "Application"      },      "tags": null,      "terminalServerConfiguration": {        "networkDeviceId": null,        "password": null,        "primaryIpv4Prefix": "20.0.1.0/30",        "primaryIpv6Prefix": null,        "secondaryIpv4Prefix": "20.0.0.0/30",        "secondaryIpv6Prefix": null,        "serialNumber": "TerminalServerSerialNumber",        "username": "****"      },      "type": "microsoft.managednetworkfabric/networkfabrics"    }"}
{"text": "Configure an NNI\nThe following table specifies the parameters that you use to create a network-to- network interconnect.\nParameter Description Example Required Type\nisMangementType\nConfiguration to use an\nTrue\nTrue NNI for management of the fabric. Possible values are \nTrue\n and\nFalse\n. The default value is \nTrue\n.\nuseOptionB\nConfiguration to enable\nTrue\nTrue Option B. Possible values are \nTrue\n and\nFalse\n.\nlayer2Configuration\nLayer 2 configuration.\nportCount\nNumber of ports that\n3\nare part of the port channel. The maximum value is based on the fabric SKU.\nmtu\nMaximum transmission\n1500\nunit between CEs and PEs.\nlayer3Configuration\nLayer 3 configuration True between CEs and PEs.\nprimaryIpv4Prefix\nIPv4 prefix for\n10.246.0.124/31\n; the String connectivity between port-channel interface for the primary CE and the the primary CE is assigned primary PE. The port-\n10.246.0.125\n, and the channel interface for port-channel interface for the primary CE is the primary PE is assigned assigned the first usable\n10.246.0.126\n. IP from the prefix. The corresponding interface on the primary PE is assigned the second usable address.\nParameter Description Example Required Type\nsecondaryIpv4Prefix\nIPv4 prefix for\n10.246.0.128/31\n; the String connectivity between port-channel interface for the secondary CE and the secondary CE is the secondary PE. The assigned \n10.246.0.129\n, port-channel interface and the port-channel for the secondary CE is interface for the assigned the first usable secondary PE is assigned IP from the prefix. The\n10.246.0.130\n. corresponding interface on the secondary PE is assigned the second usable address.\nprimaryIpv6Prefix\nIPv6 prefix for\n3FFE:FFFF:0:CD30::a1\n is String connectivity between assigned to the primary the primary CE and the CE, and primary PE. The port-\n3FFE:FFFF:0:CD30::a2\n is channel interface for assigned to the primary the primary CE is PE. Default value is assigned the first usable\n3FFE:FFFF:0:CD30::a0/126\n. IP from the prefix. The corresponding interface on the primary PE is assigned the second usable address.\nsecondaryIpv6Prefix\nIPv6 prefix for\n3FFE:FFFF:0:CD30::a5\n is String connectivity between assigned to the secondary the secondary CE and CE, and the secondary PE. The\n3FFE:FFFF:0:CD30::a6\n is port-channel interface assigned to the secondary for the secondary CE is PE. Default value is assigned the first usable\n3FFE:FFFF:0:CD30::a4/126\n. IP from the prefix. The corresponding interface on the secondary PE is assigned the second usable address.\nfabricAsn\nASN assigned on the CE\n65048\nfor BGP peering with the PE.\nParameter Description Example Required Type\npeerAsn\nASN assigned on the PE\n65048\nTrue for BGP peering with the CE. For internal BGP between the PE and the CE, the value should be the same as \nfabricAsn\n. For external BGP, the value should be different from\nfabricAsn\n.\nfabricAsn\nASN assigned on the CE\n65048\nfor BGP peering with the PE.\nvlan-Id\nVLAN for the NNI. The\n501\nrange is 501 to 4095.\nimportRoutePolicy\nDetails to import a route policy.\nexportRoutePolicy\nDetails to export a route policy."}
{"text": "Create an NNI\nYou must create the resource group and network fabric before you create a network-to- network interconnect.\nRun the following command to create the NNI:\nAzure CLI\naz nf nni create  \  --resource-group   "NFResourceGroup"  \  --location   "eastus"  \  --resource-name   "NFNNIName"  \  --fabric   "NFFabric"  \  --is-management-type   "True"  \  --use-option-b   "True"  \ --layer 2 -configuration   '{"portCount": 3, "mtu": 1500}'  \ --layer 3 -configuration   '{"peerASN": 65048, "vlanId": 501,  "primaryIpv4Prefix": "10.2.0.124/30", "secondaryIpv4Prefix":  "10.2.0.128/30", "primaryIpv6Prefix": "10:2:0:124::400/127",  "secondaryIpv6Prefix": "10:2:0:124::402/127"}'  \nExpected output:\nOutput\n{    "administrativeState": null,    "id": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/NFResourceGroup/providers/Microsoft.ManagedNetworkFabr ic/networkFabrics/nffab1031623/networkToNetworkInterconnects/NFNNIName",    "isManagementType": "True",    "layer2Configuration": {      "interfaces": null,      "mtu": 1500,      "portCount": 3    },    "layer3Configuration": {      "exportRoutePolicyId": null,      "fabricAsn": null,      "importRoutePolicyId": null,      "peerAsn": 65048,      "primaryIpv4Prefix": "10.2.0.124/30",      "primaryIpv6Prefix": "10:2:0:124::400/127",      "secondaryIpv4Prefix": "10.2.0.128/30",      "secondaryIpv6Prefix": "10:2:0:124::402/127",      "vlanId": 501    },    "name": "NFNNIName",    "provisioningState": "Succeeded",    "resourceGroup": "NFResourceGroup",    "systemData": {      "createdAt": "2023-XX-XXT13:13:22.514644+00:00",      "createdBy": "email@address.com",      "createdByType": "User",      "lastModifiedAt": "2023-XX-XXT13:13:22.514644+00:00",      "lastModifiedBy": "email@address.com",      "lastModifiedByType": "User"    },    "type":  "microsoft.managednetworkfabric/networkfabrics/networktonetworkinterconnects ",    "useOptionB": "True""}
{"text": "Show network fabric NNIs\nAzure CLI\naz nf nni show  -g   "NFResourceGroup"   --resource-name   "NFNNIName"   --fabric   "NFFabric"  \nExpected output:\nOutput\n{    "administrativeState": null,    "id": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/NFResourceGroup/providers/Microsoft.ManagedNetworkFabr ic/networkFabrics/NFFabric/networkToNetworkInterconnects/NFNNIName",    "isManagementType": "True",    "layer2Configuration": {      "interfaces": null,      "mtu": 1500,      "portCount": 3    },    "layer3Configuration": {      "exportRoutePolicyId": null,      "fabricAsn": null,      "importRoutePolicyId": null,      "peerAsn": 65048,      "primaryIpv4Prefix": "10.2.0.124/30",      "primaryIpv6Prefix": "10:2:0:124::400/127",      "secondaryIpv4Prefix": "10.2.0.128/30",      "secondaryIpv6Prefix": "10:2:0:124::402/127",      "vlanId": 501    },    "name": "NFNNIName",    "provisioningState": "Succeeded",    "resourceGroup": "NFResourceGroup",    "systemData": {      "createdAt": "2023-XX-XXT13:13:22.514644+00:00",      "createdBy": "email@address.com",      "createdByType": "User",      "lastModifiedAt": "2023-XX-XX13:13:22.514644+00:00",      "lastModifiedBy": "email@address.com",      "lastModifiedByType": "User"    },    "type":  "microsoft.managednetworkfabric/networkfabrics/networktonetworkinterconnects ",    "useOptionB": "True""}
{"text": "List or get network fabric NNIs\nAzure CLI\naz nf nni list  -g  NFResourceGroup  --fabric  NFFabric \nExpected output:\nOutput\n{      "administrativeState": null,      "id": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/NFResourceGroup/providers/Microsoft.ManagedNetworkFabr ic/networkFabrics/NFFabric/networkToNetworkInterconnects/NFNNIName",      "isManagementType": "True",      "layer2Configuration": {        "interfaces": null,        "mtu": 1500,        "portCount": 3      },      "layer3Configuration": {        "exportRoutePolicyId": null,        "fabricAsn": null,        "importRoutePolicyId": null,        "peerAsn": 65048,        "primaryIpv4Prefix": "10.2.0.124/30",        "primaryIpv6Prefix": "10:2:0:124::400/127",        "secondaryIpv4Prefix": "10.2.0.128/30",        "secondaryIpv6Prefix": "10:2:0:124::402/127",        "vlanId": 501      },      "name": "NFNNIName",      "provisioningState": "Succeeded",      "resourceGroup": "NFResourceGroup",      "systemData": {        "createdAt": "2023-XX-XXT13:13:22.514644+00:00",        "createdBy": "email@address.com.com",        "createdByType": "User",        "lastModifiedAt": "2023-XX-XXT13:13:22.514644+00:00",        "lastModifiedBy": "email@address.com.com",        "lastModifiedByType": "User"      },      "type":  "microsoft.managednetworkfabric/networkfabrics/networktonetworkinterconnects ",      "useOptionB": "True"    }"}
{"text": "Update network fabric devices\nRun the following command to update network fabric devices:\nAzure CLI\naz nf device update  \  --resource-group   "NFResourceGroup"  \  --resource-name   "Network-Device-Name"  \ \n--location   "eastus"  \  --serial-number   "xxxx"  \nExpected output:\nOutput\n{    "annotation": null,    "hostName": "AggrRack-CE01",    "id": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/NFResourceGroup/providers/Microsoft.ManagedNetworkFabr ic/networkDevices/Network-Device-Name",    "location": "eastus2euap",    "name": "Network-Device-Name",    "networkDeviceRole": "CE1",    "networkDeviceSku": "DefaultSku",    "networkRackId": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/NFResourceGroup/providers/Microsoft.ManagedNetworkFabr ic/networkRacks/Network-Device-Name",    "provisioningState": "Succeeded",    "resourceGroup": "NFResourceGroup",    "serialNumber": "AXXXX;DCS-XXXXX-24;XX.XX;JXXXXXXX",    "systemData": {      "createdAt": "2023-XX-XXT12:52:42.270551+00:00",      "createdBy": "d1bd24c7-b27f-477e-86dd-939e107873d7",      "createdByType": "Application",      "lastModifiedAt": "2023-XX-XXT13:30:24.098335+00:00",      "lastModifiedBy": "email@address.com",      "lastModifiedByType": "User"    },    "tags": null,    "type": "microsoft.managednetworkfabric/networkdevices",    "version": null  } \nThe preceding code serves only as an example. You should update all the devices that are part of both \nAggrRack\n and \ncomputeRacks\n.\nFor example, \nAggrRack\n consists of:\nCE01\nCE02\nTOR17\nTOR18\nMgmtSwitch01\nMgmtSwitch02\n (and so on, for other switches)"}
{"text": "List or get network fabric devices\nRun the following command to list network fabric devices in a resource group:\nAzure CLI\naz nf device list  --resource-group   "NFResourceGroup"  \nExpected output:\nOutput\n{      "annotation": null,      "hostName": "AggrRack-CE01",      "id": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/NFResourceGroup/providers/Microsoft.ManagedNetworkFabr ic/networkDevices/NFName-AggrRack-CE1",      "location": "eastus",      "name": "Network-Device-Name",      "networkDeviceRole": "CE1",      "networkDeviceSku": "DefaultSku",      "networkRackId": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/NFResourceGroup/providers/Microsoft.ManagedNetworkFabr ic/networkRacks/Network-Device-Name",      "provisioningState": "Succeeded",      "resourceGroup": "NFResourceGroup",      "serialNumber": "ArXXX;DCS-7XXXXXX-24;12.05;JPXXXXXXXX",      "systemData": {        "createdAt": "2023-XX-XXT12:52:42.270551+00:00",        "createdBy": "d1bd24c7-b27f-477e-86dd-939e107873d7",        "createdByType": "Application",        "lastModifiedAt": "2023-XX-XXT13:30:24.098335+00:00",        "lastModifiedBy": "email@address.com",        "lastModifiedByType": "User"      },      "tags": null,      "type": "microsoft.managednetworkfabric/networkdevices",      "version": null    },    {      "annotation": null,      "hostName": "AggrRack-CE02",      "id": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/NFResourceGroup/providers/Microsoft.ManagedNetworkFabr ic/networkDevices/NFName-AggrRack-CE2",      "location": "eastus",      "name": "Network-Device-Name",      "networkDeviceRole": "CE2",      "networkDeviceSku": "DefaultSku",      "networkRackId": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/NFResourceGroup/providers/Microsoft.ManagedNetworkFabr\nic/networkRacks/Network-Device-Name",      "provisioningState": "Succeeded",      "resourceGroup": "NFResourceGroup",      "serialNumber": "ArXXX;DCS-7XXXXXX-24;12.05;JPXXXXXXXX",      "systemData": {        "createdAt": "2023-XX-XXT12:52:43.489256+00:00",        "createdBy": "d1bd24c7-b27f-477e-86dd-939e107873d7",        "createdByType": "Application",        "lastModifiedAt": "2023-XX-XXT13:30:40.923567+00:00",        "lastModifiedBy": "email@address.com",        "lastModifiedByType": "User"      },      "tags": null,      "type": "microsoft.managednetworkfabric/networkdevices",      "version": null    },    {      "annotation": null,      "hostName": "AggRack-TOR17",      "id": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/NFResourceGroup/providers/Microsoft.ManagedNetworkFabr ic/networkDevices/NFName-AggrRack-TOR17",      "location": "eastus2euap",      "name": "Network-Device-Name",      "networkDeviceRole": "TOR17",     "networkDeviceSku": "DefaultSku",      "networkRackId": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/NFResourceGroup/providers/Microsoft.ManagedNetworkFabr ic/networkRacks/Network-Device-Name",      "provisioningState": "Succeeded",      "resourceGroup": "NFResourceGroup",      "serialNumber": "ArXXX;DCS-7XXXXXX-24;12.05;JPXXXXXXXX",      "systemData": {        "createdAt": "2023-XX-XXT12:52:44.676759+00:00",        "createdBy": "d1bd24c7-b27f-477e-86dd-939e107873d7",        "createdByType": "Application",        "lastModifiedAt": "2023-XX-XXT13:31:59.650758+00:00",        "lastModifiedBy": "email@address.com",        "lastModifiedByType": "User"      },      "tags": null,      "type": "microsoft.managednetworkfabric/networkdevices",      "version": null    },    {      "annotation": null,      "hostName": "AggRack-TOR18",      "id": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/NFResourceGroup/providers/Microsoft.ManagedNetworkFabr ic/networkDevices/NFName-AggrRack-TOR18",      "location": "eastus",      "name": "Network-Device-Name",      "networkDeviceRole": "TOR18",     "networkDeviceSku": "DefaultSku",      "networkRackId": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx-\nxxxxxx/resourceGroups/NFResourceGroup/providers/Microsoft.ManagedNetworkFabr ic/networkRacks/Network-Device-Name",      "provisioningState": "Succeeded",      "resourceGroup": "NFResourceGroup",      "serialNumber": "ArXXX;DCS-7XXXXXX-24;12.05;JPXXXXXXXX",      "systemData": {        "createdAt": "2023-03-16T12:52:45.801778+00:00",        "createdBy": "d1bd24c7-b27f-477e-86dd-939e107873d7",        "createdByType": "Application",        "lastModifiedAt": "2023-XX-XXT13:32:13.369591+00:00",        "lastModifiedBy": "email@address.com",        "lastModifiedByType": "User"      },      "tags": null,      "type": "microsoft.managednetworkfabric/networkdevices",      "version": null    },    {      "annotation": null,      "hostName": "AggRack-MGMT1",      "id": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/NFResourceGroup/providers/Microsoft.ManagedNetworkFabr ic/networkDevices/NFName-AggrRack-MgmtSwitch1",      "location": "eastus",      "name": "Network-Device-Name",      "networkDeviceRole": "MgmtSwitch1",      "networkDeviceSku": "DefaultSku",      "networkRackId": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/NFResourceGroup/providers/Microsoft.ManagedNetworkFabr ic/networkRacks/Network-Device-Name",      "provisioningState": "Succeeded",      "resourceGroup": "NFResourceGroup",      "serialNumber": "ArXXX;DCS-7XXXXXX-24;12.05;JPXXXXXXXX",      "systemData": {        "createdAt": "2023-XX-XXT12:52:46.911202+00:00",        "createdBy": "d1bd24c7-b27f-477e-86dd-939e107873d7",        "createdByType": "Application",        "lastModifiedAt": "2023-XX-XXT13:31:00.836730+00:00",        "lastModifiedBy": "email@address.com",        "lastModifiedByType": "User"      },      "tags": null,      "type": "microsoft.managednetworkfabric/networkdevices",      "version": null    },    {      "annotation": null,      "hostName": "AggRack-MGMT2",      "id": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/NFResourceGroup/providers/Microsoft.ManagedNetworkFabr ic/networkDevices/NFName-AggrRack-MgmtSwitch2",      "location": "eastus",      "name": "Network-Device-Name",      "networkDeviceRole": "MgmtSwitch2",      "networkDeviceSku": "DefaultSku", \n    "networkRackId": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/NFResourceGroup/providers/Microsoft.ManagedNetworkFabr ic/networkRacks/Network-Device-Name",      "provisioningState": "Succeeded",      "resourceGroup": "NFResourceGroup",      "serialNumber": "ArXXX;DCS-7XXXXXX-24;12.05;JPXXXXXXXX",      "systemData": {        "createdAt": "2023-XX-XXT12:52:48.020528+00:00",        "createdBy": "d1bd24c7-b27f-477e-86dd-939e107873d7",        "createdByType": "Application",        "lastModifiedAt": "2023-XX-XXT13:31:42.173645+00:00",        "lastModifiedBy": "email@address.com",        "lastModifiedByType": "User"      },      "tags": null,      "type": "microsoft.managednetworkfabric/networkdevices",      "version": null    } \nRun the following command to get or show details of a network fabric device:\nAzure CLI\naz nf device show  --resource-group   "NFResourceGroup"   --resource-name   "Network-Device-Name"  \nExpected output:\nOutput\n{    "annotation": null,    "hostName": "AggrRack-CE01",    "id": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/NFResourceGroup/providers/Microsoft.ManagedNetworkFabr ic/networkDevices/NFName-AggrRack-CE1",    "location": "eastus",    "name": "Network-Device-Name",    "networkDeviceRole": "CE1",    "networkDeviceSku": "DefaultSku",    "networkRackId": "/subscriptions/61065ccc-9543-4b91-b2d1- 0ce42a914507/resourceGroups/NFResourceGroup/providers/Microsoft.ManagedNetwo rkFabric/networkRacks/Network-Device-Name",    "provisioningState": "Succeeded",    "resourceGroup": "NFResourceGroup",    "serialNumber": "AXXXX;DCS-XXXXX-24;XX.XX;JXXXXXXX",    "systemData": {      "createdAt": "2023-XX-XXT12:52:42.270551+00:00",      "createdBy": "d1bd24c7-b27f-477e-86dd-939e107873d7",      "createdByType": "Application",      "lastModifiedAt": "2023-XX-XXT13:30:24.098335+00:00",      "lastModifiedBy": "email@address.com", \n    "lastModifiedByType": "User"    },    "tags": null,    "type": "microsoft.managednetworkfabric/networkdevices",    "version": null  }"}
{"text": "Provision a network fabric\nAfter you update the device serial number, provision and show the fabric by running the following commands:\nAzure CLI\naz nf fabric provision  --resource-group   "NFResourceGroup"    --resource-name   "NFName"  \nAzure CLI\naz nf fabric show  --resource-group   "NFResourceGroup"      --resource-name   "NFName"  \nExpected output:\nOutput\n{    "annotation": null,    "fabricAsn": 65048,    "id": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/NFResourceGroup/providers/Microsoft.ManagedNetworkFabr ic/networkFabrics/NFName",    "ipv4Prefix": "10.2.0.0/19",    "ipv6Prefix": "fda0:d59c:da02::/59",    "l2IsolationDomains": null,    "l3IsolationDomains": null,    "location": "eastus",    "managementNetworkConfiguration": {      "infrastructureVpnConfiguration": {        "administrativeState": "Enabled",        "networkToNetworkInterconnectId": null,        "optionAProperties": null,        "optionBProperties": {          "exportRouteTargets": [            "65048:10039"          ],          "importRouteTargets": [            "65048:10039" \n        ]        },        "peeringOption": "OptionB"      },      "workloadVpnConfiguration": {       "administrativeState": "Enabled",        "networkToNetworkInterconnectId": null,        "optionAProperties": null,        "optionBProperties": {          "exportRouteTargets": [            "65048:10050"          ],          "importRouteTargets": [            "65048:10050"          ]        },        "peeringOption": "OptionB"      }    },    "name": "NFName",    "networkFabricControllerId": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/NFCResourceGroup/providers/Microsoft.ManagedNetworkFab ric/networkFabricControllers/NFCName",    "networkFabricSku": "NFSKU",    "operationalState": "Provisioning",    "provisioningState": "Succeeded",    "rackCount": 3,    "racks": [      "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourcegroups/NFResourceGroup/providers/microsoft.managednetworkfabr ic/networkracks/NFName-aggrack",      "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourcegroups/NFResourceGroup/providers/microsoft.managednetworkfabr ic/networkracks/NFName-comprack1",      "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourcegroups/NFResourceGroup/providers/microsoft.managednetworkfabr ic/networkracks/NFName-comprack2"   ],    "resourceGroup": "NFResourceGroup",    "routerId": null,    "serverCountPerRack": 7,    "systemData": {      "createdAt": "2023-XX-XXT12:52:11.769525+00:00",      "createdBy": "email@address.com",      "createdByType": "User",      "lastModifiedAt": "2023-XX-XXT14:47:59.424826+00:00",      "lastModifiedBy": "d1bd24c7-b27f-477e-86dd-939e107873d7",      "lastModifiedByType": "Application"    },    "tags": null,    "terminalServerConfiguration": {      "networkDeviceId": null,      "password": null,      "primaryIpv4Prefix": "20.0.1.0/30",      "primaryIpv6Prefix": null, \n    "secondaryIpv4Prefix": "20.0.0.0/30",      "secondaryIpv6Prefix": null,      "serialNumber": "XXXXXXXXXXXX",      "username": "XXXX"    },    "type": "microsoft.managednetworkfabric/networkfabrics"  }"}
{"text": "Deprovision a network fabric\nTo deprovision a fabric, ensure that the fabric is in a provisioned operational state and then run this command:\nAzure CLI\naz nf fabric deprovision  --resource-group   "NFResourceGroup"   --resource-name   "NFName"  \nExpected output:\nOutput\n{    "annotation": null,    "fabricAsn": 65046,    "id": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/NFResourceGroup/providers/Microsoft.ManagedNetworkFabr ic/networkFabrics/NFName",    "ipv4Prefix": "10.18.0.0/19",    "ipv6Prefix": null,    "l2IsolationDomains": [],    "l3IsolationDomains": null,    "location": "eastus",    "managementNetworkConfiguration": {      "infrastructureVpnConfiguration": {        "administrativeState": "Enabled",        "networkToNetworkInterconnectId": null,        "optionAProperties": null,        "optionBProperties": {          "exportRouteTargets": [            "65048:10039"          ],          "importRouteTargets": [            "65048:10039"          ]        },        "peeringOption": "OptionB"      },      "workloadVpnConfiguration": {\n      "administrativeState": "Enabled",        "networkToNetworkInterconnectId": null,        "optionAProperties": null,        "optionBProperties": {          "exportRouteTargets": [            "65048:10050"          ],          "importRouteTargets": [            "65048:10050"          ]        },        "peeringOption": "OptionB"      }    },    "name": "NFName",    "networkFabricControllerId": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/NFCResourceGroup/providers/Microsoft.ManagedNetworkFab ric/networkFabricControllers/NFCName",    "networkFabricSku": "M4-A400-A100-C16-aa",    "operationalState": "Deprovisioned",    "provisioningState": "Succeeded",    "rackCount": 3,    "racks": [      "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourcegroups/NFResourceGroup/providers/microsoft.managednetworkfabr ic/networkracks/NFName-aggrack",      "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourcegroups/NFResourceGroup/providers/microsoft.managednetworkfabr ic/networkracks/NFName-comprack1",      "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourcegroups/NFResourceGroup/providers/microsoft.managednetworkfabr ic/networkracks/NFName-comprack2"   ],    "resourceGroup": "NFResourceGroup",    "routerId": null,    "serverCountPerRack": 8,    "systemData": {      "createdAt": "2023-XX-XXT19:30:23.319643+00:00",      "createdBy": "email@address.com",      "createdByType": "User",      "lastModifiedAt": "2023-XX-XXT06:47:36.130713+00:00",      "lastModifiedBy": "d1bd24c7-b27f-477e-86dd-939e107873d7",      "lastModifiedByType": "Application"    },    "tags": null,    "terminalServerConfiguration": {      "networkDeviceId": null,      "password": null,      "primaryIpv4Prefix": "20.0.1.12/30",      "primaryIpv6Prefix": null,      "secondaryIpv4Prefix": "20.0.0.12/30",      "secondaryIpv6Prefix": null,      "serialNumber": "XXXXXXXXXXXXX",      "username": "XXXX"    }, \n  "type": "microsoft.managednetworkfabric/networkfabrics"  }"}
{"text": "Delete a network fabric\nTo delete a fabric, run the following command. Before you do, make sure that:\nThe fabric is in a deprovisioned operational state. If it's in a provisioned state, run the \ndeprovision\n command.\nNo racks are associated with the fabric.\nAzure CLI\naz nf fabric delete  --resource-group   "NFResourceGroup"   --resource-name   "NFName"  \nExpected output:\nOutput\n{    "annotation": null,    "fabricAsn": 65044,    "id": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/NFResourceGroup/providers/Microsoft.ManagedNetworkFabr ic/networkFabrics/NFName",    "ipv4Prefix": "10.21.0.0/16",    "ipv6Prefix": "10:15:0:0::/59",   "l2IsolationDomains": null,    "l3IsolationDomains": null,    "location": "eastus",    "managementNetworkConfiguration": {      "infrastructureVpnConfiguration": {        "administrativeState": "Enabled",        "networkToNetworkInterconnectId": null,        "optionAProperties": null,        "optionBProperties": {          "exportRouteTargets": [            "65044:10039"          ],          "importRouteTargets": [            "65044:10039"          ]        },        "peeringOption": "OptionB"      },      "workloadVpnConfiguration": {\n      "administrativeState": "Enabled",        "networkToNetworkInterconnectId": null,        "optionAProperties": null,        "optionBProperties": {          "exportRouteTargets": [            "65044:10050"          ],          "importRouteTargets": [            "65044:10050"          ]        },        "peeringOption": "OptionB"      }    },    "name": "nffab2030823",    "networkFabricControllerId": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/NFCResourceGroup/providers/Microsoft.ManagedNetworkFab ric/networkFabricControllers/NFCName",    "networkFabricSku": "SKU-Name",   "operationalState": "Deprovisioned",    "provisioningState": "Deleting",    "rackCount": 3,    "racks": [      "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourcegroups/NFResourceGroup/providers/microsoft.managednetworkfabr ic/networkracks/NFName-aggrack",      "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourcegroups/NFResourceGroup/providers/microsoft.managednetworkfabr ic/networkracks/NFName-comprack1",      "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourcegroups/NFResourceGroup/providers/microsoft.managednetworkfabr ic/networkracks/NFName-comprack2"   ],    "resourceGroup": "NFResourceGroup",    "routerId": null,    "serverCountPerRack": 7,    "systemData": {      "createdAt": "2023-XX-XXT10:31:22.423399+00:00",      "createdBy": "email@address.com",      "createdByType": "User",      "lastModifiedAt": "2023-XX-XXT06:31:41.675991+00:00",      "lastModifiedBy": "d1bd24c7-b27f-477e-86dd-939e107873d7",      "lastModifiedByType": "Application"    },    "tags": null,    "terminalServerConfiguration": {      "networkDeviceId": null,      "password": null,      "primaryIpv4Prefix": "20.0.1.68/30",      "primaryIpv6Prefix": null,      "secondaryIpv4Prefix": "20.0.0.68/30",      "secondaryIpv6Prefix": null,      "serialNumber": "XXXXXXXXXXXXX",      "username": "XXXX"    }, \n  "type": "microsoft.managednetworkfabric/networkfabrics"  } \nAfter you successfully delete the network fabric, when you run the command to show the fabric, you won't find any resources available:\nAzure CLI\naz nf fabric show  --resource-group   "NFResourceGroup"   --resource-name   "NFName"  \nExpected output:\nOutput\nCommand group 'nf' is in preview and under development. Reference and  support levels: https://aka.ms/CLI_refstatus  (ResourceNotFound) The Resource  'Microsoft.ManagedNetworkFabric/NetworkFabrics/NFName' under resource group  'NFResourceGroup' was not found. For more details please go to  https://aka.ms/ARMResourceNotFoundFix  Code: ResourceNotFound"}
{"text": "Create and provision a Cluster using"}
{"text": "Azure CLI\nArticle  04/17/2023\nThis article describes how to create a Cluster by using the Azure Command Line Interface (AzCLI). This document also shows you how to check the status, update, or delete a Cluster."}
{"text": "Prerequisites\nVerify that Network Fabric Controller and Cluster Manger exist in your Azure region Verify that Network Fabric is successfully provisioned"}
{"text": "API guide and metrics\nThe  API guide  provides information on the resource providers and resource models, and the APIs.\nThe metrics generated from the logging data are available in  Azure Monitor metrics ."}
{"text": "Create a Cluster\nThe Cluster resource represents an on-premises deployment of the platform within the Cluster Manager. All other platform-specific resources are dependent upon it for their lifecycle.\nYou should have successfully created the Network Fabric for this on-premises deployment. Each Operator Nexus on-premises instance has a one-to-one association with a Network Fabric.\nCreate the Cluster:\nAzure CLI\naz networkcloud cluster create  --name   "$CLUSTER_NAME"   --location   "$LOCATION"   \     --extended-location   name= "$CL_NAME"   type= "CustomLocation"  \     --resource-group   "$CLUSTER_RG"  \     --analytics-workspace-id   "$LAW_ID"  \     --cluster-location   "$CLUSTER_LOCATION"  \     --network-rack-id   "$AGGR_RACK_RESOURCE_ID"  \ \n   --rack-sku-id   "$AGGR_RACK_SKU" \     --rack-serial-number   "$AGGR_RACK_SN"  \     --rack-location   "$AGGR_RACK_LOCATION"  \     --bare-metal-machine-configuration-data   "[" $AGGR_RACK_BMM "]"  \     --storage-appliance-configuration-data   '[{"adminCredentials": {"password":"$SA_PASS","username":"$SA_USER"},"rackSlot":1,"serialNumber":"$ SA_SN","storageApplianceName":"$SA_NAME"}]'  \     --compute-rack-definitions   '[{"networkRackId": "$COMPX_RACK_RESOURCE_ID",  "rackSkuId": "$COMPX_RACK_SKU", "rackSerialNumber": "$COMPX_RACK_SN",  "rackLocation": "$COMPX_RACK_LOCATION", "storageApplianceConfigurationData":  [], "bareMetalMachineConfigurationData":[{"bmcCredentials":  {"password":"$COMPX_SVRY_BMC_PASS", "username":"$COMPX_SVRY_BMC_USER"},  "bmcMacAddress":"$COMPX_SVRY_BMC_MAC",  "bootMacAddress":"$COMPX_SVRY_BOOT_MAC",  "machineDetails":"$COMPX_SVRY_SERVER_DETAILS",  "machineName":"$COMPX_SVRY_SERVER_NAME"}]}]' \     --managed-resource-group-configuration   name= "$MRG_NAME"   location= "$MRG_LOCATION"  \     --network  fabric -id   "$NFC_ID"  \     --cluster-service-principal   application-id= "$SP_APP_ID"  \       password= "$SP_PASS"   principal-id= "$SP_ID"   tenant-id= "$TENANT_ID"  \     --cluster-type   "$CLUSTER_TYPE"   --cluster-version   "$CLUSTER_VERSION"  \     --tags  $TAG_KEY1= "$TAG_VALUE1"  $TAG_KEY2= "$TAG_VALUE2"  \nYou can instead create a Cluster with ARM template/parameter files in  ARM Template Editor :"}
{"text": "Parameters for cluster operations\nParameter name Description\nCLUSTER_NAME Resource Name of the Cluster\nLOCATION The Azure Region where the Cluster is deployed\nCL_NAME The Cluster Manager Custom Location from Azure portal\nCLUSTER_RG The cluster resource group name\nLAW_ID Log Analytics Workspace ID for the Cluster\nCLUSTER_LOCATION The local name of the Cluster\nAGGR_RACK_RESOURCE_ID RackID for Aggregator Rack\nAGGR_RACK_SKU Rack SKU for Aggregator Rack\nAGGR_RACK_SN Rack Serial Number for Aggregator Rack\nAGGR_RACK_LOCATION Rack physical location for Aggregator Rack\nParameter name Description\nAGGR_RACK_BMM Used for single rack deployment only, empty for multi-rack\nSA_NAME Storage Appliance Device name\nSA_PASS Storage Appliance admin password\nSA_USER Storage Appliance admin user\nSA_SN Storage Appliance Serial Number\nCOMPX_RACK_RESOURCE_ID RackID for CompX Rack, repeat for each rack in compute-rack- definitions\nCOMPX_RACK_SKU Rack SKU for CompX Rack, repeat for each rack in compute- rack-definitions\nCOMPX_RACK_SN Rack Serial Number for CompX Rack, repeat for each rack in compute-rack-definitions\nCOMPX_RACK_LOCATION Rack physical location for CompX Rack, repeat for each rack in compute-rack-definitions\nCOMPX_SVRY_BMC_PASS CompX Rack ServerY BMC password, repeat for each rack in compute-rack-definitions and for each server in rack\nCOMPX_SVRY_BMC_USER CompX Rack ServerY BMC user, repeat for each rack in compute-rack-definitions and for each server in rack\nCOMPX_SVRY_BMC_MAC CompX Rack ServerY BMC MAC address, repeat for each rack in compute-rack-definitions and for each server in rack\nCOMPX_SVRY_BOOT_MAC CompX Rack ServerY boot NIC MAC address, repeat for each rack in compute-rack-definitions and for each server in rack\nCOMPX_SVRY_SERVER_DETAILS CompX Rack ServerY details, repeat for each rack in compute- rack-definitions and for each server in rack\nCOMPX_SVRY_SERVER_NAME CompX Rack ServerY name, repeat for each rack in compute- rack-definitions and for each server in rack\nMRG_NAME Cluster managed resource group name\nMRG_LOCATION Cluster Azure region\nNFC_ID Reference to Network fabric Controller\nSP_APP_ID Service Principal App ID\nSP_PASS Service Principal Password\nSP_ID Service Principal ID\nParameter name Description\nTENANT_ID Subscription tenant ID\nCLUSTER_TYPE Type of cluster, Single or MultiRack\nCLUSTER_VERSION NC Version of cluster\nTAG_KEY1 Optional tag1 to pass to Cluster Create\nTAG_VALUE1 Optional tag1 value to pass to Cluster Create\nTAG_KEY2 Optional tag2 to pass to Cluster Create\nTAG_VALUE2 Optional tag2 value to pass to Cluster Create"}
{"text": "Cluster validation\nA successful Operator Nexus Cluster creation results in the creation of an AKS cluster inside your subscription. The cluster ID, cluster provisioning state and deployment state are returned as a result of a successful \ncluster create\n.\nView the status of the Cluster:\nAzure CLI\naz networkcloud cluster show  --resource-group   "$CLUSTER_RG"  \     --resource-name   "$CLUSTER_RESOURCE_NAME"  \nThe Cluster creation is complete when the \nprovisioningState\n of the resource shows:\n"provisioningState": "Succeeded""}
{"text": "Cluster logging\nCluster create Logs can be viewed in the following locations:\n1. Azure portal Resource/ResourceGroup Activity logs. 2. Azure CLI with \n--debug\n flag passed on command-line."}
{"text": "Deploy Cluster\nOnce a Cluster has been created, the deploy cluster action can be triggered. The deploy Cluster action creates the bootstrap image and deploys the Cluster.\nDeploy Cluster initiates a sequence of events to occur in the Cluster Manager\n1. Validation of the cluster/rack properties 2. Generation of a bootable image for the ephemeral bootstrap cluster (Validation of Infrastructure). 3. Interaction with the IPMI interface of the targeted bootstrap machine. 4. Perform hardware validation checks 5. Monitoring of the Cluster deployment process.\nDeploy the on-premises Cluster:\nAzure CLI\naz networkcloud cluster deploy  \     --name   "$CLUSTER_NAME"  \     --resource-group   "$CLUSTER_RESOURCE_GROUP"  \     --subscription   "$SUBSCRIPTION_ID"  \     --no-wait   --debug   \n  Tip\nTo check the status of the \naz networkcloud cluster deploy\n command, it can be\nexecuted using the \n--debug\n flag. This will allow you to obtain the \nAzure-\nAsyncOperation\n or \nLocation\n header used to query the \noperationStatuses\n resource.\nSee the section  Cluster Deploy Failed  for more detailed steps. Optionally, the command can run asynchronously using the \n--no-wait\n flag."}
{"text": "Cluster Deploy with hardware validation\nDuring a Cluster deploy process, one of the steps executed is hardware validation. The hardware validation procedure runs various test and checks against the machines provided through the Cluster's rack definition. Based on the results of these checks and any user skipped machines, a determination is done on whether sufficient nodes passed and/or are available to meet the thresholds necessary for deployment to continue."}
{"text": "Cluster Deploy Action with skipping specific bare-metal-machine\nA parameter can be passed in to the deploy command that represents the names of bare metal machines in the cluster that should be skipped during hardware validation. Nodes skipped aren't validated and aren't added to the node pool. Additionally, nodes skipped don't count against the total used by threshold calculations.\nAzure CLI\naz networkcloud cluster deploy  \     --name   "$CLUSTER_NAME"  \     --resource-group   "$CLUSTER_RESOURCE_GROUP"  \     --subscription   "$SUBSCRIPTION_ID"  \     --skip-validations-for-machines   "$COMPX_SVRY_SERVER_NAME""}
{"text": "Cluster Deploy failed\nTo track the status of an asynchronous operation, run with a \n--debug\n flag enabled.\nWhen \n--debug\n is specified, the progress of the request can be monitored. The operation\nstatus URL can be found by examining the debug output looking for the \nAzure-\nAsyncOperation\n or \nLocation\n header on the HTTP response to the creation request. The\nheaders can provide the \nOPERATION_ID\n field used in the HTTP API call.\nAzure CLI\nOPERATION_ID= "12312312-1231-1231-1231-123123123123*99399E995..."   az rest  -m  GET  -u   "https://management.azure.com/subscriptions/${SUBSCRIPTION_ID}/providers/Mic rosoft.NetworkCloud/locations/${LOCATION}/operationStatuses/${OPERATION_ID}? api-version=2022-12-12-preview"  \nThe output is similar to the JSON struct example. When the error code is\nHardwareValidationThresholdFailed\n, then the error message contains a list of bare metal\nmachine(s) that failed the hardware validation (for example, \nCOMP0_SVR0_SERVER_NAME\n,\nCOMP1_SVR1_SERVER_NAME\n). These names can be used to parse the logs for further details.\nJSON\n{     "endTime" :  "2023-03-24T14:56:59.0510455Z" ,     "error" : {       "code" :  "HardwareValidationThresholdFailed" ,       "message" :  "HardwareValidationThresholdFailed error hardware validation  threshold for cluster layout plan is not met for cluster $CLUSTER_NAME in  namespace nc-system with listed failed devices $COMP0_SVR0_SERVER_NAME,  $COMP1_SVR1_SERVER_NAME"     },     "id" :  "/subscriptions/$SUBSCRIPTION_ID/providers/Microsoft.NetworkCloud/locations/ $LOCATION/operationStatuses/12312312-1231-1231-1231- 123123123123*99399E995..." ,     "name" :  "12312312-1231-1231-1231-123123123123*99399E995..." ,     "resourceId" :  "/subscriptions/$SUBSCRIPTION_ID/resourceGroups/$CLUSTER_RESOURCE_GROUP/prov\niders/Microsoft.NetworkCloud/clusters/$CLUSTER_NAME" ,     "startTime" :  "2023-03-24T14:56:26.6442125Z" ,     "status" :  "Failed"   } \nSee the article  Tracking Asynchronous Operations Using Azure CLI  for another example."}
{"text": "Cluster deployment validation\nView the status of the cluster:\nAzure CLI\naz networkcloud cluster show  --resource-group   "$CLUSTER_RG"  \     --resource-name   "$CLUSTER_RESOURCE_NAME"  \nThe Cluster deployment is complete when detailedStatus is set to \nRunning\n and\ndetailedStatusMessage shows message \nCluster is up and running\n."}
{"text": "Cluster deployment Logging\nCluster create Logs can be viewed in the following locations:\n1. Azure portal Resource/ResourceGroup Activity logs. 2. Azure CLI with \n--debug\n flag passed on command-line."}
{"text": "Instance readiness testing\nArticle  07/14/2023\nInstance Readiness Testing (IRT) is a framework built to orchestrate real-world workloads for testing of the Azure Operator Nexus Platform."}
{"text": "Environment requirements\nA Linux environment (Ubuntu suggested) capable of calling Azure APIs Knowledge of networks to use for the test Networks to use for the test are specified in a "networks-blueprint.yml" file, see Input Configuration . curl or wget to download IRT package"}
{"text": "Before execution\n1. From your Linux environment, download nexus-irt.tar.gz from aka.ms/nexus-irt\ncurl -Lo nexus-irt.tar.gz aka.ms/nexus-irt\n .\n2. Extract the tarball to the local file system: \nmkdir -p irt && tar xf nexus-\nirt.tar.gz --directory ./irt\n .\n3. Switch to the new directory \ncd irt\n .\n4. The \nsetup.sh\n  script is provided to aid in the initial set up of an environment.\nsetup.sh\n  assumes a nonroot user and attempts to use \nsudo\n , which installs:\na. \njq\n  version 1.6\nyq\n  version 4.33 b. \nc. \nazcopy\n  version 10\naz\n  Azure CLI minimum version not known, stay up to date. d. \ne. \nelinks\n  for viewing html files on the command line\nf. \ntree\n  for viewing directory structures\nmoreutils\n  utilities for viewing progress from the ACI container g. \n5. [Optional] Set up a storage account to archive test results over time. For help, see the  instructions . 6. Log into Azure, if not already logged in: \naz login --use-device\n .\nUser should have \nContributor\n  role\n7. Create an Azure Managed Identity for the container to use.\nUsing the provided script: \nMI_RESOURCE_GROUP="<your resource group>\nMI_NAME="<managed identity name>" SUBSCRIPTION="<subscription>" ./create-\nmanaged-identity.sh\nCan be created manually via the Azure portal, refer to the script for needed permissions 8. Create a service principal and security group. The service principal is used as the executor of the test. The group informs the kubernetes cluster of valid users. The service principal must be a part of the security group, so it has the ability to log into the cluster.\nYou can provide your own, or use our provided script, here's an example of how it could be executed; \nAAD_GROUP_NAME=external-test-aad-group-8\nSERVICE_PRINCIPAL_NAME=external-test-sp-8 ./irt/create-service-\nprincipal.sh\n .\nThis script prints four key/value pairs for you to include in your input file.\n9. If necessary, create the isolation domains required to execute the tests. They aren't lifecycled as part of this test scenario.\nNote:  If deploying isolation domains, your network blueprint must define at least one external network per isolation domain. see \nnetworks-\nblueprint.example.yml\n  for help with configuring your network blueprint.\ncreate-l3-isolation-domains.sh\n  takes one parameter, a path to your\nnetworks blueprint file; here's an example of the script being invoked:\ncreate-l3-isolation-domains.sh ./networks-blueprint.yml"}
{"text": "Input configuration\n1. Build your input file. The IRT tarball provides \nirt-input.example.yml\n  as an\nexample. These values  will not work for all instances , they need to be manually changed and the file also needs to be renamed to \nirt-input.yml\n .\n2. define the values of networks-blueprint input, an example of this file is given in networks-blueprint.example.yml.\nThe network blueprint input schema for IRT is defined in the networks- blueprint.example.yml. Currently IRT has the following network requirements. The networks are created as part of the test, provide network details that aren't in use.\n1. Three (3) L3 Networks\nTwo of them with MTU 1500 One of them with MTU 9000 and shouldn't have fabric_asn definition\n2. One (1) Trunked Network\n3. All vlans should be greater than 500"}
{"text": "Execution\n./irt.sh irt-input.yml\n1. Execute: \nAssumes irt-input.yml is in the same location as irt.sh. If in a different location provides the full file path."}
{"text": "Results\n1. A file named \nsummary-<cluster_name>-<timestamp>.html\n  is downloaded at the end\nof the run and contains the testing results. It can be viewed: a. From any browser b. Using elinks or lynx to view from the command line; for example: i. \nelinks summary-<cluster_name>-<timestamp>..html\nc. When an SAS Token is provided for the \nPUBLISH_RESULTS_TO\n  parameter the\nresults are uploaded to the blob container you specified. It can be previewed by navigating to the link presented to you at the end of the IRT run."}
{"text": "Uploading results to your own archive\ncreate-archive-storage.sh\n  to allow you to set up 1. We offer a supplementary script, \na storage container to store your results. The script generates an SAS Token for a storage container that is valid for three days. The script creates a storage container, storage account, and resource group if they don't already exist. a. The script expects the following environment variables to be defined: i. RESOURCE_GROUP ii. SUBSCRIPTION iii. STORAGE_ACCOUNT_NAME iv. STORAGE_CONTAINER_NAME 2. Copy the last output from the script, into your IRT YAML input. The output looks like this:\nPUBLISH_RESULTS_TO="<sas-token>""}
{"text": "Upgrading cluster runtime from Azure"}
{"text": "CLI\nArticle  06/23/2023\nThis how-to guide explains the steps for installing the required Azure CLI and extensions required to interact with Operator Nexus."}
{"text": "Prerequisites\n1. The  Install Azure CLI  must be installed. 2. the \nnetworkcloud\n extension is required. If the \nnetworkcloud\n extension isn't\ninstalled, it can be installed following the steps listed  here . 3. Access to the Azure portal for the target cluster to be upgraded. 4. You must be logged in to the same subscription as your target cluster via \naz login\n5. Target cluster must be in a running state, with all control plane nodes healthy and 80+% of compute nodes in a running and healthy state."}
{"text": "Finding available runtime versions"}
{"text": "Via Portal\nTo find available upgradeable runtime versions, navigate to the target cluster in the Azure portal. In the cluster's overview pane, navigate to the  Available upgrade versions tab."}
{"text": "From the  available upgrade versions  tab, we're able to see the different cluster versions that are currently available to upgrade. The operator can select from the listed the target runtime versions. Once selected, proceed to upgrade the cluster."}
{"text": ""}
{"text": "Via Azure CLI\nAvailable upgrades are retrievable via the Azure CLI:\nAzure CLI\naz networkcloud cluster show  --name   "clusterName"   --resource-group   "resourceGroup"  \nIn the output, you can find the \navailableUpgradeVersions\n property and look at the\ntargetClusterVersion\n field:\n  "availableUpgradeVersions": [      {        "controlImpact": "True",        "expectedDuration": "Upgrades may take up to 4 hours + 2 hours per  rack",        "impactDescription": "Workloads will be disrupted during rack-by-rack  upgrade",        "supportExpiryDate": "2023-07-31",        "targetClusterVersion": "3.2.0",        "workloadImpact": "True"      }    ], \nIf there are no available cluster upgrades, the list will be empty."}
{"text": "Upgrading cluster runtime using CLI\nTo perform an upgrade of the runtime, use the following Azure CLI command:\nAzure CLI\naz networkcloud cluster update-version  --cluster-name   "clusterName"   -- target-cluster-version      "versionNumber"   --resource-group   "resourceGroupName"  \nThe runtime upgrade is a long process. The upgrade is considered to be finished 80% of compute nodes and 100% of management/control nodes have been successfully upgraded.\nUpgrading all the nodes takes multiple hours but can take more if other processes, like firmware updates, are also part of the upgrade. Due to the length of the upgrade process, it's advised to check the Cluster's detail status periodically for the current state of the upgrade. To check on the status of the upgrade observe the detailed status of the cluster. This check can be done via the portal or az CLI.\nTo view the upgrade status through the Azure portal, navigate to the targeted cluster resource. In the cluster's  Overview  screen, the detailed status is provided along with a detailed status message."}
{"text": "To view the upgrade status through the Azure CLI, use \naz networkcloud cluster show\n.\nAzure CLI\naz networkcloud cluster show  --cluster-name   "clusterName"   --resource-group   "resourceGroupName"  \nThe output should be the target cluster's information and the cluster's detailed status and detail status message should be present."}
{"text": "Frequently Asked Questions"}
{"text": "Identifying Cluster Upgrade Stalled/Stuck\nDuring a runtime upgrade it's possible that the upgrade fails to move forward but the detail status reflects that the upgrade is still ongoing.  Because the runtime upgrade may take a very long time to successfully finish, there is no set timeout length currently specified . Hence, it's advisable to also check periodically on your cluster's detail status and logs to determine if your upgrade is indefinitely attempting to upgrade.\nWe can identify when this is the case by looking at the Cluster's logs, detailed message, and detailed status message. If a timeout has occurred, we would observe that the Cluster is continuously reconciling over the same indefinitely and not moving forward. The Cluster's detailed status message would reflect, \n"Cluster is in the process of\nbeing updated."\n. From here, we recommend checking Cluster logs or configured LAW,\nto see if there is a failure, or a specific upgrade that is causing the lack of progress."}
{"text": "Hardware Failure doesn't require Upgrade re-execution\nIf a hardware failure during an upgrade has occurred, the runtime upgrade continues as long as the set thresholds are met for the compute and management/control nodes. Once the machine is fixed or replaced, it gets provisioned with the current platform runtime's OS, which contains the targeted version of the runtime.\nIf a hardware failure occurs, and the runtime upgrade has failed because thresholds weren't met for compute and control nodes, re-execution of the runtime upgrade may be needed depending on when the failure occurred and the state of the individual servers in a rack. If a rack was updated before a failure, then the upgraded runtime version would be used when the nodes are reprovisioned. If the rack's spec wasn't updated to the upgraded runtime version before the hardware failure, the machine\nwould be provisioned with the previous runtime version. To upgrade to the new runtime version, submit a new cluster upgrade request and only the nodes with the previous runtime version will upgrade. Hosts that were successful in the previous upgrade action won't."}
{"text": "After a runtime upgrade the cluster shows "Failed""}
{"text": "Provisioning State\nDuring a runtime upgrade the cluster will enter a state of "Upgrading." In the event of a failure of the runtime upgrade, for reasons related to the resources, the cluster will go into a "Failed" Provisioning state. This state could be linked to the lifecycle of the components related to the cluster (e.g StorageAppliance) and may be necessary to diagnose the failure with Microsoft support."}
{"text": "Operator Nexus Azure resources"}
{"text": "prerequisites\nArticle  07/14/2023\nTo get started with Operator Nexus, you need to create a Network Fabric Controller (NFC) and then a Cluster Manager (CM) in your target Azure region.\nEach NFC is associated with a CM in the same Azure region and your subscription. The NFC/CM pair lifecycle manages up to 32 Azure Operator Nexus instances deployed in your sites connected to this Azure region.\nYou'll need to complete the prerequisites before you can deploy the Operator Nexus first NFC and CM pair. In subsequent deployments of Operator Nexus, you can skip to creating the NFC and CM."}
{"text": "Resource Provider Registration\nEnsure Azure Subscription for Operator Nexus resources has been permitted access to the necessary Azure Resource Providers: Microsoft.NetworkCloud Microsoft.ManagedNetworkFabric Microsoft.HybridContainerService Microsoft.HybridNetwork Microsoft.Storage Microsoft.Keyvault Microsoft.Network Microsoft.ExtendedLocation Microsoft.HybridCompute Microsoft.HybridConnectivity Microsoft.HybridContainerService Microsoft.Insights Microsoft.Kubernetes Microsoft.KubernetesConfiguration Microsoft.OperationalInsights Microsoft.OperationsManagement Microsoft.ResourceConnector Microsoft.Resources"}
{"text": "Dependent Azure resources setup\nEstablish  ExpressRoute  connectivity from your on-premises network to an Azure Region: ExpressRoute circuit  creation and verification  can be performed via the Azure portal In the ExpressRoute blade, ensure Circuit status indicates the status of the circuit on the Microsoft side. Provider status indicates if the circuit has been provisioned or not provisioned on the service-provider side. For an ExpressRoute circuit to be operational, Circuit status must be Enabled, and Provider status must be Provisioned Set up Azure Key Vault to store encryption and security tokens, service principals, passwords, certificates, and API keys Set up Log Analytics WorkSpace (LAW) to store logs and analytics data for Operator Nexus subcomponents (Network Fabric, Cluster, etc.) Set up Azure Storage account to store Operator Nexus data objects: Azure Storage supports blobs and files accessible from anywhere in the world over HTTP or HTTPS this storage isn't for user/consumer data."}
{"text": "Install CLI Extensions and sign-in to your Azure"}
{"text": "subscription\nInstall latest version of the  necessary CLI extensions ."}
{"text": "Azure subscription sign-in\nAzure CLI\n  az login   az account set  --subscription  $SUBSCRIPTION_ID   az account show\n  Note\nThe account must have permissions to read/write/publish in the subscription"}
{"text": "Create steps\nStep 1:  Create Network Fabric Controller Step 2:  Create Cluster Manager"}
{"text": "Create and modify a network fabric controller by using the"}
{"text": "Azure CLI\nArticle  05/24/2023\nThis article describes how to create a network fabric controller (NFC) for Azure Operator Nexus by using the Azure CLI. This article also shows you how to check the status of and delete an NFC."}
{"text": "Prerequisites\nValidate Azure ExpressRoute circuits for correct connectivity (\nCircuitId\n and \nAuthId\n). NFC provisioning will fail if connectivity is incorrect. Make sure that names, such as for resources, don't contain the underscore (_) character."}
{"text": "Parameters for NFC creation\nParameter Description Values Example Required Type\nResource-\nA resource\nNFCResourceGroupName XYZNFCResourceGroupName\nTrue String group is a\nGroup\ncontainer that holds related resources for an Azure solution.\nLocation\nThe Azure\neastus\n, \nwestus3 eastus\nTrue String region is mandatory to provision your deployment.\nResource-\nThe resource\nnfcname XYZnfcname\nTrue String name is the\nName\nname of the fabric.\nNFC IP\nThis block is\n10.0.0.0/19 10.0.0.0/19\nNot String the NFC IP required\nBlock\nsubnet. The default subnet block is 10.0.0.0/19, and it shouldn't overlap with any of the ExpressRoute IPs.\nParameter Description Values Example Required Type\nExpress\nThe\n--workload-er-connections subscriptions/xxxxxx-xxxxxx-xxxx-xxxx-\nTrue String ExpressRoute\nRoute '[{"expressRouteCircuitId": xxxxxx/resourceGroups/ER-Dedicated-WUS2-AFO-\ncircuit is a\nCircuits "xxxxxx-xxxxxx-xxxx-xxxx- Circuits/providers/Microsoft.Network/expressRouteCircuits/MSFT-\ndedicated\nxxxxxx", ER-Dedicated-PvtPeering-WestUS2-AFO-Ckt-01",\n10G link that\n"expressRouteAuthorizationKey": "expressRouteAuthorizationKey": "xxxxxx-xxxxxx-xxxx-xxxx-\nconnects\n"xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx"}]\nAzure and on-\nxxxxxx"}]'\n   premises. You need to know\n--infra-er-connections\nthe\n'[{"expressRouteCircuitId":\nExpressRoute\n"xxxxxx-xxxxxx-xxxx-xxxx-\ncircuit ID and\nxxxxxx",\nauthentication\n"expressRouteAuthorizationKey":\nkey to\n"xxxxxx-xxxxxx-xxxx-xxxx-\nsuccessfully\nxxxxxx"}]'\nprovision an NFC. There are two ExpressRoute circuits: one for the infrastructure services and one for workload (tenant) services."}
{"text": "Create a network fabric controller\nYou must create a resource group before you create your NFC. Create a separate resource group for each NFC.\nYou create a resource group by running the following command:\nAzure CLI\naz group create  -n  NFCResourceGroupName  -l   "East US"  \nHere's an example of how you can create an NFC by using the Azure CLI:\nAzure CLI\naz nf controller create  \     --resource-group   "NFCResourceGroupName"  \     --location   "eastus"   \     --resource-name   "nfcname"  \     --ipv 4 -address-space   "10.0.0.0/19"  \     --infra-er-connections   '[{"expressRouteCircuitId": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/ER-Dedicated-WUS2-AFO-Circuits/providers/Microsoft.Network/expressRouteCircuits/MSFT-ER- Dedicated-PvtPeering-WestUS2-AFO-Ckt-01", "expressRouteAuthorizationKey": "<auth-key>"}]'      --workload-er-connections   '[{"expressRouteCircuitId": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/ER-Dedicated-WUS2-AFO-Circuits/providers/Microsoft.Network/expressRouteCircuits/MSFT-ER- Dedicated-PvtPeering-WestUS2-AFO-Ckt-01"", "expressRouteAuthorizationKey": "<auth-key>"}]'  \nExpected output:\nJSON\n  "annotation" :  null ,     "id" :  "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/NFCResourceGroupName/providers/Microsoft.ManagedNetworkFabric/networkFabricControllers/nfcna me" ,     "infrastructureExpressRouteConnections" : [      {         "expressRouteAuthorizationKey" :  null ,         "expressRouteCircuitId" :  "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx-xxxxxx/resourceGroups/ER-Dedicated-WUS2-\nAFO-Circuits/providers/Microsoft.Network/expressRouteCircuits/MSFT-ER-Dedicated-PvtPeering-WestUS2-AFO-Ckt-01"       }    ],     "infrastructureServices" :  null ,     "ipv4AddressSpace" :  "10.0.0.0/19" ,     "ipv6AddressSpace" :  null ,     "location" :  "eastus" ,     "managedResourceGroupConfiguration" : {       "location" :  "eastus2euap" ,       "name" :  "nfcname-HostedResources-7DE8EEC1"     },     "name" :  "nfcname" ,     "networkFabricIds" :  null ,     "operationalState" :  null ,     "provisioningState" :  "Accepted" ,     "resourceGroup" :  "NFCresourcegroupname" ,     "systemData" : {       "createdAt" :  "2022-10-31T10:47:08.072025+00:00" ,       "createdBy" :  "email@address.com" ,       "createdByType" :  "User" ,       "lastModifiedAt" :  "2022-10-31T10:47:08.072025+00:00" ,       "lastModifiedBy" :  "email@address.com" , \nNFC creation takes 30 to 45 minutes. Use the \nshow\n command to monitor the progress. Provisioning states include \nAccepted\n,\nUpdating\n, \nSucceeded\n, and \nFailed\n. Delete and re-create the NFC if the creation fails (\nFailed\n)."}
{"text": "Get a network fabric controller\nAzure CLI\n  az nf controller show  --resource-group   "NFCResourceGroupName"   --resource-name   "nfcname"\nExpected output:\nJSON\n{     "annotation" :  null ,     "id" :  "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/NFCResourceGroupName/providers/Microsoft.ManagedNetworkFabric/networkFabricControllers/nfcna me" ,     "infrastructureExpressRouteConnections" : [      {         "expressRouteAuthorizationKey" :  null ,         "expressRouteCircuitId" :  "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx-xxxxxx/resourceGroups/ER-Dedicated-WUS2- AFO-Circuits/providers/Microsoft.Network/expressRouteCircuits/MSFT-ER-Dedicated-PvtPeering-WestUS2-AFO-Ckt-02"       }    ],     "infrastructureServices" : {       "ipv4AddressSpaces" : [ "10.0.0.0/21" ],       "ipv6AddressSpaces" : []    },     "ipv4AddressSpace" :  "10.0.0.0/19" ,     "ipv6AddressSpace" :  null ,     "location" :  "eastus" ,     "managedResourceGroupConfiguration" : {       "location" :  "eastus" ,       "name" :  "nfcname-HostedResources-XXXXXXXX"     },     "name" :  "nfcname" ,     "networkFabricIds" : [],     "operationalState" :  null ,     "provisioningState" :  "Succeeded" ,     "resourceGroup" :  "NFCResourceGroupName" ,     "systemData" : {       "createdAt" :  "2022-10-27T16:02:13.618823+00:00" ,       "createdBy" :  "email@address.com" ,       "createdByType" :  "User" ,       "lastModifiedAt" :  "2022-10-27T17:13:18.278423+00:00" ,       "lastModifiedBy" :  "d1bd24c7-b27f-477e-86dd-939e107873d7" ,       "lastModifiedByType" :  "Application"     }, \n   "tags" :  null ,     "type" :  "microsoft.managednetworkfabric/networkfabriccontrollers" ,     "workloadExpressRouteConnections" : [      {         "expressRouteAuthorizationKey" :  null ,         "expressRouteCircuitId" :  "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx-xxxxxx/resourceGroups/ER-Dedicated-WUS2- AFO-Circuits/providers/Microsoft.Network/expressRouteCircuits/MSFT-ER-Dedicated-PvtPeering-WestUS2-AFO-Ckt-03"       }    ],     "workloadManagementNetwork" :  true ,     "workloadServices" : {       "ipv4AddressSpaces" : [ "10.0.28.0/22" ],       "ipv6AddressSpaces" : []    }  }"}
{"text": "Delete a network fabric controller\nYou should delete an NFC only after deleting all associated network fabrics. Use this command to delete an NFC:\nAzure CLI\n  az nf controller delete  --resource-group   "NFCResourceGroupName"   --resource-name   "nfcname"  \nExpected output:\nJSON\n"name" :  "nfcname" ,       "networkFabricIds" : [],       "operationalState" :  null ,       "provisioningState" :  "succeeded" ,       "resourceGroup" :  "NFCResourceGroupName" ,       "systemData" : {         "createdAt" :  "2022-10-31T10:47:08.072025+00:00" , \nIt takes 30 minutes for the deletion to finish. In the Azure portal, verify that the hosted resources are deleted."}
{"text": "Next steps\nAfter you successfully create an NFC, the next step is to create a  cluster manager ."}
{"text": "Cluster Manager: How to manage the"}
{"text": "Cluster Manager in Operator Nexus\nArticle  04/05/2023\nThe Cluster Manager is deployed in the operator's Azure subscription to manage the lifecycle of Operator Nexus Clusters."}
{"text": "Before you begin\nYou'll need:\nAzure Subscription ID  - The Azure subscription ID where Cluster Manager needs to be created (should be the same subscription ID of the Network Fabric Controller). Network Fabric Controller ID  - Network Fabric Controller and Cluster Manager have a 1:1 association. You'll need the resource ID of the Network Fabric Controller associated with the Cluster Manager. Log Analytics Workspace ID  - The resource ID of the Log Analytics Workspace used for the logs collection. Azure Region  - The Cluster Manager should be created in the same Azure region as the Network Fabric Controller. This Azure region should be used in the \nLocation\nfield of the Cluster Manager and all associated Operator Nexus instances."}
{"text": "Global arguments\nSome arguments that are available for every Azure CLI command\n--debug  - prints even more information about CLI operations, used for debugging purposes. If you find a bug, provide output generated with the \n--debug\n flag on\nwhen submitting a bug report. --help -h  - prints CLI reference information about commands and their arguments and lists available subgroups and commands. --only-show-errors  - Only show errors, suppressing warnings. --output -o  - specifies the output format. The available output formats are Json, Jsonc (colorized JSON), tsv (Tab-Separated Values), table (human-readable ASCII tables), and yaml. By default the CLI outputs Json. --query  - uses the JMESPath query language to filter the output returned from Azure services.\n--verbose  - prints information about resources created in Azure during an operation, and other useful information"}
{"text": "Cluster Manager elements\nElements Description\nName, ID, location, tags, type Name: User friendly name   ID: < Resource ID >   Location: Azure region where the Cluster Manager is created. Values from: \naz account list -locations\n.  Tags: Resource tags   Type: Microsoft.NetworkCloud/clusterManagers\nmanagerExtendedLocation The ExtendedLocation associated with the Cluster Manager\nmanagedResourceGroupConfiguration Information about the Managed Resource Group\nfabricControllerId A reference to the Network Fabric Controller that is 1:1 with this Cluster Manager\nanalyticsWorkspaceId This workspace will be where any logs that 's relevant to the customer will be relayed.\nclusterVersions[] List of ClusterAvailableVersions objects.   Cluster versions that the manager supports. Will be used as an input in the cluster clusterVersion property.\nprovisioningState Succeeded, Failed, Canceled, Provisioning, Accepted, Updating\ndetailedStatus Detailed statuses that provide additional information about the status of the Cluster Manager.\ndetailedStatusMessage Descriptive message about the current detailedStatus."}
{"text": "Create a Cluster Manager\nUse the \naz networkcloud clustermanager create\n command to create a Cluster Manager.\nThis command creates a new Cluster Manager or updates the properties of the Cluster Manager if it exists. If you have multiple Azure subscriptions, select the appropriate subscription ID using the  az account set  command.\nAzure CLI\naz networkcloud clustermanager create  \       --name   <Cluster Manager name>  \       --location   <region>  \       --analytics-workspace-id   <log analytics workspace ID>        --fabric-controller-id   <Fabric controller ID associated with this  Cluster Manager>        --managed-resource-group-configuration   < name=<Managed Resource group  Name>   location= <Managed Resource group location>  >       --tags   <key=value key=value>        --resource-group   <Resource Group Name>        --subscription   <subscription ID>  \nArguments --name -n [Required]  - The name of the Cluster Manager. --fabric-controller-id [Required]  - The resource ID of the Network Fabric Controller that is associated with the Cluster Manager. --resource-group -g [Required]  - Name of resource group. You can configure the default resource group using \naz configure --defaults group=<name>\n.\n--analytics-workspace-id  - The resource ID of the Log Analytics Workspace that is used for the logs collection --location -l  - Location. Azure region where the Cluster Manager is created. Values from: \naz account list -locations\n. You can configure the default\nlocation using \naz configure --defaults location=<location>\n.\n--managed-resource-group-configuration  - The configuration of the managed resource group associated with the resource. Usage: --managed-resource-group-configuration location=XX name=XX location: The region of the managed resource group. If not specified, the region of the parent resource is chosen. name: The name for the managed resource group. If not specified, a unique name is automatically generated. wait/--no-wait  - Wait for command to complete or don't wait for the long- running operation to finish. --tags  - Space-separated tags: key[=value] [key[=value]...]. Use '' to clear existing tags --subscription  - Name or ID of subscription. You can configure the default subscription using \naz account set -s NAME_OR_ID\n."}
{"text": "List/show Cluster Manager(s)\nList and show commands are used to get a list of existing Cluster Managers or the properties of a specific Cluster Manager."}
{"text": "List Cluster Managers in resource group\nThis command lists the Cluster Managers in the specified Resource group.\nAzure CLI\naz networkcloud clustermanager list  --resource-group   <Azure Resource group>"}
{"text": "List Cluster Managers in subscription\nThis command lists the Cluster Managers in the specified subscription.\nAzure CLI\naz networkcloud clustermanager list    --subscription   <subscription ID>"}
{"text": "Show Cluster Manager properties\nThis command lists the properties of the specified Cluster Manager.\nAzure CLI\naz networkcloud clustermanager show  \       --name   <Cluster Manager name>  \       --resource-group   <Resource group Name>        --subscription   <subscription ID>"}
{"text": "List/show command arguments\n--name -n  - The name of the Cluster Manager. --IDs  - One or more resource IDs (space-delimited). It should be a complete resource ID containing all information of 'Resource ID' arguments. --resource-group -g  - Name of resource group. You can configure the default group using \naz configure --defaults group=<name>\n.\n--subscription  - Name or ID of subscription. You can configure the default subscription using \naz account set -s NAME_OR_ID\n."}
{"text": "Update Cluster Manager\nThis command is used to patch properties of the provided Cluster Manager, or update the tags assigned to the Cluster Manager. Properties and tag updates can be done\nindependently.\nAzure CLI\naz networkcloud clustermanager update  \       --name   <Cluster Manager name>  \       --tags   < <key1=value1>   <key2=value2> >       --resource-group   <Resource group Name>        --subscription   <subscription ID>  \nArguments --tags  - TSpace-separated tags: key[=value] [key[=value] ...]. Use '' to clear existing tags. --name -n  - The name of the Cluster Manager. --IDs  - One or more resource IDs (space-delimited). It should be a complete resource ID containing all information of 'Resource ID' arguments. --resource-group -g  - Name of resource group. You can configure the default group using \naz configure --defaults group=<name>\n.\n--subscription  - Name or ID of subscription. You can configure the default subscription using \naz account set -s NAME_OR_ID\n."}
{"text": "Delete Cluster Manager\nThis command is used to Delete the provided Cluster Manager.\n  Warning\nA Cluster Manager that has an existing associated Network Fabric Controller, or any Clusters that reference this Cluster Manager may not be deleted.\nAzure CLI\naz networkcloud clustermanager delete  \       --name   <Cluster Manager name>  \       --resource-group   <Resource Group Name>        --subscription   <subscription ID>  \nArguments --no-wait  - Don't wait for the long-running operation to complete. --yes -y  - Don't prompt for confirmation. --name -n  - The name of the Cluster Manager.\n--IDs  - One or more resource IDs (space-delimited). It should be a complete resource ID containing all information of 'Resource ID' arguments. --resource-group -g  - Name of resource group. You can configure the default group using \naz configure --defaults group=<name>\n.\n--subscription  - Name or ID of subscription. You can configure the default subscription using \naz account set -s NAME_OR_ID\n."}
{"text": "Operator Nexus platform prerequisites\nArticle  03/14/2023\nOperators will need to complete the prerequisites before the deploy of the Operator Nexus platform software. Some of these steps may take extended amounts of time, thus, a review of these prerequisites may prove beneficial.\nIn subsequent deployments of Operator Nexus instances, you can skip to creating the on-premises  Network Fabric  and the  Cluster ."}
{"text": "Azure prerequisites\nWhen deploying Operator Nexus for the first time or in a new region, you'll first need to create a Network Fabric Controller and then a Cluster Manager as specified  here . Additionally, the following tasks will need to be accomplished:\nSet up users, policies, permissions, and RBAC Set up Resource Groups to place and group resources in a logical manner that will be created for Operator Nexus platform. Establish ExpressRoute connectivity from your WAN to an Azure Region"}
{"text": "On your premises prerequisites\nWhen deploying Operator Nexus on-premises instance in your datacenter, various teams are likely involved to perform a variety of roles. The following tasks must be performed accurately in order to ensure a successful platform software installation."}
{"text": "Physical hardware setup\nAn operator that wishes to take advantage of the Operator Nexus service will need to purchase, install, configure, and operate hardware resources. This section of the document will describe the necessary components and efforts to purchase and implement the appropriate hardware systems. This section will discuss the bill of materials, the rack elevations diagram and the cabling diagram, as well as the steps required to assemble the hardware."}
{"text": "Using the Bill of Materials (BOM)\nTo ensure a seamless operator experience, Operator Nexus has developed a BOM for the hardware acquisition necessary for the service. This BOM is a comprehensive list of the necessary components and quantities needed to implement the environment for a successful implementation and maintenance of the on-premises instance. The BOM is structured to provide the operator with a series of stock keeping units (SKU) that can be ordered from hardware vendors. SKUs will be discussed later in the document."}
{"text": "Using the elevation diagram\nThe rack elevation diagram is a graphical reference that demonstrates how the servers and other components fit into the assembled and configured racks. The rack elevation diagram is provided as part of the overall build instructions and will help the operators staff to correctly configure and install all of the hardware components necessary for service operation."}
{"text": "Cabling diagram\nCabling diagrams are graphical representations of the cable connections that are required to provide network services to components installed within the racks. Following the cabling diagram ensures proper implementation of the various components in the build."}
{"text": "How to order based on SKU"}
{"text": "SKU definition\nA SKU is an inventory management and tracking method that allows grouping of multiple components into a single designator. A SKU allows an operator to order all needed components with through specify one SKU number. This expedites the operator and vendor interaction while reducing ordering errors due to complex parts lists."}
{"text": "Placing a SKU based order\nOperator Nexus has created a series of SKUs with vendors such as Dell, Pure Storage and Arista that the operator will be able to reference when they place an order. Thus, an operator simply needs to place an order based on the SKU information provided by Operator Nexus to the vendor to receive the correct parts list for the build."}
{"text": "How to build the physical hardware footprint\nThe physical hardware build is executed through a series of steps which will be detailed in this section. There are three prerequisite steps prior to the build execution. This section will also discuss assumptions concerning the skills of the operator's employees to execute the build."}
{"text": "Ordering and receipt of the specific hardware infrastructure SKU\nThe ordering of the appropriate SKU and delivery of hardware to the site must occur before the start of building. Adequate time should be allowed for this step. We recommend the operator communicate with the supplier of the hardware early in the process to ensure and understand delivery timeframes."}
{"text": "Site preparation\nThe installation site must be capable of supporting the hardware infrastructure from a space, power, and network perspective. The specific site requirements will be defined by the SKU purchased for the site. This step can be accomplished after the order is placed and before the receipt of the SKU."}
{"text": "Scheduling resources\nThe build process will require several different staff members to perform the build, such as engineers to provide power, network access and cabling, systems staff to assemble the racks, switches, and servers, to name a few. To ensure that the build is accomplished in a timely manner, we recommend scheduling these team members in advance based on the delivery schedule."}
{"text": "Assumptions regarding build staff skills\nThe staff performing the build should be experienced at assembling systems hardware such as racks, switches, PDUs and servers. The instructions provided will discuss the steps of the process, while referencing rack elevations and cabling diagrams."}
{"text": "Build process overview\nIf the site preparation is complete and validated to support the ordered SKU, the build process occurs in the following steps:\n1. Assemble the racks based on the rack elevations of the SKU. Specific rack assembly instructions will be provided by the rack manufacturer.\n2. After the racks are assembled, install the fabric devices in the racks per the elevation diagram. 3. Cable the fabric devices by connecting the network interfaces per the cabling diagram. 4. Assemble and install the servers per rack elevation diagram. 5. Assemble and install the storage device per rack elevation diagram. 6. Cable the server and storage devices by connecting the network interfaces per the cabling diagram. 7. Cable power from each device. 8. Review/validate the build through the checklists provided by Operator Nexus and other vendors."}
{"text": "How to visually inspect the physical hardware installation\nIt is recommended to label on all cables following ANSI/TIA 606 Standards, or the operator's standards, during the build process. The build process should also create reverse mapping for cabling from a switch port to far end connection. The reverse mapping can be compared to the cabling diagram to validate the installation."}
{"text": "Terminal Server and storage array setup\nNow that the physical installation and validation has completed, the next steps involved configuring up the default settings required before platform software installation."}
{"text": "Set up Terminal Server\nTerminal Server has been deployed and configured as follows:\nTerminal Server is configured for Out-of-Band management Authentication credentials have been set up DHCP client is enabled on the out-of-band management port HTTP access is enabled Terminal Server interface is connected to the operators on-premises Provider Edge routers (PEs) and configured with the IP addresses and credentials Terminal Server is accessible from the management VPN\n1. Setup hostname:  CLI Reference\nBash\nsudo ogcli update system/hostname hostname=\"$TS_HOSTNAME\" \nParameter name Description\nTS_HOSTNAME The terminal server hostname\n2. Setup network:\nBash\nsudo ogcli create conn <<  'END'     description= "PE1 to TS NET1"     mode= "static"     ipv4_static_settings.address= "$TS_NET1_IP"     ipv4_static_settings.netmask= "$TS_NET1_NETMASK"     ipv4_static_settings.gateway= "$TS_NET1_GW"     physif= "net1"     END \nsudo ogcli create conn <<  'END'     description= "PE2 to TS NET2"     mode= "static"     ipv4_static_settings.address= "$TS_NET2_IP"     ipv4_static_settings.netmask= "$TS_NET2_NETMASK"     ipv4_static_settings.gateway= "$TS_NET2_GW"     physif= "net2"     END \nParameter name Description\nTS_NET1_IP The terminal server PE1 to TS NET1 IP\nTS_NET1_NETMASK The terminal server PE1 to TS NET1 netmask\nTS_NET1_GW The terminal server PE1 to TS NET1 gateway\nTS_NET2_IP The terminal server PE2 to TS NET2 IP\nTS_NET2_NETMASK The terminal server PE2 to TS NET2 netmask\nTS_NET2_GW The terminal server PE2 to TS NET2 gateway\n3. Setup support admin user:\nFor each port\nBash\nogcli create user <<  'END'   description= "Support Admin User"   enabled= true   groups[0]= "admin"  \ngroups[1]= "netgrp"   hashed_password= "$HASHED_SUPPORT_PWD"   username= "$SUPPORT_USER"   END \nParameter name Description\nSUPPORT_USER Support admin user\nHASHED_SUPPORT_PWD Encoded support admin user password\n4. Verify settings:\nBash\n ping $PE1_IP -c 3   # ping test to PE1    ping $PE2_IP -c 3  # ping test to PE2    ogcli get conns  # verify NET1, NET2    ogcli get users  # verify support admin user    ogcli get static_routes  # there should be no static routes    ip r  # verify only interface routes    ip a  # verify loopback, NET1, NET2"}
{"text": "Set up storage array\n1. Operator needs to install the storage array hardware as specified by the BOM and rack elevation within the Aggregation Rack. 2. Operator will need to provide the storage array Technician with information, in order for the storage array Technician to arrive on-site to configure the appliance. 3. Required location-specific data that will be shared with storage array technician:\nCustomer Name: Physical Inspection Date: Chassis Serial Number: Storage array Array Hostname: CLLI code (Common Language location identifier): Installation Address: FIC/Rack/Grid Location:\n4. Data provided to the operator and shared with storage array technician, which will be common to all installations:\nPurity Code Level: 6.1.14 Array Time zone: UTC\nDNS Server IP Address: 172.27.255.201 DNS Domain Suffix: not set by operator during setup NTP Server IP Address or FQDN: 172.27.255.212 Syslog Primary: 172.27.255.210 Syslog Secondary: 172.27.255.211 SMTP Gateway IP address or FQDN: not set by operator during setup Email Sender Domain Name: not set by operator during setup Email Address(es) to be alerted: not set by operator during setup Proxy Server and Port: not set by operator during setup Management: Virtual Interface IP Address: 172.27.255.200 Gateway: 172.27.255.1 Subnet Mask: 255.255.255.0 MTU: 1500 Bond: not set by operator during setup Management: Controller 0 IP Address: 172.27.255.254 Gateway: 172.27.255.1 Subnet Mask: 255.255.255.0 MTU: 1500 Bond: not set by operator during setup Management: Controller 1 IP Address: 172.27.255.253 Gateway: 172.27.255.1 Subnet Mask: 255.255.255.0 MTU: 1500 Bond: not set by operator during setup VLAN Number / Prefix: 43 ct0.eth10: not set by operator during setup ct0.eth11: not set by operator during setup ct0.eth18: not set by operator during setup ct0.eth19: not set by operator during setup ct1.eth10: not set by operator during setup ct1.eth11: not set by operator during setup ct1.eth18: not set by operator during setup ct1.eth19: not set by operator during setup"}
{"text": "Default setup for other devices installed\nAll network fabric devices (except for the Terminal Server) are set to \nZTP\n mode\nServers have default factory settings"}
{"text": "Install CLI extensions and sign-in to your Azure"}
{"text": "subscription\nInstall latest version of the  necessary CLI extensions ."}
{"text": "Azure subscription sign-in\nAzure CLI\n  az login    az account set  --subscription  $SUBSCRIPTION_ID    az account show \n  Note\nThe account must have permissions to read/write/publish in the subscription"}
{"text": "Create and provision a network fabric"}
{"text": "by using the Azure CLI\nArticle  05/24/2023\nThis article describes how to create a network fabric for Azure Operator Nexus by using the Azure CLI. This article also shows you how to check the status of, update, and delete a network fabric."}
{"text": "Prerequisites\nAn Azure account with an active subscription. The latest version of the Azure CLI commands (2.0 or later). For more information, see  Install the Azure CLI . A network fabric controller (NFC) that manages multiple network fabrics in the same Azure region. A physical Azure Operator Nexus instance with cabling, as described in the bill of materials (BoM). Azure ExpressRoute connectivity between NFC and Azure Operator Nexus instances. A terminal server  installed and configured  with a username and password. Provider edge (PE) devices preconfigured with necessary VLANs, route targets, and IP addresses.\nSupported SKUs for network fabric instances are:\nM4-A400-A100-C16-aa for up to four compute racks M8-A400-A100-C16-aa for up to eight compute racks"}
{"text": "Steps to provision a fabric and racks\n1. Create a network fabric by providing racks, server count, SKU, and network configuration. 2. Create a network-to-network interconnect (NNI) by providing Layer 2 and Layer 3 parameters. 3. Update the serial number in the network device resource with the actual serial number on the device. The device sends the serial number as part of a DHCP request. 4. Configure the terminal server (which also hosts the DHCP server) with the serial numbers of all the devices.\n5. Provision the network devices via zero-touch provisioning mode. Based on the serial number in the DHCP request, the DHCP server responds with the boot configuration file for the corresponding device."}
{"text": "Configure a network fabric\nThe following table specifies parameters that you use to create a network fabric. In the table, \n$prefix\n is \n/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx-\nxxxxxx/resourceGroups/NFResourceGroupName/providers/Microsoft.ManagedNetworkFabric/\nnetworkFabricControllers\n.\nParameter Description Example Required Type\nresource-group\nName of the resource\nNFResourceGroup\nTrue group.\nlocation\nAzure Operator\neastus\nTrue Nexus region.\nresource-name\nName of the fabric\nNF-ResourceName\nTrue resource.\nnf-sku\nFabric SKU ID, which\nM4-A400-A100-\nTrue String is the SKU of the\nC16-aa\nordered BoM. The two supported SKUs are M4-A400-A100- C16-aa and M8- A400-A100-C16-aa.\nnfc-id\nAzure Resource\n$prefix/NFCName\nTrue Manager resource ID for the network fabric controller.\nrackcount\nNumber of compute\n8\nTrue racks per fabric. Possible values are \n2\nto \n8\n.\nserverCountPerRack\nNumber of compute\n16\nTrue servers per rack. Possible values are \n4\n,\n8\n, \n12\n, and \n16\n.\nParameter Description Example Required Type\nipv4Prefix\nIPv4 prefix of the\n10.246.0.0/19\nTrue management network. This prefix should be unique across all network fabrics in a network fabric controller. Prefix length should be at least 19 (/20 isn't allowed, but /18 and lower are allowed).\nipv6Prefix\nIPv6 prefix of the\n10:5:0:0::/59\nTrue management network. This prefix should be unique across all network fabrics in a network fabric controller.\nmanagement-network-config\nDetails of the True management network.\ninfrastructureVpnConfiguration\nDetails of the True management VPN connection between the network fabric and infrastructure services in the network fabric controller.\noptionBProperties\nDetails of MPLS True Option 10B, which is used for connectivity between the network fabric and the network fabric controller.\nParameter Description Example Required Type\nimportRouteTargets\nValues of import\n65048:10039\nTrue (if route targets to be Option B configured on is customer edges (CEs) enabled) for exchanging routes between a CE and provider edge (PE) via MPLS Option 10B.\nexportRouteTargets\nValues of export route\n65048:10039\nTrue (if targets to be Option B configured on CEs for is exchanging routes enabled) between a CE and a PE via MPLS Option 10B.\nworkloadVpnConfiguration\nDetails of the workload VPN connection between the network fabric and workload services in the network fabric controller.\noptionBProperties\nDetails of MPLS Option 10B, which is used for connectivity between the network fabric and the network fabric controller.\nimportRouteTargets\nValues of import\n65048:10050\nTrue (if route targets to be Option B configured on CEs for is exchanging routes enabled) between a CE and a PE via MPLS Option 10B.\nexportRouteTargets\nValues of export route\n65048:10050\nTrue (if targets to be Option B configured on CEs for is exchanging routes enabled) between a CE and a PE via MPLS Option 10B.\nParameter Description Example Required Type\nts-config\nTerminal server True configuration details.\nprimaryIpv4Prefix\nIPv4 prefix for\n20.0.10.0/30\n; True connectivity between the terminal the terminal server server interface and the primary PE. for the primary The terminal server network is interface for the assigned primary network is\n20.0.10.1\n, and assigned the first the PE interface usable IP from the is assigned prefix. The\n20.0.10.2\n. corresponding interface on the PE is assigned the second usable address.\nsecondaryIpv4Prefix\nIPv4 prefix for\n20.0.0.4/30\n; the True connectivity between terminal server the terminal server interface for the and the secondary PE. secondary The terminal server network is interface for the assigned secondary network is\n20.0.10.5\n, and assigned the first the PE interface usable IP from the is assigned prefix. The\n20.0.10.6\n. corresponding interface on the PE is assigned the second usable address.\nusername\nUsername that the True services use to configure the terminal server.\npassword\nPassword that the True services use to configure the terminal server.\nserialNumber\nSerial number of the terminal server."}
{"text": "Create a network fabric\nYou must create a resource group before you create a network fabric. We recommend that you create a separate resource group for each network fabric. You can create a resource group by using the following command:\nAzure CLI\naz group create  -n  NFResourceGroup  -l   "East US"  \nRun the following command to create the network fabric. The rack count is either \n4\n or\n8\n, depending on your setup.\nAzure CLI\naz nf fabric create  \   --resource-group   "NFResourceGroupName"    --location   "eastus"  \  --resource-name   "NFName"  \  --nf-sku   "NFSKU"  \  --nfc-id   "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/NFResourceGroupName/providers/Microsoft.ManagedNetwork Fabric/networkFabricControllers/NFCName"    --fabric-asn  65048   --ipv 4 -prefix  10.2.0.0/19   --ipv 6 -prefix  fda0:d59c:da02::/59   --rack-count  4  --server-count-per-rack  8  --ts-config   '{"primaryIpv4Prefix":"20.0.1.0/30",  "secondaryIpv4Prefix":"20.0.0.0/30", "username":"****", "password": "****",  "serialNumber":"TerminalServerSerialNumber"}'    --managed-network-config   '{"infrastructureVpnConfiguration": {"peeringOption":"OptionB","optionBProperties":{"importRouteTargets": ["65048:10039"],"exportRouteTargets":["65048:10039"]}},  "workloadVpnConfiguration":{"peeringOption": "OptionB", "optionBProperties":  {"importRouteTargets": ["65048:10050"], "exportRouteTargets":  ["65048:10050"]}}}'  \nExpected output:\nOutput\n{    "annotation": null,    "fabricAsn": 65048,    "id": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/NFResourceGroup/providers/Microsoft.ManagedNetworkFabr ic/networkFabrics/NFName",    "ipv4Prefix": "10.2.0.0/19",    "ipv6Prefix": "fda0:d59c:da02::/59", \n  "l2IsolationDomains": null,    "l3IsolationDomains": null,    "location": "eastus",    "managementNetworkConfiguration": {      "infrastructureVpnConfiguration": {        "administrativeState": "Enabled",        "networkToNetworkInterconnectId": null,        "optionAProperties": null,        "optionBProperties": {          "exportRouteTargets": [            "65048:10039"          ],          "importRouteTargets": [            "65048:10039"          ]        },        "peeringOption": "OptionB"      },      "workloadVpnConfiguration": {       "administrativeState": "Enabled",        "networkToNetworkInterconnectId": null,        "optionAProperties": null,        "optionBProperties": {          "exportRouteTargets": [            "65048:10050"          ],          "importRouteTargets": [            "65048:10050"          ]        },        "peeringOption": "OptionB"      }    },    "name": "NFName",    "networkFabricControllerId": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/NFCResourceGroupName/providers/Microsoft.ManagedNetwor kFabric/networkFabricControllers/NFCName",    "networkFabricSku": "NFSKU",    "operationalState": null,    "provisioningState": "Accepted",    "rackCount": 4,    "racks": null,    "resourceGroup": "NFResourceGroupName",    "routerId": null,    "serverCountPerRack": 8,    "systemData": {      "createdAt": "2023-XX-X-6T12:52:11.769525+00:00",      "createdBy": "email@address.com",      "createdByType": "User",      "lastModifiedAt": "2023-XX-XX-6T12:52:11.769525+00:00",      "lastModifiedBy": "email@address.com",      "lastModifiedByType": "User"    },    "tags": null,    "terminalServerConfiguration": { \n    "networkDeviceId": null,      "password": null,      "primaryIpv4Prefix": "20.0.1.0/30",      "primaryIpv6Prefix": null,      "secondaryIpv4Prefix": "20.0.0.0/30",      "secondaryIpv6Prefix": null,      "serialNumber": "TerminalServerSerialNumber",      "username": "****"    },    "type": "microsoft.managednetworkfabric/networkfabrics"  }"}
{"text": "Show network fabrics\nAzure CLI\naz nf fabric show  --resource-group   "NFResourceGroupName"   --resource-name   "NFName"  \nExpected output:\nOutput\n{    "annotation": null,    "fabricAsn": 65048,    "id": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/NFResourceGroup/providers/Microsoft.ManagedNetworkFabr ic/networkFabrics/NFName",    "ipv4Prefix": "10.2.0.0/19",    "ipv6Prefix": "fda0:d59c:da02::/59",    "l2IsolationDomains": null,    "l3IsolationDomains": null,    "location": "eastus",    "managementNetworkConfiguration": {      "infrastructureVpnConfiguration": {        "administrativeState": "Enabled",        "networkToNetworkInterconnectId": null,        "optionAProperties": null,        "optionBProperties": {          "exportRouteTargets": [            "65048:10039"          ],          "importRouteTargets": [            "65048:10039"          ]        },        "peeringOption": "OptionB"      },      "workloadVpnConfiguration": {\n      "administrativeState": "Enabled",        "networkToNetworkInterconnectId": null,        "optionAProperties": null,        "optionBProperties": {          "exportRouteTargets": [            "65048:10050"          ],          "importRouteTargets": [            "65048:10050"          ]        },        "peeringOption": "OptionB"      }    },    "name": "nffab1031623",    "networkFabricControllerId": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/NFCResourceGroupName/providers/Microsoft.ManagedNetwor kFabric/networkFabricControllers/NFCName",    "networkFabricSku": "NFSKU",    "operationalState": null,    "provisioningState": "Succeeded",    "rackCount": 4,    "racks": [      "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourcegroups/NFResourceGroup/providers/microsoft.managednetworkfabr ic/networkracks/NFName-aggrack",      "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourcegroups/NFResourceGroup/providers/microsoft.managednetworkfabr ic/networkracks/NFName-comprack1",      "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourcegroups/NFResourceGroup/providers/microsoft.managednetworkfabr ic/networkracks/NFName-comprack2"   ],    "resourceGroup": "NFResourceGroup",    "routerId": null,    "serverCountPerRack": 8,    "systemData": {      "createdAt": "2023-XX-XXT12:52:11.769525+00:00",      "createdBy": "email@address.com",      "createdByType": "User",      "lastModifiedAt": "2023-XX-XXT12:53:02.504974+00:00",      "lastModifiedBy": "d1bd24c7-b27f-477e-86dd-939e107873d7",      "lastModifiedByType": "Application"    },    "tags": null,    "terminalServerConfiguration": {      "networkDeviceId": null,      "password": null,      "primaryIpv4Prefix": "20.0.1.0/30",      "primaryIpv6Prefix": null,      "secondaryIpv4Prefix": "20.0.0.0/30",      "secondaryIpv6Prefix": null,      "serialNumber": "TerminalServerSerialNumber",      "username": "****"    }, \n  "type": "microsoft.managednetworkfabric/networkfabrics"  }"}
{"text": "List all network fabrics in a resource group\nAzure CLI\naz nf fabric list  --resource-group   "NFResourceGroup"    \nExpected output:\nOutput\n{      "annotation": null,      "fabricAsn": 65048,      "id": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/NFResourceGroup/providers/Microsoft.ManagedNetworkFabr ic/networkFabrics/NFName",      "ipv4Prefix": "10.2.0.0/19",      "ipv6Prefix": "fda0:d59c:da02::/59",      "l2IsolationDomains": [Null],         "l3IsolationDomains": [Null],     "location": "eastus",      "managementNetworkConfiguration": {        "infrastructureVpnConfiguration": {          "administrativeState": "Enabled",          "networkToNetworkInterconnectId": null,          "optionAProperties": null,          "optionBProperties": {            "exportRouteTargets": [             "65048:10039"            ],            "importRouteTargets": [             "65048:10039"            ]          },          "peeringOption": "OptionB"        },        "workloadVpnConfiguration": {          "administrativeState": "Enabled",          "networkToNetworkInterconnectId": null,          "optionAProperties": null,          "optionBProperties": {            "exportRouteTargets": [             "65048:10050"            ],            "importRouteTargets": [             "65048:10050"            ] \n        },          "peeringOption": "OptionB"        }      },      "name": "NFName",      "networkFabricControllerId": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/NFCResourceGroup/providers/Microsoft.ManagedNetworkFab ric/networkFabricControllers/NFCName",      "networkFabricSku": "NFSKU",      "operationalState": "Provisioned",      "provisioningState": "Succeeded",      "rackCount": 4,      "racks": [        "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourcegroups/NFResourceGroup/providers/microsoft.managednetworkfabr ic/networkracks/NFName-aggrack",        "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourcegroups/NFResourceGroup/providers/microsoft.managednetworkfabr ic/networkracks/NFName-comprack1",        "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourcegroups/NFResourceGroup/providers/microsoft.managednetworkfabr ic/networkracks/NFName-comprack2"     ],      "resourceGroup": "NFResourceGroup",      "routerId": null,      "serverCountPerRack": 8,      "systemData": {        "createdAt": "2023-XX-XXT12:52:11.769525+00:00",        "createdBy": "email@address.com",        "createdByType": "User",        "lastModifiedAt": "2023-XX-XXT02:05:44.043591+00:00",        "lastModifiedBy": "d1bd24c7-b27f-477e-86dd-939e107873d7",        "lastModifiedByType": "Application"      },      "tags": null,      "terminalServerConfiguration": {        "networkDeviceId": null,        "password": null,        "primaryIpv4Prefix": "20.0.1.0/30",        "primaryIpv6Prefix": null,        "secondaryIpv4Prefix": "20.0.0.0/30",        "secondaryIpv6Prefix": null,        "serialNumber": "TerminalServerSerialNumber",        "username": "****"      },      "type": "microsoft.managednetworkfabric/networkfabrics"    }"}
{"text": "Configure an NNI\nThe following table specifies the parameters that you use to create a network-to- network interconnect.\nParameter Description Example Required Type\nisMangementType\nConfiguration to use an\nTrue\nTrue NNI for management of the fabric. Possible values are \nTrue\n and\nFalse\n. The default value is \nTrue\n.\nuseOptionB\nConfiguration to enable\nTrue\nTrue Option B. Possible values are \nTrue\n and\nFalse\n.\nlayer2Configuration\nLayer 2 configuration.\nportCount\nNumber of ports that\n3\nare part of the port channel. The maximum value is based on the fabric SKU.\nmtu\nMaximum transmission\n1500\nunit between CEs and PEs.\nlayer3Configuration\nLayer 3 configuration True between CEs and PEs.\nprimaryIpv4Prefix\nIPv4 prefix for\n10.246.0.124/31\n; the String connectivity between port-channel interface for the primary CE and the the primary CE is assigned primary PE. The port-\n10.246.0.125\n, and the channel interface for port-channel interface for the primary CE is the primary PE is assigned assigned the first usable\n10.246.0.126\n. IP from the prefix. The corresponding interface on the primary PE is assigned the second usable address.\nParameter Description Example Required Type\nsecondaryIpv4Prefix\nIPv4 prefix for\n10.246.0.128/31\n; the String connectivity between port-channel interface for the secondary CE and the secondary CE is the secondary PE. The assigned \n10.246.0.129\n, port-channel interface and the port-channel for the secondary CE is interface for the assigned the first usable secondary PE is assigned IP from the prefix. The\n10.246.0.130\n. corresponding interface on the secondary PE is assigned the second usable address.\nprimaryIpv6Prefix\nIPv6 prefix for\n3FFE:FFFF:0:CD30::a1\n is String connectivity between assigned to the primary the primary CE and the CE, and primary PE. The port-\n3FFE:FFFF:0:CD30::a2\n is channel interface for assigned to the primary the primary CE is PE. Default value is assigned the first usable\n3FFE:FFFF:0:CD30::a0/126\n. IP from the prefix. The corresponding interface on the primary PE is assigned the second usable address.\nsecondaryIpv6Prefix\nIPv6 prefix for\n3FFE:FFFF:0:CD30::a5\n is String connectivity between assigned to the secondary the secondary CE and CE, and the secondary PE. The\n3FFE:FFFF:0:CD30::a6\n is port-channel interface assigned to the secondary for the secondary CE is PE. Default value is assigned the first usable\n3FFE:FFFF:0:CD30::a4/126\n. IP from the prefix. The corresponding interface on the secondary PE is assigned the second usable address.\nfabricAsn\nASN assigned on the CE\n65048\nfor BGP peering with the PE.\nParameter Description Example Required Type\npeerAsn\nASN assigned on the PE\n65048\nTrue for BGP peering with the CE. For internal BGP between the PE and the CE, the value should be the same as \nfabricAsn\n. For external BGP, the value should be different from\nfabricAsn\n.\nfabricAsn\nASN assigned on the CE\n65048\nfor BGP peering with the PE.\nvlan-Id\nVLAN for the NNI. The\n501\nrange is 501 to 4095.\nimportRoutePolicy\nDetails to import a route policy.\nexportRoutePolicy\nDetails to export a route policy."}
{"text": "Create an NNI\nYou must create the resource group and network fabric before you create a network-to- network interconnect.\nRun the following command to create the NNI:\nAzure CLI\naz nf nni create  \  --resource-group   "NFResourceGroup"  \  --location   "eastus"  \  --resource-name   "NFNNIName"  \  --fabric   "NFFabric"  \  --is-management-type   "True"  \  --use-option-b   "True"  \ --layer 2 -configuration   '{"portCount": 3, "mtu": 1500}'  \ --layer 3 -configuration   '{"peerASN": 65048, "vlanId": 501,  "primaryIpv4Prefix": "10.2.0.124/30", "secondaryIpv4Prefix":  "10.2.0.128/30", "primaryIpv6Prefix": "10:2:0:124::400/127",  "secondaryIpv6Prefix": "10:2:0:124::402/127"}'  \nExpected output:\nOutput\n{    "administrativeState": null,    "id": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/NFResourceGroup/providers/Microsoft.ManagedNetworkFabr ic/networkFabrics/nffab1031623/networkToNetworkInterconnects/NFNNIName",    "isManagementType": "True",    "layer2Configuration": {      "interfaces": null,      "mtu": 1500,      "portCount": 3    },    "layer3Configuration": {      "exportRoutePolicyId": null,      "fabricAsn": null,      "importRoutePolicyId": null,      "peerAsn": 65048,      "primaryIpv4Prefix": "10.2.0.124/30",      "primaryIpv6Prefix": "10:2:0:124::400/127",      "secondaryIpv4Prefix": "10.2.0.128/30",      "secondaryIpv6Prefix": "10:2:0:124::402/127",      "vlanId": 501    },    "name": "NFNNIName",    "provisioningState": "Succeeded",    "resourceGroup": "NFResourceGroup",    "systemData": {      "createdAt": "2023-XX-XXT13:13:22.514644+00:00",      "createdBy": "email@address.com",      "createdByType": "User",      "lastModifiedAt": "2023-XX-XXT13:13:22.514644+00:00",      "lastModifiedBy": "email@address.com",      "lastModifiedByType": "User"    },    "type":  "microsoft.managednetworkfabric/networkfabrics/networktonetworkinterconnects ",    "useOptionB": "True""}
{"text": "Show network fabric NNIs\nAzure CLI\naz nf nni show  -g   "NFResourceGroup"   --resource-name   "NFNNIName"   --fabric   "NFFabric"  \nExpected output:\nOutput\n{    "administrativeState": null,    "id": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/NFResourceGroup/providers/Microsoft.ManagedNetworkFabr ic/networkFabrics/NFFabric/networkToNetworkInterconnects/NFNNIName",    "isManagementType": "True",    "layer2Configuration": {      "interfaces": null,      "mtu": 1500,      "portCount": 3    },    "layer3Configuration": {      "exportRoutePolicyId": null,      "fabricAsn": null,      "importRoutePolicyId": null,      "peerAsn": 65048,      "primaryIpv4Prefix": "10.2.0.124/30",      "primaryIpv6Prefix": "10:2:0:124::400/127",      "secondaryIpv4Prefix": "10.2.0.128/30",      "secondaryIpv6Prefix": "10:2:0:124::402/127",      "vlanId": 501    },    "name": "NFNNIName",    "provisioningState": "Succeeded",    "resourceGroup": "NFResourceGroup",    "systemData": {      "createdAt": "2023-XX-XXT13:13:22.514644+00:00",      "createdBy": "email@address.com",      "createdByType": "User",      "lastModifiedAt": "2023-XX-XX13:13:22.514644+00:00",      "lastModifiedBy": "email@address.com",      "lastModifiedByType": "User"    },    "type":  "microsoft.managednetworkfabric/networkfabrics/networktonetworkinterconnects ",    "useOptionB": "True""}
{"text": "List or get network fabric NNIs\nAzure CLI\naz nf nni list  -g  NFResourceGroup  --fabric  NFFabric \nExpected output:\nOutput\n{      "administrativeState": null,      "id": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/NFResourceGroup/providers/Microsoft.ManagedNetworkFabr ic/networkFabrics/NFFabric/networkToNetworkInterconnects/NFNNIName",      "isManagementType": "True",      "layer2Configuration": {        "interfaces": null,        "mtu": 1500,        "portCount": 3      },      "layer3Configuration": {        "exportRoutePolicyId": null,        "fabricAsn": null,        "importRoutePolicyId": null,        "peerAsn": 65048,        "primaryIpv4Prefix": "10.2.0.124/30",        "primaryIpv6Prefix": "10:2:0:124::400/127",        "secondaryIpv4Prefix": "10.2.0.128/30",        "secondaryIpv6Prefix": "10:2:0:124::402/127",        "vlanId": 501      },      "name": "NFNNIName",      "provisioningState": "Succeeded",      "resourceGroup": "NFResourceGroup",      "systemData": {        "createdAt": "2023-XX-XXT13:13:22.514644+00:00",        "createdBy": "email@address.com.com",        "createdByType": "User",        "lastModifiedAt": "2023-XX-XXT13:13:22.514644+00:00",        "lastModifiedBy": "email@address.com.com",        "lastModifiedByType": "User"      },      "type":  "microsoft.managednetworkfabric/networkfabrics/networktonetworkinterconnects ",      "useOptionB": "True"    }"}
{"text": "Update network fabric devices\nRun the following command to update network fabric devices:\nAzure CLI\naz nf device update  \  --resource-group   "NFResourceGroup"  \  --resource-name   "Network-Device-Name"  \ \n--location   "eastus"  \  --serial-number   "xxxx"  \nExpected output:\nOutput\n{    "annotation": null,    "hostName": "AggrRack-CE01",    "id": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/NFResourceGroup/providers/Microsoft.ManagedNetworkFabr ic/networkDevices/Network-Device-Name",    "location": "eastus2euap",    "name": "Network-Device-Name",    "networkDeviceRole": "CE1",    "networkDeviceSku": "DefaultSku",    "networkRackId": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/NFResourceGroup/providers/Microsoft.ManagedNetworkFabr ic/networkRacks/Network-Device-Name",    "provisioningState": "Succeeded",    "resourceGroup": "NFResourceGroup",    "serialNumber": "AXXXX;DCS-XXXXX-24;XX.XX;JXXXXXXX",    "systemData": {      "createdAt": "2023-XX-XXT12:52:42.270551+00:00",      "createdBy": "d1bd24c7-b27f-477e-86dd-939e107873d7",      "createdByType": "Application",      "lastModifiedAt": "2023-XX-XXT13:30:24.098335+00:00",      "lastModifiedBy": "email@address.com",      "lastModifiedByType": "User"    },    "tags": null,    "type": "microsoft.managednetworkfabric/networkdevices",    "version": null  } \nThe preceding code serves only as an example. You should update all the devices that are part of both \nAggrRack\n and \ncomputeRacks\n.\nFor example, \nAggrRack\n consists of:\nCE01\nCE02\nTOR17\nTOR18\nMgmtSwitch01\nMgmtSwitch02\n (and so on, for other switches)"}
{"text": "List or get network fabric devices\nRun the following command to list network fabric devices in a resource group:\nAzure CLI\naz nf device list  --resource-group   "NFResourceGroup"  \nExpected output:\nOutput\n{      "annotation": null,      "hostName": "AggrRack-CE01",      "id": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/NFResourceGroup/providers/Microsoft.ManagedNetworkFabr ic/networkDevices/NFName-AggrRack-CE1",      "location": "eastus",      "name": "Network-Device-Name",      "networkDeviceRole": "CE1",      "networkDeviceSku": "DefaultSku",      "networkRackId": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/NFResourceGroup/providers/Microsoft.ManagedNetworkFabr ic/networkRacks/Network-Device-Name",      "provisioningState": "Succeeded",      "resourceGroup": "NFResourceGroup",      "serialNumber": "ArXXX;DCS-7XXXXXX-24;12.05;JPXXXXXXXX",      "systemData": {        "createdAt": "2023-XX-XXT12:52:42.270551+00:00",        "createdBy": "d1bd24c7-b27f-477e-86dd-939e107873d7",        "createdByType": "Application",        "lastModifiedAt": "2023-XX-XXT13:30:24.098335+00:00",        "lastModifiedBy": "email@address.com",        "lastModifiedByType": "User"      },      "tags": null,      "type": "microsoft.managednetworkfabric/networkdevices",      "version": null    },    {      "annotation": null,      "hostName": "AggrRack-CE02",      "id": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/NFResourceGroup/providers/Microsoft.ManagedNetworkFabr ic/networkDevices/NFName-AggrRack-CE2",      "location": "eastus",      "name": "Network-Device-Name",      "networkDeviceRole": "CE2",      "networkDeviceSku": "DefaultSku",      "networkRackId": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/NFResourceGroup/providers/Microsoft.ManagedNetworkFabr\nic/networkRacks/Network-Device-Name",      "provisioningState": "Succeeded",      "resourceGroup": "NFResourceGroup",      "serialNumber": "ArXXX;DCS-7XXXXXX-24;12.05;JPXXXXXXXX",      "systemData": {        "createdAt": "2023-XX-XXT12:52:43.489256+00:00",        "createdBy": "d1bd24c7-b27f-477e-86dd-939e107873d7",        "createdByType": "Application",        "lastModifiedAt": "2023-XX-XXT13:30:40.923567+00:00",        "lastModifiedBy": "email@address.com",        "lastModifiedByType": "User"      },      "tags": null,      "type": "microsoft.managednetworkfabric/networkdevices",      "version": null    },    {      "annotation": null,      "hostName": "AggRack-TOR17",      "id": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/NFResourceGroup/providers/Microsoft.ManagedNetworkFabr ic/networkDevices/NFName-AggrRack-TOR17",      "location": "eastus2euap",      "name": "Network-Device-Name",      "networkDeviceRole": "TOR17",     "networkDeviceSku": "DefaultSku",      "networkRackId": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/NFResourceGroup/providers/Microsoft.ManagedNetworkFabr ic/networkRacks/Network-Device-Name",      "provisioningState": "Succeeded",      "resourceGroup": "NFResourceGroup",      "serialNumber": "ArXXX;DCS-7XXXXXX-24;12.05;JPXXXXXXXX",      "systemData": {        "createdAt": "2023-XX-XXT12:52:44.676759+00:00",        "createdBy": "d1bd24c7-b27f-477e-86dd-939e107873d7",        "createdByType": "Application",        "lastModifiedAt": "2023-XX-XXT13:31:59.650758+00:00",        "lastModifiedBy": "email@address.com",        "lastModifiedByType": "User"      },      "tags": null,      "type": "microsoft.managednetworkfabric/networkdevices",      "version": null    },    {      "annotation": null,      "hostName": "AggRack-TOR18",      "id": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/NFResourceGroup/providers/Microsoft.ManagedNetworkFabr ic/networkDevices/NFName-AggrRack-TOR18",      "location": "eastus",      "name": "Network-Device-Name",      "networkDeviceRole": "TOR18",     "networkDeviceSku": "DefaultSku",      "networkRackId": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx-\nxxxxxx/resourceGroups/NFResourceGroup/providers/Microsoft.ManagedNetworkFabr ic/networkRacks/Network-Device-Name",      "provisioningState": "Succeeded",      "resourceGroup": "NFResourceGroup",      "serialNumber": "ArXXX;DCS-7XXXXXX-24;12.05;JPXXXXXXXX",      "systemData": {        "createdAt": "2023-03-16T12:52:45.801778+00:00",        "createdBy": "d1bd24c7-b27f-477e-86dd-939e107873d7",        "createdByType": "Application",        "lastModifiedAt": "2023-XX-XXT13:32:13.369591+00:00",        "lastModifiedBy": "email@address.com",        "lastModifiedByType": "User"      },      "tags": null,      "type": "microsoft.managednetworkfabric/networkdevices",      "version": null    },    {      "annotation": null,      "hostName": "AggRack-MGMT1",      "id": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/NFResourceGroup/providers/Microsoft.ManagedNetworkFabr ic/networkDevices/NFName-AggrRack-MgmtSwitch1",      "location": "eastus",      "name": "Network-Device-Name",      "networkDeviceRole": "MgmtSwitch1",      "networkDeviceSku": "DefaultSku",      "networkRackId": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/NFResourceGroup/providers/Microsoft.ManagedNetworkFabr ic/networkRacks/Network-Device-Name",      "provisioningState": "Succeeded",      "resourceGroup": "NFResourceGroup",      "serialNumber": "ArXXX;DCS-7XXXXXX-24;12.05;JPXXXXXXXX",      "systemData": {        "createdAt": "2023-XX-XXT12:52:46.911202+00:00",        "createdBy": "d1bd24c7-b27f-477e-86dd-939e107873d7",        "createdByType": "Application",        "lastModifiedAt": "2023-XX-XXT13:31:00.836730+00:00",        "lastModifiedBy": "email@address.com",        "lastModifiedByType": "User"      },      "tags": null,      "type": "microsoft.managednetworkfabric/networkdevices",      "version": null    },    {      "annotation": null,      "hostName": "AggRack-MGMT2",      "id": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/NFResourceGroup/providers/Microsoft.ManagedNetworkFabr ic/networkDevices/NFName-AggrRack-MgmtSwitch2",      "location": "eastus",      "name": "Network-Device-Name",      "networkDeviceRole": "MgmtSwitch2",      "networkDeviceSku": "DefaultSku", \n    "networkRackId": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/NFResourceGroup/providers/Microsoft.ManagedNetworkFabr ic/networkRacks/Network-Device-Name",      "provisioningState": "Succeeded",      "resourceGroup": "NFResourceGroup",      "serialNumber": "ArXXX;DCS-7XXXXXX-24;12.05;JPXXXXXXXX",      "systemData": {        "createdAt": "2023-XX-XXT12:52:48.020528+00:00",        "createdBy": "d1bd24c7-b27f-477e-86dd-939e107873d7",        "createdByType": "Application",        "lastModifiedAt": "2023-XX-XXT13:31:42.173645+00:00",        "lastModifiedBy": "email@address.com",        "lastModifiedByType": "User"      },      "tags": null,      "type": "microsoft.managednetworkfabric/networkdevices",      "version": null    } \nRun the following command to get or show details of a network fabric device:\nAzure CLI\naz nf device show  --resource-group   "NFResourceGroup"   --resource-name   "Network-Device-Name"  \nExpected output:\nOutput\n{    "annotation": null,    "hostName": "AggrRack-CE01",    "id": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/NFResourceGroup/providers/Microsoft.ManagedNetworkFabr ic/networkDevices/NFName-AggrRack-CE1",    "location": "eastus",    "name": "Network-Device-Name",    "networkDeviceRole": "CE1",    "networkDeviceSku": "DefaultSku",    "networkRackId": "/subscriptions/61065ccc-9543-4b91-b2d1- 0ce42a914507/resourceGroups/NFResourceGroup/providers/Microsoft.ManagedNetwo rkFabric/networkRacks/Network-Device-Name",    "provisioningState": "Succeeded",    "resourceGroup": "NFResourceGroup",    "serialNumber": "AXXXX;DCS-XXXXX-24;XX.XX;JXXXXXXX",    "systemData": {      "createdAt": "2023-XX-XXT12:52:42.270551+00:00",      "createdBy": "d1bd24c7-b27f-477e-86dd-939e107873d7",      "createdByType": "Application",      "lastModifiedAt": "2023-XX-XXT13:30:24.098335+00:00",      "lastModifiedBy": "email@address.com", \n    "lastModifiedByType": "User"    },    "tags": null,    "type": "microsoft.managednetworkfabric/networkdevices",    "version": null  }"}
{"text": "Provision a network fabric\nAfter you update the device serial number, provision and show the fabric by running the following commands:\nAzure CLI\naz nf fabric provision  --resource-group   "NFResourceGroup"    --resource-name   "NFName"  \nAzure CLI\naz nf fabric show  --resource-group   "NFResourceGroup"      --resource-name   "NFName"  \nExpected output:\nOutput\n{    "annotation": null,    "fabricAsn": 65048,    "id": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/NFResourceGroup/providers/Microsoft.ManagedNetworkFabr ic/networkFabrics/NFName",    "ipv4Prefix": "10.2.0.0/19",    "ipv6Prefix": "fda0:d59c:da02::/59",    "l2IsolationDomains": null,    "l3IsolationDomains": null,    "location": "eastus",    "managementNetworkConfiguration": {      "infrastructureVpnConfiguration": {        "administrativeState": "Enabled",        "networkToNetworkInterconnectId": null,        "optionAProperties": null,        "optionBProperties": {          "exportRouteTargets": [            "65048:10039"          ],          "importRouteTargets": [            "65048:10039" \n        ]        },        "peeringOption": "OptionB"      },      "workloadVpnConfiguration": {       "administrativeState": "Enabled",        "networkToNetworkInterconnectId": null,        "optionAProperties": null,        "optionBProperties": {          "exportRouteTargets": [            "65048:10050"          ],          "importRouteTargets": [            "65048:10050"          ]        },        "peeringOption": "OptionB"      }    },    "name": "NFName",    "networkFabricControllerId": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/NFCResourceGroup/providers/Microsoft.ManagedNetworkFab ric/networkFabricControllers/NFCName",    "networkFabricSku": "NFSKU",    "operationalState": "Provisioning",    "provisioningState": "Succeeded",    "rackCount": 3,    "racks": [      "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourcegroups/NFResourceGroup/providers/microsoft.managednetworkfabr ic/networkracks/NFName-aggrack",      "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourcegroups/NFResourceGroup/providers/microsoft.managednetworkfabr ic/networkracks/NFName-comprack1",      "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourcegroups/NFResourceGroup/providers/microsoft.managednetworkfabr ic/networkracks/NFName-comprack2"   ],    "resourceGroup": "NFResourceGroup",    "routerId": null,    "serverCountPerRack": 7,    "systemData": {      "createdAt": "2023-XX-XXT12:52:11.769525+00:00",      "createdBy": "email@address.com",      "createdByType": "User",      "lastModifiedAt": "2023-XX-XXT14:47:59.424826+00:00",      "lastModifiedBy": "d1bd24c7-b27f-477e-86dd-939e107873d7",      "lastModifiedByType": "Application"    },    "tags": null,    "terminalServerConfiguration": {      "networkDeviceId": null,      "password": null,      "primaryIpv4Prefix": "20.0.1.0/30",      "primaryIpv6Prefix": null, \n    "secondaryIpv4Prefix": "20.0.0.0/30",      "secondaryIpv6Prefix": null,      "serialNumber": "XXXXXXXXXXXX",      "username": "XXXX"    },    "type": "microsoft.managednetworkfabric/networkfabrics"  }"}
{"text": "Deprovision a network fabric\nTo deprovision a fabric, ensure that the fabric is in a provisioned operational state and then run this command:\nAzure CLI\naz nf fabric deprovision  --resource-group   "NFResourceGroup"   --resource-name   "NFName"  \nExpected output:\nOutput\n{    "annotation": null,    "fabricAsn": 65046,    "id": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/NFResourceGroup/providers/Microsoft.ManagedNetworkFabr ic/networkFabrics/NFName",    "ipv4Prefix": "10.18.0.0/19",    "ipv6Prefix": null,    "l2IsolationDomains": [],    "l3IsolationDomains": null,    "location": "eastus",    "managementNetworkConfiguration": {      "infrastructureVpnConfiguration": {        "administrativeState": "Enabled",        "networkToNetworkInterconnectId": null,        "optionAProperties": null,        "optionBProperties": {          "exportRouteTargets": [            "65048:10039"          ],          "importRouteTargets": [            "65048:10039"          ]        },        "peeringOption": "OptionB"      },      "workloadVpnConfiguration": {\n      "administrativeState": "Enabled",        "networkToNetworkInterconnectId": null,        "optionAProperties": null,        "optionBProperties": {          "exportRouteTargets": [            "65048:10050"          ],          "importRouteTargets": [            "65048:10050"          ]        },        "peeringOption": "OptionB"      }    },    "name": "NFName",    "networkFabricControllerId": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/NFCResourceGroup/providers/Microsoft.ManagedNetworkFab ric/networkFabricControllers/NFCName",    "networkFabricSku": "M4-A400-A100-C16-aa",    "operationalState": "Deprovisioned",    "provisioningState": "Succeeded",    "rackCount": 3,    "racks": [      "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourcegroups/NFResourceGroup/providers/microsoft.managednetworkfabr ic/networkracks/NFName-aggrack",      "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourcegroups/NFResourceGroup/providers/microsoft.managednetworkfabr ic/networkracks/NFName-comprack1",      "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourcegroups/NFResourceGroup/providers/microsoft.managednetworkfabr ic/networkracks/NFName-comprack2"   ],    "resourceGroup": "NFResourceGroup",    "routerId": null,    "serverCountPerRack": 8,    "systemData": {      "createdAt": "2023-XX-XXT19:30:23.319643+00:00",      "createdBy": "email@address.com",      "createdByType": "User",      "lastModifiedAt": "2023-XX-XXT06:47:36.130713+00:00",      "lastModifiedBy": "d1bd24c7-b27f-477e-86dd-939e107873d7",      "lastModifiedByType": "Application"    },    "tags": null,    "terminalServerConfiguration": {      "networkDeviceId": null,      "password": null,      "primaryIpv4Prefix": "20.0.1.12/30",      "primaryIpv6Prefix": null,      "secondaryIpv4Prefix": "20.0.0.12/30",      "secondaryIpv6Prefix": null,      "serialNumber": "XXXXXXXXXXXXX",      "username": "XXXX"    }, \n  "type": "microsoft.managednetworkfabric/networkfabrics"  }"}
{"text": "Delete a network fabric\nTo delete a fabric, run the following command. Before you do, make sure that:\nThe fabric is in a deprovisioned operational state. If it's in a provisioned state, run the \ndeprovision\n command.\nNo racks are associated with the fabric.\nAzure CLI\naz nf fabric delete  --resource-group   "NFResourceGroup"   --resource-name   "NFName"  \nExpected output:\nOutput\n{    "annotation": null,    "fabricAsn": 65044,    "id": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/NFResourceGroup/providers/Microsoft.ManagedNetworkFabr ic/networkFabrics/NFName",    "ipv4Prefix": "10.21.0.0/16",    "ipv6Prefix": "10:15:0:0::/59",   "l2IsolationDomains": null,    "l3IsolationDomains": null,    "location": "eastus",    "managementNetworkConfiguration": {      "infrastructureVpnConfiguration": {        "administrativeState": "Enabled",        "networkToNetworkInterconnectId": null,        "optionAProperties": null,        "optionBProperties": {          "exportRouteTargets": [            "65044:10039"          ],          "importRouteTargets": [            "65044:10039"          ]        },        "peeringOption": "OptionB"      },      "workloadVpnConfiguration": {\n      "administrativeState": "Enabled",        "networkToNetworkInterconnectId": null,        "optionAProperties": null,        "optionBProperties": {          "exportRouteTargets": [            "65044:10050"          ],          "importRouteTargets": [            "65044:10050"          ]        },        "peeringOption": "OptionB"      }    },    "name": "nffab2030823",    "networkFabricControllerId": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/NFCResourceGroup/providers/Microsoft.ManagedNetworkFab ric/networkFabricControllers/NFCName",    "networkFabricSku": "SKU-Name",   "operationalState": "Deprovisioned",    "provisioningState": "Deleting",    "rackCount": 3,    "racks": [      "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourcegroups/NFResourceGroup/providers/microsoft.managednetworkfabr ic/networkracks/NFName-aggrack",      "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourcegroups/NFResourceGroup/providers/microsoft.managednetworkfabr ic/networkracks/NFName-comprack1",      "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourcegroups/NFResourceGroup/providers/microsoft.managednetworkfabr ic/networkracks/NFName-comprack2"   ],    "resourceGroup": "NFResourceGroup",    "routerId": null,    "serverCountPerRack": 7,    "systemData": {      "createdAt": "2023-XX-XXT10:31:22.423399+00:00",      "createdBy": "email@address.com",      "createdByType": "User",      "lastModifiedAt": "2023-XX-XXT06:31:41.675991+00:00",      "lastModifiedBy": "d1bd24c7-b27f-477e-86dd-939e107873d7",      "lastModifiedByType": "Application"    },    "tags": null,    "terminalServerConfiguration": {      "networkDeviceId": null,      "password": null,      "primaryIpv4Prefix": "20.0.1.68/30",      "primaryIpv6Prefix": null,      "secondaryIpv4Prefix": "20.0.0.68/30",      "secondaryIpv6Prefix": null,      "serialNumber": "XXXXXXXXXXXXX",      "username": "XXXX"    }, \n  "type": "microsoft.managednetworkfabric/networkfabrics"  } \nAfter you successfully delete the network fabric, when you run the command to show the fabric, you won't find any resources available:\nAzure CLI\naz nf fabric show  --resource-group   "NFResourceGroup"   --resource-name   "NFName"  \nExpected output:\nOutput\nCommand group 'nf' is in preview and under development. Reference and  support levels: https://aka.ms/CLI_refstatus  (ResourceNotFound) The Resource  'Microsoft.ManagedNetworkFabric/NetworkFabrics/NFName' under resource group  'NFResourceGroup' was not found. For more details please go to  https://aka.ms/ARMResourceNotFoundFix  Code: ResourceNotFound"}
{"text": "Create and provision a Cluster using"}
{"text": "Azure CLI\nArticle  04/17/2023\nThis article describes how to create a Cluster by using the Azure Command Line Interface (AzCLI). This document also shows you how to check the status, update, or delete a Cluster."}
{"text": "Prerequisites\nVerify that Network Fabric Controller and Cluster Manger exist in your Azure region Verify that Network Fabric is successfully provisioned"}
{"text": "API guide and metrics\nThe  API guide  provides information on the resource providers and resource models, and the APIs.\nThe metrics generated from the logging data are available in  Azure Monitor metrics ."}
{"text": "Create a Cluster\nThe Cluster resource represents an on-premises deployment of the platform within the Cluster Manager. All other platform-specific resources are dependent upon it for their lifecycle.\nYou should have successfully created the Network Fabric for this on-premises deployment. Each Operator Nexus on-premises instance has a one-to-one association with a Network Fabric.\nCreate the Cluster:\nAzure CLI\naz networkcloud cluster create  --name   "$CLUSTER_NAME"   --location   "$LOCATION"   \     --extended-location   name= "$CL_NAME"   type= "CustomLocation"  \     --resource-group   "$CLUSTER_RG"  \     --analytics-workspace-id   "$LAW_ID"  \     --cluster-location   "$CLUSTER_LOCATION"  \     --network-rack-id   "$AGGR_RACK_RESOURCE_ID"  \ \n   --rack-sku-id   "$AGGR_RACK_SKU" \     --rack-serial-number   "$AGGR_RACK_SN"  \     --rack-location   "$AGGR_RACK_LOCATION"  \     --bare-metal-machine-configuration-data   "[" $AGGR_RACK_BMM "]"  \     --storage-appliance-configuration-data   '[{"adminCredentials": {"password":"$SA_PASS","username":"$SA_USER"},"rackSlot":1,"serialNumber":"$ SA_SN","storageApplianceName":"$SA_NAME"}]'  \     --compute-rack-definitions   '[{"networkRackId": "$COMPX_RACK_RESOURCE_ID",  "rackSkuId": "$COMPX_RACK_SKU", "rackSerialNumber": "$COMPX_RACK_SN",  "rackLocation": "$COMPX_RACK_LOCATION", "storageApplianceConfigurationData":  [], "bareMetalMachineConfigurationData":[{"bmcCredentials":  {"password":"$COMPX_SVRY_BMC_PASS", "username":"$COMPX_SVRY_BMC_USER"},  "bmcMacAddress":"$COMPX_SVRY_BMC_MAC",  "bootMacAddress":"$COMPX_SVRY_BOOT_MAC",  "machineDetails":"$COMPX_SVRY_SERVER_DETAILS",  "machineName":"$COMPX_SVRY_SERVER_NAME"}]}]' \     --managed-resource-group-configuration   name= "$MRG_NAME"   location= "$MRG_LOCATION"  \     --network  fabric -id   "$NFC_ID"  \     --cluster-service-principal   application-id= "$SP_APP_ID"  \       password= "$SP_PASS"   principal-id= "$SP_ID"   tenant-id= "$TENANT_ID"  \     --cluster-type   "$CLUSTER_TYPE"   --cluster-version   "$CLUSTER_VERSION"  \     --tags  $TAG_KEY1= "$TAG_VALUE1"  $TAG_KEY2= "$TAG_VALUE2"  \nYou can instead create a Cluster with ARM template/parameter files in  ARM Template Editor :"}
{"text": "Parameters for cluster operations\nParameter name Description\nCLUSTER_NAME Resource Name of the Cluster\nLOCATION The Azure Region where the Cluster is deployed\nCL_NAME The Cluster Manager Custom Location from Azure portal\nCLUSTER_RG The cluster resource group name\nLAW_ID Log Analytics Workspace ID for the Cluster\nCLUSTER_LOCATION The local name of the Cluster\nAGGR_RACK_RESOURCE_ID RackID for Aggregator Rack\nAGGR_RACK_SKU Rack SKU for Aggregator Rack\nAGGR_RACK_SN Rack Serial Number for Aggregator Rack\nAGGR_RACK_LOCATION Rack physical location for Aggregator Rack\nParameter name Description\nAGGR_RACK_BMM Used for single rack deployment only, empty for multi-rack\nSA_NAME Storage Appliance Device name\nSA_PASS Storage Appliance admin password\nSA_USER Storage Appliance admin user\nSA_SN Storage Appliance Serial Number\nCOMPX_RACK_RESOURCE_ID RackID for CompX Rack, repeat for each rack in compute-rack- definitions\nCOMPX_RACK_SKU Rack SKU for CompX Rack, repeat for each rack in compute- rack-definitions\nCOMPX_RACK_SN Rack Serial Number for CompX Rack, repeat for each rack in compute-rack-definitions\nCOMPX_RACK_LOCATION Rack physical location for CompX Rack, repeat for each rack in compute-rack-definitions\nCOMPX_SVRY_BMC_PASS CompX Rack ServerY BMC password, repeat for each rack in compute-rack-definitions and for each server in rack\nCOMPX_SVRY_BMC_USER CompX Rack ServerY BMC user, repeat for each rack in compute-rack-definitions and for each server in rack\nCOMPX_SVRY_BMC_MAC CompX Rack ServerY BMC MAC address, repeat for each rack in compute-rack-definitions and for each server in rack\nCOMPX_SVRY_BOOT_MAC CompX Rack ServerY boot NIC MAC address, repeat for each rack in compute-rack-definitions and for each server in rack\nCOMPX_SVRY_SERVER_DETAILS CompX Rack ServerY details, repeat for each rack in compute- rack-definitions and for each server in rack\nCOMPX_SVRY_SERVER_NAME CompX Rack ServerY name, repeat for each rack in compute- rack-definitions and for each server in rack\nMRG_NAME Cluster managed resource group name\nMRG_LOCATION Cluster Azure region\nNFC_ID Reference to Network fabric Controller\nSP_APP_ID Service Principal App ID\nSP_PASS Service Principal Password\nSP_ID Service Principal ID\nParameter name Description\nTENANT_ID Subscription tenant ID\nCLUSTER_TYPE Type of cluster, Single or MultiRack\nCLUSTER_VERSION NC Version of cluster\nTAG_KEY1 Optional tag1 to pass to Cluster Create\nTAG_VALUE1 Optional tag1 value to pass to Cluster Create\nTAG_KEY2 Optional tag2 to pass to Cluster Create\nTAG_VALUE2 Optional tag2 value to pass to Cluster Create"}
{"text": "Cluster validation\nA successful Operator Nexus Cluster creation results in the creation of an AKS cluster inside your subscription. The cluster ID, cluster provisioning state and deployment state are returned as a result of a successful \ncluster create\n.\nView the status of the Cluster:\nAzure CLI\naz networkcloud cluster show  --resource-group   "$CLUSTER_RG"  \     --resource-name   "$CLUSTER_RESOURCE_NAME"  \nThe Cluster creation is complete when the \nprovisioningState\n of the resource shows:\n"provisioningState": "Succeeded""}
{"text": "Cluster logging\nCluster create Logs can be viewed in the following locations:\n1. Azure portal Resource/ResourceGroup Activity logs. 2. Azure CLI with \n--debug\n flag passed on command-line."}
{"text": "Deploy Cluster\nOnce a Cluster has been created, the deploy cluster action can be triggered. The deploy Cluster action creates the bootstrap image and deploys the Cluster.\nDeploy Cluster initiates a sequence of events to occur in the Cluster Manager\n1. Validation of the cluster/rack properties 2. Generation of a bootable image for the ephemeral bootstrap cluster (Validation of Infrastructure). 3. Interaction with the IPMI interface of the targeted bootstrap machine. 4. Perform hardware validation checks 5. Monitoring of the Cluster deployment process.\nDeploy the on-premises Cluster:\nAzure CLI\naz networkcloud cluster deploy  \     --name   "$CLUSTER_NAME"  \     --resource-group   "$CLUSTER_RESOURCE_GROUP"  \     --subscription   "$SUBSCRIPTION_ID"  \     --no-wait   --debug   \n  Tip\nTo check the status of the \naz networkcloud cluster deploy\n command, it can be\nexecuted using the \n--debug\n flag. This will allow you to obtain the \nAzure-\nAsyncOperation\n or \nLocation\n header used to query the \noperationStatuses\n resource.\nSee the section  Cluster Deploy Failed  for more detailed steps. Optionally, the command can run asynchronously using the \n--no-wait\n flag."}
{"text": "Cluster Deploy with hardware validation\nDuring a Cluster deploy process, one of the steps executed is hardware validation. The hardware validation procedure runs various test and checks against the machines provided through the Cluster's rack definition. Based on the results of these checks and any user skipped machines, a determination is done on whether sufficient nodes passed and/or are available to meet the thresholds necessary for deployment to continue."}
{"text": "Cluster Deploy Action with skipping specific bare-metal-machine\nA parameter can be passed in to the deploy command that represents the names of bare metal machines in the cluster that should be skipped during hardware validation. Nodes skipped aren't validated and aren't added to the node pool. Additionally, nodes skipped don't count against the total used by threshold calculations.\nAzure CLI\naz networkcloud cluster deploy  \     --name   "$CLUSTER_NAME"  \     --resource-group   "$CLUSTER_RESOURCE_GROUP"  \     --subscription   "$SUBSCRIPTION_ID"  \     --skip-validations-for-machines   "$COMPX_SVRY_SERVER_NAME""}
{"text": "Cluster Deploy failed\nTo track the status of an asynchronous operation, run with a \n--debug\n flag enabled.\nWhen \n--debug\n is specified, the progress of the request can be monitored. The operation\nstatus URL can be found by examining the debug output looking for the \nAzure-\nAsyncOperation\n or \nLocation\n header on the HTTP response to the creation request. The\nheaders can provide the \nOPERATION_ID\n field used in the HTTP API call.\nAzure CLI\nOPERATION_ID= "12312312-1231-1231-1231-123123123123*99399E995..."   az rest  -m  GET  -u   "https://management.azure.com/subscriptions/${SUBSCRIPTION_ID}/providers/Mic rosoft.NetworkCloud/locations/${LOCATION}/operationStatuses/${OPERATION_ID}? api-version=2022-12-12-preview"  \nThe output is similar to the JSON struct example. When the error code is\nHardwareValidationThresholdFailed\n, then the error message contains a list of bare metal\nmachine(s) that failed the hardware validation (for example, \nCOMP0_SVR0_SERVER_NAME\n,\nCOMP1_SVR1_SERVER_NAME\n). These names can be used to parse the logs for further details.\nJSON\n{     "endTime" :  "2023-03-24T14:56:59.0510455Z" ,     "error" : {       "code" :  "HardwareValidationThresholdFailed" ,       "message" :  "HardwareValidationThresholdFailed error hardware validation  threshold for cluster layout plan is not met for cluster $CLUSTER_NAME in  namespace nc-system with listed failed devices $COMP0_SVR0_SERVER_NAME,  $COMP1_SVR1_SERVER_NAME"     },     "id" :  "/subscriptions/$SUBSCRIPTION_ID/providers/Microsoft.NetworkCloud/locations/ $LOCATION/operationStatuses/12312312-1231-1231-1231- 123123123123*99399E995..." ,     "name" :  "12312312-1231-1231-1231-123123123123*99399E995..." ,     "resourceId" :  "/subscriptions/$SUBSCRIPTION_ID/resourceGroups/$CLUSTER_RESOURCE_GROUP/prov\niders/Microsoft.NetworkCloud/clusters/$CLUSTER_NAME" ,     "startTime" :  "2023-03-24T14:56:26.6442125Z" ,     "status" :  "Failed"   } \nSee the article  Tracking Asynchronous Operations Using Azure CLI  for another example."}
{"text": "Cluster deployment validation\nView the status of the cluster:\nAzure CLI\naz networkcloud cluster show  --resource-group   "$CLUSTER_RG"  \     --resource-name   "$CLUSTER_RESOURCE_NAME"  \nThe Cluster deployment is complete when detailedStatus is set to \nRunning\n and\ndetailedStatusMessage shows message \nCluster is up and running\n."}
{"text": "Cluster deployment Logging\nCluster create Logs can be viewed in the following locations:\n1. Azure portal Resource/ResourceGroup Activity logs. 2. Azure CLI with \n--debug\n flag passed on command-line."}
{"text": "Upgrading cluster runtime from Azure"}
{"text": "CLI\nArticle  06/23/2023\nThis how-to guide explains the steps for installing the required Azure CLI and extensions required to interact with Operator Nexus."}
{"text": "Prerequisites\n1. The  Install Azure CLI  must be installed. 2. the \nnetworkcloud\n extension is required. If the \nnetworkcloud\n extension isn't\ninstalled, it can be installed following the steps listed  here . 3. Access to the Azure portal for the target cluster to be upgraded. 4. You must be logged in to the same subscription as your target cluster via \naz login\n5. Target cluster must be in a running state, with all control plane nodes healthy and 80+% of compute nodes in a running and healthy state."}
{"text": "Finding available runtime versions"}
{"text": "Via Portal\nTo find available upgradeable runtime versions, navigate to the target cluster in the Azure portal. In the cluster's overview pane, navigate to the  Available upgrade versions tab."}
{"text": "From the  available upgrade versions  tab, we're able to see the different cluster versions that are currently available to upgrade. The operator can select from the listed the target runtime versions. Once selected, proceed to upgrade the cluster."}
{"text": ""}
{"text": "Via Azure CLI\nAvailable upgrades are retrievable via the Azure CLI:\nAzure CLI\naz networkcloud cluster show  --name   "clusterName"   --resource-group   "resourceGroup"  \nIn the output, you can find the \navailableUpgradeVersions\n property and look at the\ntargetClusterVersion\n field:\n  "availableUpgradeVersions": [      {        "controlImpact": "True",        "expectedDuration": "Upgrades may take up to 4 hours + 2 hours per  rack",        "impactDescription": "Workloads will be disrupted during rack-by-rack  upgrade",        "supportExpiryDate": "2023-07-31",        "targetClusterVersion": "3.2.0",        "workloadImpact": "True"      }    ], \nIf there are no available cluster upgrades, the list will be empty."}
{"text": "Upgrading cluster runtime using CLI\nTo perform an upgrade of the runtime, use the following Azure CLI command:\nAzure CLI\naz networkcloud cluster update-version  --cluster-name   "clusterName"   -- target-cluster-version      "versionNumber"   --resource-group   "resourceGroupName"  \nThe runtime upgrade is a long process. The upgrade is considered to be finished 80% of compute nodes and 100% of management/control nodes have been successfully upgraded.\nUpgrading all the nodes takes multiple hours but can take more if other processes, like firmware updates, are also part of the upgrade. Due to the length of the upgrade process, it's advised to check the Cluster's detail status periodically for the current state of the upgrade. To check on the status of the upgrade observe the detailed status of the cluster. This check can be done via the portal or az CLI.\nTo view the upgrade status through the Azure portal, navigate to the targeted cluster resource. In the cluster's  Overview  screen, the detailed status is provided along with a detailed status message."}
{"text": "To view the upgrade status through the Azure CLI, use \naz networkcloud cluster show\n.\nAzure CLI\naz networkcloud cluster show  --cluster-name   "clusterName"   --resource-group   "resourceGroupName"  \nThe output should be the target cluster's information and the cluster's detailed status and detail status message should be present."}
{"text": "Frequently Asked Questions"}
{"text": "Identifying Cluster Upgrade Stalled/Stuck\nDuring a runtime upgrade it's possible that the upgrade fails to move forward but the detail status reflects that the upgrade is still ongoing.  Because the runtime upgrade may take a very long time to successfully finish, there is no set timeout length currently specified . Hence, it's advisable to also check periodically on your cluster's detail status and logs to determine if your upgrade is indefinitely attempting to upgrade.\nWe can identify when this is the case by looking at the Cluster's logs, detailed message, and detailed status message. If a timeout has occurred, we would observe that the Cluster is continuously reconciling over the same indefinitely and not moving forward. The Cluster's detailed status message would reflect, \n"Cluster is in the process of\nbeing updated."\n. From here, we recommend checking Cluster logs or configured LAW,\nto see if there is a failure, or a specific upgrade that is causing the lack of progress."}
{"text": "Hardware Failure doesn't require Upgrade re-execution\nIf a hardware failure during an upgrade has occurred, the runtime upgrade continues as long as the set thresholds are met for the compute and management/control nodes. Once the machine is fixed or replaced, it gets provisioned with the current platform runtime's OS, which contains the targeted version of the runtime.\nIf a hardware failure occurs, and the runtime upgrade has failed because thresholds weren't met for compute and control nodes, re-execution of the runtime upgrade may be needed depending on when the failure occurred and the state of the individual servers in a rack. If a rack was updated before a failure, then the upgraded runtime version would be used when the nodes are reprovisioned. If the rack's spec wasn't updated to the upgraded runtime version before the hardware failure, the machine\nwould be provisioned with the previous runtime version. To upgrade to the new runtime version, submit a new cluster upgrade request and only the nodes with the previous runtime version will upgrade. Hosts that were successful in the previous upgrade action won't."}
{"text": "After a runtime upgrade the cluster shows "Failed""}
{"text": "Provisioning State\nDuring a runtime upgrade the cluster will enter a state of "Upgrading." In the event of a failure of the runtime upgrade, for reasons related to the resources, the cluster will go into a "Failed" Provisioning state. This state could be linked to the lifecycle of the components related to the cluster (e.g StorageAppliance) and may be necessary to diagnose the failure with Microsoft support."}
{"text": "Configure L2 and L3 isolation domains by using a managed"}
{"text": "network fabric\nArticle  05/24/2023\nFor Azure Operator Nexus instances, isolation domains enable communication between workloads hosted on the same rack (intra-rack communication) or different racks (inter-rack communication). This article describes how you can manage Layer 2 (L2) and Layer 3 (L3) isolation domains by using the Azure CLI. You can use the commands in this article to create, update, delete, and check the status of L2 and L3 isolation domains."}
{"text": "Prerequisites\n1. Ensure that a network fabric controller (NFC) and a network fabric have been created.\n2. Install the latest version of the  Azure CLI extension for managed network fabrics .\n3. Use the following command to sign in to your Azure account and set the subscription to your Azure subscription ID. This should be the same subscription ID that you use for all the resources in an Azure Operator Nexus instance.\nAzure CLI\n    az login      az account set  --subscription  ******** - **** - **** - **** - ********* \n4. Register providers for a managed network fabric:\na. In the Azure CLI, enter the command \naz provider register --namespace Microsoft.ManagedNetworkFabric\n.\nb. Monitor the registration process by using the command \naz provider show -n Microsoft.ManagedNetworkFabric -o table\n.\nRegistration can take up to 10 minutes. When it's finished, \nRegistrationState\n in the output changes to \nRegistered\n.\nIsolation domains are used to enable Layer 2 or Layer 3 connectivity between workloads hosted across the Azure Operator Nexus instance and external networks.\n  Note\nAzure Operator Nexus reserves VLANs up to 500 for platform use. You can't use VLANs in this range for your (tenant) workload networks. You should use VLAN values from 501 through 4095."}
{"text": "Configure L2 isolation domains\nYou use an L2 isolation domain to establish Layer 2 connectivity between workloads running on Azure Operator Nexus compute nodes.\nThe following parameters are available for configuring isolation domains.\nParameter Description Example Required\nresource-group\nResource\nResourceGroupName\nTrue group name specifically for the isolation domain of your choice.\nresource-name\nResource\nexample-l2domain\nTrue name of the L2 isolation domain.\nlocation\nAzure\neastus\nTrue Operator Nexus region used during NFC creation.\nParameter Description Example Required\nnf-Id\nNetwork\n/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx-\nTrue fabric ID.\nxxxxxx/resourceGroups/NFresourcegroupname/providers/Microsoft.ManagedNetworkFabric/NetworkFabrics/NFname Vlan-id\nVLAN\n501\nTrue identifier value. VLANs 1 to 500 are reserved and can't be used. The VLAN identifier value can't be changed after you specify it. You must delete and re- create the isolation domain if you need to modify the VLAN identifier value. The range is \n501\nto \n4095\n.\nmtu\nMaximum\n1500\ntransmission unit. If you don't specify a value, the default is\n1500\n.\nadministrativeState\nAdministrative\nEnable\nstate of the isolation domain, which you can enable or disable.\nsubscriptionId\nAzure subscription ID for your Azure Operator Nexus instance.\nprovisioningState\nProvisioning state.\nCreate an L2 isolation domain\nUse the following commands to create an L2 isolation domain:\nAzure CLI\naz nf l2domain create  \  --resource-group   "ResourceGroupName"  \  --resource-name   "example-l2domain"  \  --location   "eastus"  \  --nf-id   "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/NFResourceGroupName/providers/Microsoft.ManagedNetworkFabric/NetworkFabrics/NFname"  \  --vlan-id   750\  --mtu  1501 \nExpected output:\nOutput\n{    "administrativeState": "Disabled",    "annotation": null,user    "disabledOnResources": null,    "id": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/ResourceGroupName/providers/Microsoft.ManagedNetworkFabric/l2IsolationDomains/example-l2domain",    "location": "eastus",    "mtu": 1501,    "name": "example-l2domain",    "networkFabricId": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/NFresourceGroups/resourcegroupname/providers/Microsoft.ManagedNetworkFabric/networkFabrics/NFName",    "provisioningState": "Succeeded",    "resourceGroup": "ResourceGroupName",    "systemData": {      "createdAt": "2023-XX-XXT14:57:59.167177+00:00",      "createdBy": "email@address.com",      "createdByType": "User",      "lastModifiedAt": "2023-XX-XXT14:57:59.167177+00:00",      "lastModifiedBy": "email@address.com",      "lastModifiedByType": "User"    },    "tags": null,    "type": "microsoft.managednetworkfabric/l2isolationdomains",    "vlanId": 750  } \nShow L2 isolation domains\nThis command shows details about L2 isolation domains, including their administrative states:\nAzure CLI\naz nf l2domain show  --resource-group   "ResourceGroupName"   --resource-name   "example-l2domain"  \nExpected output:\nOutput\n{    "administrativeState": "Disabled",    "annotation": null,    "disabledOnResources": null,    "id": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/ResourceGroupName/providers/Microsoft.ManagedNetworkFabric/l2IsolationDomains/example-l2domain",    "location": "eastus",    "mtu": 1501,    "name": "example-l2domain",    "networkFabricId": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/NFResourceGroupName/providers/Microsoft.ManagedNetworkFabric/networkFabrics/NFName",    "provisioningState": "Succeeded",    "resourceGroup": "ResourceGroupName",    "systemData": {      "createdAt": "2023-XX-XXT14:57:59.167177+00:00",      "createdBy": "email@address.com",      "createdByType": "User",      "lastModifiedAt": "2023-XX-XXT14:57:59.167177+00:00",      "lastModifiedBy": "email@address.com",      "lastModifiedByType": "User"    },    "tags": null,    "type": "microsoft.managednetworkfabric/l2isolationdomains",    "vlanId": 750  } \nList all L2 isolation domains\nThis command lists all L2 isolation domains available in a resource group:\nAzure CLI\naz nf l2domain list  --resource-group   "ResourceGroupName"  \nExpected output:\nOutput\n {      "administrativeState": "Enabled",      "annotation": null,      "disabledOnResources": null,      "id": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/ResourceGroupName/providers/Microsoft.ManagedNetworkFabric/l2IsolationDomains/example-l2domain",      "location": "eastus",      "mtu": 1501,      "name": "example-l2domain",      "networkFabricId": "/subscriptions/xxxxxx-xxxxxx-xxxx- xxxxxxxxxx/resourceGroups/NFResourceGroupName/providers/Microsoft.ManagedNetworkFabric/networkFabrics/NFName",      "provisioningState": "Succeeded",      "resourceGroup": "ResourceGroupName",      "systemData": {        "createdAt": "2022-XX-XXT22:26:33.065672+00:00",        "createdBy": "email@address.com",        "createdByType": "User",        "lastModifiedAt": "2022-XX-XXT14:46:45.753165+00:00",        "lastModifiedBy": "d1bd24c7-b27f-477e-86dd-939e107873d7",        "lastModifiedByType": "Application"      },      "tags": null,      "type": "microsoft.managednetworkfabric/l2isolationdomains",      "vlanId": 750    } \nChange the administrative state of an L2 isolation domain\nYou must enable an isolation domain to push the configuration to the network fabric devices. Use the following command to change the administrative state of an isolation domain:\nAzure CLI\naz nf l2domain update-admin-state  --resource-group   "ResourceGroupName"   --resource-name   "example-l2domain"   --state   Enable/Disable \nExpected output:\nOutput\n{    "administrativeState": "Enabled",    "annotation": null,    "disabledOnResources": null,    "id": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/ResourceGroupName/providers/Microsoft.ManagedNetworkFabric/l2IsolationDomains/example-l2domain",    "location": "eastus",    "mtu": 1501,    "name": "example-l2domain",    "networkFabricId": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/NFResourceGroupName/providers/Microsoft.ManagedNetworkFabric/networkFabrics/NFName",    "provisioningState": "Succeeded",    "resourceGroup": "ResourceGroupName",    "systemData": {      "createdAt": "2023-XX-XXT14:57:59.167177+00:00",      "createdBy": "email@address.com",      "createdByType": "User",      "lastModifiedAt": "2023-XX-XXT14:57:59.167177+00:00",      "lastModifiedBy": "d1bd24c7-b27f-477e-86dd-939e107873d7",      "lastModifiedByType": "Application"    },    "tags": null,    "type": "microsoft.managednetworkfabric/l2isolationdomains",    "vlanId": 501  } \nDelete an L2 isolation domain\nUse this command to delete an L2 isolation domain:\nAzure CLI\naz nf l2domain delete  --resource-group   "ResourceGroupName"   --resource-name   "example-l2domain"  \nExpected output:\nOutput\nPlease use show or list command to validate that the isolation domain is deleted. Deleted resources will not appear in the  output"}
{"text": "Configure L3 isolation domains\nA Layer 3 isolation domain enables L3 connectivity between workloads running on Azure Operator Nexus compute nodes. The L3 isolation domain enables the workloads to exchange L3 information with network fabric devices.\nA Layer 3 isolation domain has two components:\nAn  internal network  defines Layer 3 connectivity between network fabrics running on Azure Operator Nexus compute nodes and an optional external network. You must create at least one internal network. An  external network  provides connectivity between the internet and internal networks via your private endpoints.\nAn L3 isolation domain enables deploying workloads that advertise service IPs to the fabric via BGP.\nAn L3 isolation domain has two ASNs:\nThe  fabric ASN  is the ASN of the network devices on the fabric. It's specified while you're creating the network fabric. The  peer ASN  is the ASN of the network functions in Azure Operator Nexus. It can't be the same as the fabric ASN.\nThe workflow for a successful provisioning of an L3 isolation domain is as follows:\n1. Create an L3 isolation domain. 2. Create one or more internal networks. 3. Enable an L3 isolation domain.\nTo make changes to the L3 isolation domain, first disable it (administrative state). Re-enable the L3 isolation domain (administrative state) after you finish the changes.\nThe procedure to show, enable/disable, and delete IPv6-based isolation domains is the same as the one that you use for IPv4. The VLAN range for creating an isolation domain is 501 to 4095.\nThe following parameters are available for configuring L3 isolation domains.\nParameter Description Example Required\nresource-\nResource group\nResourceGroupName\nTrue name\ngroup\nspecifically for the isolation domain of your choice\nresource-\nResource name\nexample-l3domain\nTrue of the L3\nname\nisolation domain\nlocation\nAzure Operator\neastus\nTrue Nexus region used during NFC creation\nnf-Id\nAzure\n/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx-\nTrue subscription ID\nxxxxxx/resourceGroups/NFResourceGroupName/providers/Microsoft.ManagedNetworkFabric/NetworkFabrics/NFName\nused during NFC creation\nThe following parameters for isolation domains are optional.\nParameter Description Example Required Parameter Description Example Required\nredistributeConnectedSubnet\nAdvertised connected subnets, which can have a value of \nTrue\n or \nFalse\n. The default value is \nTrue\n.\nTrue redistributeStaticRoutes\nAdvertised static routes, which can have a value of \nTrue\n or \nFalse\n. The default value is \nFalse\n.\nFalse aggregateRouteConfiguration\nList of IPv4 and IPv6 route configurations.\nCreate an L3 isolation domain\nUse this command to create an L3 isolation domain:\nAzure CLI\naz nf l3domain create    --resource-group   "ResourceGroupName"    --resource-name   "example-l3domain"   --location   "eastus"    --nf-id   "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/NFResourceGroupName/providers/Microsoft.ManagedNetworkFabric/NetworkFabrics/NFName"  \n  Note\nFor MPLS Option B connectivity to external networks via private endpoint devices, you can specify Option B parameters while creating an isolation domain.\nExpected output:\nOutput\n{    "administrativeState": "Disabled",    "aggregateRouteConfiguration": null,    "annotation": null,    "connectedSubnetRoutePolicy": null,    "description": null,    "disabledOnResources": null,    "id": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/ResourceGroupName/providers/Microsoft.ManagedNetworkFabric/l3IsolationDomains/example-l3domain",    "location": "eastus",    "name": "example-l3domain",    "networkFabricId": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/NFresourceGroups/NFResourceGroupName/providers/Microsoft.ManagedNetworkFabric/networkFabrics/NFName",    "optionBDisabledOnResources": null,    "provisioningState": "Accepted",    "redistributeConnectedSubnets": "True",    "redistributeStaticRoutes": "False",    "resourceGroup": "ResourceGroupName",    "systemData": {      "createdAt": "2022-XX-XXT06:23:43.372461+00:00",      "createdBy": "email@example.com",      "createdByType": "User",      "lastModifiedAt": "2023-XX-XXT09:40:38.815959+00:00",      "lastModifiedBy": "email@example.com",      "lastModifiedByType": "User"    },    "tags": null,    "type": "microsoft.managednetworkfabric/l3isolationdomains"  } \nCreate an untrusted L3 isolation domain\nAzure CLI\naz nf l3domain create  --resource-group   "ResourceGroupName"   --resource-name   "l3untrust"   --location   "eastus"   --nf-id   "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/NFResourceGroupName/providers/Microsoft.ManagedNetworkFabric/networkFabrics/NFName"   \nCreate a trusted L3 isolation domain\nAzure CLI\naz nf l3domain create  --resource-group   "ResourceGroupName"   --resource-name   "l3trust"   --location   "eastus"   --nf-id   "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/NFResourceGroupName/providers/Microsoft.ManagedNetworkFabric/networkFabrics/NFName"  \nCreate a management L3 isolation domain\nAzure CLI\naz nf l3domain create  --resource-group   "ResourceGroupName"   --resource-name   "l3mgmt"   --location   "eastus"   --nf-id   "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/NFResourceGroupName/providers/Microsoft.ManagedNetworkFabric/networkFabrics/NFName"  \nShow L3 isolation domains\nThis command shows details about L3 isolation domains, including their administrative states:\nAzure CLI\naz nf l3domain show  --resource-group   "ResourceGroupName"   --resource-name   "example-l3domain"  \nExpected output:\nOutput\n{    "administrativeState": "Disabled",    "aggregateRouteConfiguration": null,    "annotation": null,    "connectedSubnetRoutePolicy": null,    "description": null,    "disabledOnResources": null,    "id": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/ResourceGroupName/providers/Microsoft.ManagedNetworkFabric/l3IsolationDomains/example-l3domain",    "location": "eastus",    "name": "example-l3domain",    "networkFabricId": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/NFresourceGroups/NFResourceGroupName/providers/Microsoft.ManagedNetworkFabric/networkFabrics/NFName",    "optionBDisabledOnResources": null,    "provisioningState": "Succeeded",    "redistributeConnectedSubnets": "True",    "redistributeStaticRoutes": "False",    "resourceGroup": "ResourceGroupName",    "systemData": {      "createdAt": "2023-XX-XXT09:40:38.815959+00:00",      "createdBy": "email@example.com",      "createdByType": "User",      "lastModifiedAt": "2023-XX-XXT09:40:46.923037+00:00",      "lastModifiedBy": "d1bd24c7-b27f-477e-86dd-939e107873d7",      "lastModifiedByType": "Application"    },    "tags": null,    "type": "microsoft.managednetworkfabric/l3isolationdomains"  } \nList all L3 isolation domains\nUse this command to get a list of all L3 isolation domains available in a resource group:\nAzure CLI\naz nf l3domain list  --resource-group   "ResourceGroupName"  \nExpected output:\nOutput\n{    "administrativeState": "Disabled", \n  "aggregateRouteConfiguration": null,    "annotation": null,    "connectedSubnetRoutePolicy": null,    "description": null,    "disabledOnResources": null,    "id": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/ResourceGroupName/providers/Microsoft.ManagedNetworkFabric/l3IsolationDomains/example-l3domain",    "location": "eastus",    "name": "example-l3domain",    "networkFabricId": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/NFresourceGroups/NFResourceGroupName/providers/Microsoft.ManagedNetworkFabric/networkFabrics/NFName",    "optionBDisabledOnResources": null,    "provisioningState": "Succeeded",    "redistributeConnectedSubnets": "True",    "redistributeStaticRoutes": "False",    "resourceGroup": "ResourceGroupName",    "systemData": {      "createdAt": "2023-XX-XXT09:40:38.815959+00:00",      "createdBy": "email@example.com",      "createdByType": "User",      "lastModifiedAt": "2023-XX-XXT09:40:46.923037+00:00",      "lastModifiedBy": "d1bd24c7-b27f-477e-86dd-939e107873d7",      "lastModifiedByType": "Application"    },    "tags": null,    "type": "microsoft.managednetworkfabric/l3isolationdomains"  } \nChange the administrative state of an L3 isolation domain\nUse the following command to change the administrative state of an L3 isolation domain to enabled or disabled:\nAzure CLI\naz nf l3domain update-admin-state  --resource-group   "ResourceGroupName"   --resource-name   "example-l3domain"   --state   Enable/Disable \nExpected output:\nOutput\n{      "administrativeState": "Enabled",      "annotation": null,      "description": null,      "disabledOnResources": null,      "external": null,      "id": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/ResourceGroupName/providers/Microsoft.ManagedNetworkFabric/l3IsolationDomains/example-l3domain",      "internal": null,      "location": "eastus",      "name": "example-l3domain",      "networkFabricId": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/NFresourceGroups/NFResourceGroupName/providers/Microsoft.ManagedNetworkFabric/networkFabrics/NFName",      "optionBDisabledOnResources": null,      "provisioningState": "Succeeded",      "resourceGroup": "NFResourceGroupName",      "systemData": {        "createdAt": "2022-XX-XXT06:23:43.372461+00:00",        "createdBy": "email@address.com",        "createdByType": "User",        "lastModifiedAt": "2022-XX-XXT06:25:53.240975+00:00",        "lastModifiedBy": "d1bd24c7-b27f-477e-86dd-939e107873d7",        "lastModifiedByType": "Application"      },      "tags": null,      "type": "microsoft.managednetworkfabric/l3isolationdomains"    } \nUse the \naz show\n command to verify whether the administrative state has changed to \nEnabled\n.\nDelete an L3 isolation domain\nUse this command to delete an L3 isolation domain:\nAzure CLI\n az nf l3domain delete  --resource-group   "ResourceGroupName"   --resource-name   "example-l3domain"  \nUse the \nshow\n or \nlist\n command to validate that the isolation domain has been deleted."}
{"text": "Create internal networks\nAfter you successfully create an L3 isolation domain, the next step is to create an internal network. Internal networks enable Layer 3 inter- rack and intra-rack communication between workloads by exchanging routes with the fabric. An L3 isolation domain can support multiple internal networks, each on a separate VLAN.\nThe following diagram represents an example network function with three internal networks: trusted, untrusted, and management. Each of the internal networks is created in its own L3 isolation domain.\nThe IPv4 prefixes for these networks are:\nTrusted network: 10.151.1.11/24 Management network: 10.151.2.11/24 Untrusted network: 10.151.3.11/24\nThe following parameters are available for creating internal networks.\nParameter Description Example Required\nvlan-Id\nVLAN identifier with a range from 501 to 4095\n1001\nTrue\nresource-group\nCorresponding NFC resource group name\nNFCresourcegroupname\nTrue\nl3-isolation-domain-name\nResource name of the L3 isolation domain\nexample-l3domain\nTrue\nlocation\nAzure Operator Nexus region used during NFC creation\neastus\nTrue\nThe following parameters are optional for creating internal networks.\nParameter Description Example Required\nconnectedIPv4Subnets\nIPv4 subnet that the Azure Kubernetes Service hybrid (HAKS) cluster's workloads use.\n10.0.0.0/24 connectedIPv6Subnets\nIPv6 subnet that the HAKS cluster's workloads use.\ndf8:f53b:82e4::53/127 staticRouteConfiguration\nIPv4 prefix of the static route.\n10.0.0.0/24 bgpConfiguration\nIPv4 next-hop address.\n10.0.0.0/24 defaultRouteOriginate True\n/\n False\n parameter that enables the default route to be originated when you're\nTrue\nadvertising routes via BGP.\nParameter Description Example Required\npeerASN\nPeer ASN of a network function.\n65047 allowAS\nAllows for routes to be received and processed even if the router detects its own ASN in\n2\nthe AS path. Input \n0\n to disable. Otherwise, possible values are \n1\n to \n10\n. The default is\n2\n.\nallowASOverride\nEnables or disables \nallowAS\n.\nEnable ipv4ListenRangePrefixes\nBGP IPv4 listen range; maximum range allowed in /28.\n10.1.0.0/26 ipv6ListenRangePrefixes\nBGP IPv6 listen range; maximum range allowed in /127.\n3FFE:FFFF:0:CD30::/126 ipv4NeighborAddress\nIPv4 neighbor address.\n10.0.0.11 ipv6NeighborAddress\nIPv6 neighbor address.\ndf8:f53b:82e4::53/127\nYou need to create an internal network before you enable an L3 isolation domain. This command creates an internal network with BGP configuration and a specified peering address:\nAzure CLI\naz nf internalnetwork create    --resource-group   "ResourceGroupName"    --l 3 -isolation-domain-name   "example-l3domain"    --resource-name   "example-internalnetwork"    --location   "eastus"   --vlan-id  805   --connected-ipv 4 -subnets   '[{"prefix":"10.1.2.0/24"}]'    --mtu  1500   --bgp-configuration    '{"defaultRouteOriginate": "True", "allowAS": 2, "allowASOverride": "Enable", "PeerASN": 65535,  "ipv4ListenRangePrefixes": ["10.1.2.0/28"]}'  \nExpected output:\nOutput\n{     "administrativeState": "Enabled",     "annotation": null,     "bfdDisabledOnResources": null,     "bfdForStaticRoutesDisabledOnResources": null,     "bgpConfiguration": {       "allowAs": 2,       "allowAsOverride": "Enable",      "annotation": null,       "bfdConfiguration": null,       "defaultRouteOriginate": "True",       "fabricAsn": 65046,       "ipv4ListenRangePrefixes": [        "10.1.2.0/28"       ],       "ipv4NeighborAddress": null,      "ipv6ListenRangePrefixes": null,       "ipv6NeighborAddress": null,      "peerAsn": 65535     },     "bgpDisabledOnResources": null,     "connectedIPv4Subnets": [       {         "annotation": null,         "prefix": "10.1.2.0/24"       }     ],     "connectedIPv6Subnets": null,     "disabledOnResources": null,     "exportRoutePolicyId": null,     "id": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/ResourceGroupName/providers/Microsoft.ManagedNetworkFabric/l3IsolationDomains/example-l3domain",     "importRoutePolicyId": null,     "mtu": 1500,     "name": "internalnetwork805",     "provisioningState": "Accepted",     "resourceGroup": "ResourceGroupName",     "staticRouteConfiguration": null,     "systemData": {  \n    "createdAt": "2023-XX-XXT05:26:33.547816+00:00",       "createdBy": "email@example.com",       "createdByType": "User",       "lastModifiedAt": "2023-XX-XXT05:26:33.547816+00:00",       "lastModifiedBy": "email@example.com",       "lastModifiedByType": "User"    },     "type": "microsoft.managednetworkfabric/l3isolationdomains/internalnetworks",     "vlanId": 805   } \nCreate an untrusted internal network for an L3 isolation domain\nAzure CLI\naz nf internalnetwork create  --resource-group   "ResourceGroupName"   --l 3 -isolation-domain-name  l3untrust  --resource-name   untrustnetwork  --location   "eastus"   --vlan-id  502  --fabric-asn  65048  --peer-asn  65047 --connected-i-pv 4 -subnets   prefix= "10.151.3.11/24"   --mtu  1500 \nCreate a trusted internal network for an L3 isolation domain\nAzure CLI\naz nf internalnetwork create  --resource-group   "ResourceGroupName"   --l 3 -isolation-domain-name  l3trust  --resource-name   trustnetwork  --location   "eastus"   --vlan-id  503  --fabric-asn  65048  --peer-asn  65047 --connected-i-pv 4 -subnets   prefix= "10.151.1.11/24"   --mtu  1500 \nCreate an internal management network for an L3 isolation domain\nAzure CLI\naz nf internalnetwork create  --resource-group   "ResourceGroupName"   --l 3 -isolation-domain-name  l3mgmt  --resource-name   mgmtnetwork  --location   "eastus"   --vlan-id  504  --fabric-asn  65048  --peer-asn  65047 --connected-i-pv 4 -subnets   prefix= "10.151.2.11/24"   --mtu  1500 \nCreate multiple static routes with a single next hop\nAzure CLI\naz nf internalnetwork create    --resource-name   "example-internalnetwork"    --l 3domain  "example-l3domain"    --resource-group   "ResourceGroupName"    --location   "eastus"    --vlan-id   "2028"    --mtu   "1500"    --connected-ipv 4 -subnets   '[{"prefix":"10.18.34.0/24","gateway":"10.18.34.2"}]'   --bgp-configuration   '{"defaultRouteOriginate":true,"peerASN":65510,"ipv4Prefix":"10.18.34.0/24"}'   --static-route-configuration   '{"ipv4Routes":[{"prefix":"10.23.0.0/19","nextHop":["10.20.0.1"]}, {"prefix":"10.24.0.0/19","nextHop":["10.20.0.1"]}]}'  \nExpected output:\nOutput\n{     "administrativeState": "Enabled",     "annotation": null,     "bfdDisabledOnResources": null,     "bfdForStaticRoutesDisabledOnResources": null,     "bgpConfiguration": {       "allowAs": 2,       "allowAsOverride": "Enable",      "annotation": null,       "bfdConfiguration": null,       "defaultRouteOriginate": "True",       "fabricAsn": 65046,       "ipv4ListenRangePrefixes": null,  \n    "ipv4NeighborAddress": null,      "ipv6ListenRangePrefixes": null,       "ipv6NeighborAddress": null,      "peerAsn": 65510     },     "bgpDisabledOnResources": null,     "connectedIPv4Subnets": [      {         "annotation": null,         "prefix": "10.18.34.0/24"       }     ],     "connectedIPv6Subnets": null,     "disabledOnResources": null,     "exportRoutePolicyId": null,     "id": "/subscriptions//xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx7/resourceGroups/NFResourceGroupName/providers/Microsoft.ManagedNetworkFabric/l3IsolationDomains/example- l3domain/internalNetworks/example-internalnetwor",     "importRoutePolicyId": null,     "mtu": 1500,     "name": "example-internalnetwork",     "provisioningState": "Accepted",     "resourceGroup": "ResourceGroupName",     "staticRouteConfiguration": {       "bfdConfiguration": null,       "ipv4Routes": [         {           "nextHop": [             "10.20.0.1"           ],           "prefix": "10.23.0.0/19"        },         {           "nextHop": [             "10.20.0.1"           ],           "prefix": "10.24.0.0/19"        }       ],       "ipv6Routes": null     },     "systemData": {       "createdAt": "2023-XX-XXT13:46:26.394343+00:00",       "createdBy": "email@example.com",       "createdByType": "User",       "lastModifiedAt": "2023-XX-XXT13:46:26.394343+00:00",       "lastModifiedBy": "email@example.com",       "lastModifiedByType": "User"    },     "type": "microsoft.managednetworkfabric/l3isolationdomains/internalnetworks",     "vlanId": 2028   }  \nCreate an internal network by using IPv6\nAzure CLI\naz nf internalnetwork create    --resource-group   "ResourceGroupName"    --l 3 -isolation-domain-name   "example-l3domain"    --resource-name   "example-internalipv6network"    --location   "eastus"    --vlan-id  1090   --connected-ipv 6 -subnets   '[{"prefix":"10:101:1::0/64", "gateway":"10:101:1::1"}]'    --mtu  1500  --bgp-configuration   '{"defaultRouteOriginate":true,"peerASN": 65020,"ipv6NeighborAddress":[{"address":  "df8:f53b:82e4::53/127"}]}'  \nExpected output:\nOutput\n{     "administrativeState": "Enabled",     "annotation": null,     "bfdDisabledOnResources": null,     "bfdForStaticRoutesDisabledOnResources": null,     "bgpConfiguration": {  \n    "allowAs": 2,       "allowAsOverride": "Enable",      "annotation": null,       "bfdConfiguration": null,       "defaultRouteOriginate": "True",       "fabricAsn": 65046,       "ipv4ListenRangePrefixes": null,       "ipv4NeighborAddress": null,      "ipv6ListenRangePrefixes": null,       "ipv6NeighborAddress": [         {           "address": "df8:f53b:82e4::53/127",           "operationalState": "Disabled"         }       ],       "peerAsn": 65020     },     "bgpDisabledOnResources": null,     "connectedIPv4Subnets": null,     "connectedIPv6Subnets": [       {         "annotation": null,         "prefix": "10:101:1::0/64"      }     ],     "disabledOnResources": null,     "exportRoutePolicyId": null,     "id": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/NFResourceGroupName/providers/Microsoft.ManagedNetworkFabric/l3IsolationDomains/l3domain2/internalNe tworks/internalipv6network",     "importRoutePolicyId": null,     "mtu": 1500,     "name": "internalipv6network",    "provisioningState": "Succeeded",     "resourceGroup": "ResourceGroupName",     "staticRouteConfiguration": null,     "systemData": {       "createdAt": "2023-XX-XXT10:34:33.933814+00:00",       "createdBy": "email@example.com",       "createdByType": "User",       "lastModifiedAt": "2023-XX-XXT10:34:33.933814+00:00",       "lastModifiedBy": "email@example.com",       "lastModifiedByType": "User"    },     "type": "microsoft.managednetworkfabric/l3isolationdomains/internalnetworks",     "vlanId": 1090   }"}
{"text": "Create external networks\nExternal networks enable workloads to have Layer 3 connectivity with your provider edge. They also allow for workloads to interact with external services like firewalls and DNS. You need the fabric ASN (created during network fabric creation) to create external networks.\nThe commands for creating an external network by using Azure CLI include the following parameters.\nParameter Description Example Required\npeeringOption\nPeering that uses either Option A or Option B. Possible values are \nOptionA\n and \nOptionB\n.\nOptionB\nTrue\noptionBProperties\nConfiguration of Option B properties. To specify, use \nexportRouteTargets\n or \nimportRouteTargets\n.\n"exportRouteTargets": ["1234:1234"]}}\noptionAProperties\nConfiguration of Option A properties.\nexternal\nOptional parameter to input MPLS Option B connectivity to external networks via private endpoint devices. By using this option, you can input import and export route targets as shown in the example.\nFor Option A, you need to create an external network before you enable the L3 isolation domain. An external network is dependent on an internal network, so an external network can't be enabled without an internal network. The \nvlan-id\n value should be from \n501\n to \n4095\n.\nCreate an external network by using Option B\nAzure CLI\naz nf externalnetwork create    --resource-group   "ResourceGroupName"    --l 3domain  "examplel3domain"    --resource-name   "examplel3-externalnetwork"    --location   "eastus"    --peering-option   "OptionB"   --option-b-properties   '{"importRouteTargets": ["65541:2001"], "exportRouteTargets":  ["65531:2001"]}'  \nExpected output:\nOutput\n{    "administrativeState": "Enabled",    "annotation": null,    "disabledOnResources": null,    "exportRoutePolicyId": null,    "id": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxxX/resourceGroups/NFResourceGroupName/providers/Microsoft.ManagedNetworkFabric/l3IsolationDomains/examplel3isolationd omain/externalNetworks/example-externalnetwork",    "importRoutePolicyId": null,    "name": "examplel3-externalnetwork",    "networkToNetworkInterconnectId": null,    "optionAProperties": null,    "optionBProperties": {      "exportRouteTargets": [        "65531:2001"      ],      "importRouteTargets": [        "65541:2001"      ]    },    "peeringOption": "OptionB",    "provisioningState": "Succeeded",    "resourceGroup": "ResourceGroupName",    "systemData": {      "createdAt": "2023-XX-XXT15:45:31.938216+00:00",      "createdBy": "email@address.com",      "createdByType": "User",      "lastModifiedAt": "2023-XX-XXT15:45:31.938216+00:00",      "lastModifiedBy": "email@address.com",      "lastModifiedByType": "User"    },    "type": "microsoft.managednetworkfabric/l3isolationdomains/externalnetworks"  } \nCreate an external network by using Option A\nAzure CLI\naz nf externalnetwork create   --resource-group   "ResourceGroupName"    --l 3domain  "example-l3domain"    --resource-name   "example-externalipv4network"    --location   "eastus"   --peering-option   "OptionA"    --option-a-properties   '{"peerASN": 65026,"vlanId": 2423, "mtu": 1500, "primaryIpv4Prefix": "10.18.0.148/30",  "secondaryIpv4Prefix": "10.18.0.152/30"}'  \nExpected output:\nOutput\n{     "administrativeState": "Enabled",     "annotation": null,     "disabledOnResources": null,     "exportRoutePolicyId": null,     "id": "/subscriptions/xxxxxx-xxxxxx-xxxx-xxxx- xxxxxxX/resourceGroups/NFResourceGroupName/providers/Microsoft.ManagedNetworkFabric/l3IsolationDomains/examplel3isolationd omain/externalNetworks/example-externalnetwork",     "importRoutePolicyId": null,     "name": "example-externalipv4network",     "networkToNetworkInterconnectId": null,     "optionAProperties": {       "bfdConfiguration": null,  \n    "fabricAsn": 65026,       "mtu": 1500,       "peerAsn": 65026,       "primaryIpv4Prefix": "10.18.0.148/30",       "primaryIpv6Prefix": null,       "secondaryIpv4Prefix": "10.18.0.152/30",       "secondaryIpv6Prefix": null,      "vlanId": 2423     },     "optionBProperties": null,     "peeringOption": "OptionA",     "provisioningState": "Accepted",     "resourceGroup": "ResourceGroupName",     "systemData": {       "createdAt": "2023-XX-XXT07:23:54.396679+00:00",       "createdBy": "email@address.com",       "createdByType": "User",       "lastModifiedAt": "2023-XX-XX1T07:23:54.396679+00:00",       "lastModifiedBy": "email@address.com",       "lastModifiedByType": "User"    },     "type": "microsoft.managednetworkfabric/l3isolationdomains/externalnetworks"   } \nCreate an external network by using IPv6\nAzure CLI\naz nf externalnetwork create    --resource-group   "ResourceGroupName"   --l 3 -isolation-domain-name   "example-l3domain"   --resource-name   "example-externalipv6network"   --location   "eastus"   --vlan-id  506  --peer-asn  65022  --primary-ipv 6 -prefix   "10:101:2::0/127"   --secondary-ipv 6 -prefix   "10:101:3::0/127"  \nThe supported primary and secondary IPv6 prefix size is /127.\nExpected output:\nOutput\n{    "administrativeState": null,    "annotation": null,    "bfdConfiguration": null,    "bfdDisabledOnResources": null,   "bgpDisabledOnResources": null,   "disabledOnResources": null,    "exportRoutePolicyId": null,    "fabricAsn": 65026,    "id": "/subscriptions//xxxxxx-xxxxxx-xxxx-xxxx- xxxxxx/resourceGroups/NFResourceGroupName/providers/Microsoft.ManagedNetworkFabric/l3IsolationDomains/example- l3domain/externalNetworks/example-externalipv6network",    "importRoutePolicyId": null,    "mtu": 1500,    "name": "example-externalipv6network",    "peerAsn": 65022,    "primaryIpv4Prefix": "10:101:2::0/127",    "primaryIpv6Prefix": null,    "provisioningState": "Succeeded",    "resourceGroup": "ResourceGroupName",    "secondaryIpv4Prefix": null,    "secondaryIpv6Prefix": "10:101:3::0/127",    "systemData": {      "createdAt": "2022-XX-XXT07:52:26.366069+00:00",      "createdBy": "email@address.com",      "createdByType": "User",      "lastModifiedAt": "2022-XX-XXT07:52:26.366069+00:00",      "lastModifiedBy": "",      "lastModifiedByType": "User"    },    "type": "microsoft.managednetworkfabric/l3isolationdomains/externalnetworks", \n  "vlanId": 506  }"}
{"text": "Enable an L2 isolation domain\nAzure CLI\naz nf l2domain update-administrative-state  --resource-group   "ResourceGroupName"   --resource-name   "l2HAnetwork"   --state   Enable"}
{"text": "Enable an L3 isolation domain\nUse this command to enable an untrusted L3 isolation domain:\nAzure CLI\naz nf l3domain update-admin-state  --resource-group   "ResourceGroupName"   --resource-name   "l3untrust"   --state  Enable  \nUse this command to enable a trusted L3 isolation domain:\nAzure CLI\naz nf l3domain update-admin-state  --resource-group   "ResourceGroupName"   --resource-name   "l3trust"   --state  Enable  \nUse this command to enable a management L3 isolation domain:\nAzure CLI\naz nf l3domain update-admin-state  --resource-group   "ResourceGroupName"   --resource-name   "l3mgmt"   --state  Enable"}
{"text": "Route Policy in Network Fabric\nArticle  05/25/2023\nRoute policies provides Operators the capability to allow or deny routes in regards to Layer 3 isolation domains in Network Fabric.\nWith route policies, routes are tagged with certain attributes via community values and extended community values when they're distributed via Border Gateway Patrol (BGP). Similarly, on the BGP listener side, route policies can be authored to discard/allow routes based on community values and extended community value attributes.\nRoute policies enable operators to control routes learnt/distributed via BGP. Each route policy is modeled as a separate top level Azure Resource Manager (ARM) resource under \nMicrosoft.managednetworkfabric\n. Operators can create, read, and delete route\npolicy resources. The operator creates a route policy ARM resource and then sets the ID in the L3 isolation domain at the required enforcement point. A route policy can only be applied at a single enforcement point. A route policy can't be applied at multiple enforcement points.\nIn a network fabric, route policies can be enforced at the following endpoints of a layer 3 isolation domain:\nExternal networks  (option  A  and option  B ):\nFor egress, set the \nexportRoutePolicyId\n property of the external network resource to\nthe route policy resource ID created for egress direction. Set the \nimportRoutePolicyId\nproperty of the external network resource to the route policy resource ID created for ingress direction.\nInternal networks:\nFor egress, set the \nexportRoutePolicyId\n property of the internal network resource to the\nroute policy resource ID created for egress direction. Set the \nimportRoutePolicyId\nproperty of the internal network resource to the route policy resource ID created for ingress direction.\nConnected subnets across all internal networks:\nFor egress, set the \nconnectedSubnetRoutePolicy\n property of the L3 isolation domain to\nthe route policy resource ID created for egress direction."}
{"text": "Conditions and actions of a route policy\nThe following combinations of conditions can be specified:\nIP Prefix IP community Extended community list"}
{"text": "Actions\nThe following actions can be specified when there's a match of conditions:\nDiscard the route Permit the route and apply one of the following specific actions Add/Remove specified community values and extended community values Overwrite specified community values and extended community values"}
{"text": "IP prefix\nIP prefixes are used in specifying match conditions for route policies. An IP prefix resource allows operators to manipulate routes based on the IP prefix (IPv4 and IPv6). The IP prefixes enable operators to drop certain prefixes from being propagated up- stream/down-stream or tag them with specific community or extended community values. The operator must create an ARM resource of the type IP-Prefix by providing a list of prefixes with sequence numbers and action.\nThe prefixes in the list are processed in ascending order and the matching process stops after the first match. If the first match condition is "deny", the route is dropped and isn't propagated further. If the first match condition is "allow", further matching is aborted and the route is handled based on the action part of the route policies.\nIP prefixes specify only the match conditions of route policies. They don't specify the action part of route policies."}
{"text": "Parameters for IP prefix\nParameter Description Example Required\nresource-group Use an appropriate resource group name ResourceGroupName True specifically for the IP prefix of your choice\nresource-name Resource Name of the IP prefix ipprefixv4-1204-cn1 True\nlocation Azure region used during NFC creation eastus True\nParameter Description Example Required\naction Action to be taken for the prefix  Permit Deny or Permit True\nsequenceNumber Sequence in which the prefixes are 100 True processed. Prefix lists are evaluated starting with the lowest sequence number and continue down the list until a match is made. Once a match is made, the permit or deny statement is applied to that network and the rest of the list is ignored\nnetworkPrefix Network Prefix specifying IPv4/IPv6 1.1.1.0/24 True packets to be permitted or denied.\ncondition Specified prefix list bounds- EqualTo | EqualTo GreaterThanOrEqualTo | LesserThanOrEqualTo\n| subnetMaskLength | SubnetMaskLength specifies the minimum networkPrefix length to be matched. Required when condition is specified. | 32| |"}
{"text": "Create IP Prefix\nThis command creates an IP prefix resource with IPv4 prefix rules:\nAzure CLI\naz nf ipprefix create  \  --resource-group   "ResourceGroupName"  \  --resource-name   "ipprefixv4-1204-cn1"  \  --location   "eastus"  \  --ip-prefix-rules   '[{"action": "Permit", "sequenceNumber": 10,  "networkPrefix": "10.10.10.0/28", "condition": "EqualTo",  "subnetMaskLength": 28}, {"action": "Permit", "sequenceNumber": 12,  "networkPrefix": "20.20.20.0/24", "condition": "EqualTo",  "subnetMaskLength": 24}]'  \nExpected output:\nOutput\n{    "annotation": null,    "id": "/subscriptions/xxxx- xxxx/resourceGroups/ResourceGroupName/providers/Microsoft.ManagedNetworkFabr ic/ipPrefixes/ipprefixv4-1204-cn1",    "ipPrefixRules": [ \n    {        "action": "Permit",        "condition": "GreaterThanOrEqualTo",        "networkPrefix": "10.10.10.0/28",        "sequenceNumber": 10,        "subnetMaskLength": 28      }    ],    "location": "eastus",    "name": " ipprefixv4-1204-cn1",   "provisioningState": "Succeeded",    "resourceGroup": "ResourceGroupName",    "systemData": {      "createdAt": "2023-XX-XXT09:34:19.095543+00:00",      "createdBy": "user@address.com",      "createdByType": "User",      "lastModifiedAt": "2023-XX-XXT09:34:19.095543+00:00",      "lastModifiedBy": "user@address.com",      "lastModifiedByType": "User"    },    "tags": null,    "type": "microsoft.managednetworkfabric/ipprefixes"  } \nThis command creates an IP prefix resource with IPv6 prefix rules,\nAzure CLI\naz nf ipprefix create  \  --resource-group   "ResourceGroupName"  \  --resource-name   "ipprefixv6-2701-cn1"  \  --location   "eastus"  \  --ip-prefix-rules   '[{"action": "Permit", "sequenceNumber": 10,  "networkPrefix": "fda0:d59c:da12:20::/64", "condition":  "GreaterThanOrEqualTo", "subnetMaskLength": 68}]'  \nExpected Output\nOutput\n{    "annotation": null,    "id": "/subscriptions/xxxx- xxxx/resourceGroups/ResourceGroupName/providers/Microsoft.ManagedNetworkFabr ic/ipPrefixes/ipprefixv6-2701-cn1",    "ipPrefixRules": [      {        "action": "Permit",        "condition": "GreaterThanOrEqualTo",        "networkPrefix": "fda0:d59c:da12:20::/64",        "sequenceNumber": 10,        "subnetMaskLength": 68 \n    }    ],    "location": "eastus",    "name": "ipprefixv6-2701-cn1",    "provisioningState": "Succeeded",    "resourceGroup": "ResourceGroupName",    "systemData": {      "createdAt": "2023-XX-XXT09:34:19.095543+00:00",      "createdBy": "user@address.com",      "createdByType": "User",      "lastModifiedAt": "2023-XX-XXT09:34:19.095543+00:00",      "lastModifiedBy": "user@address.com",      "lastModifiedByType": "User"    },    "tags": null,    "type": "microsoft.managednetworkfabric/ipprefixes"  }"}
{"text": "IP Community\nIP community resource allows operators to manipulate routes based on Community values tagged to routes. This community resource enables operators to specify conditions and actions for adding/removing routes as they're propagated up- stream/down-stream or tag them with specific community values. The operator must create an ARM resource of the type IP-Community. The operator specifies conditions and actions for adding/removing routes as they're propagated up-stream/down-stream or tags them with specific community values."}
{"text": "Parameters for IP community\nParameter Description Example Required\nresource-group Use an appropriate resource group ResourceGroupName True name specifically for your IP prefix\nresource-name Resource Name of the IP-Prefix ipprefixv4-1204-cn1 True\nlocation AzON Azure Region used during eastus True NFC Creation\naction Action to be taken for the IP Deny or Permit True community  Permit\nParameter Description Example Required\nwellKnownCommunities Supported well known community LocalAS True list.\n Internet\n - Advertise routes to internet community. \nLocalAS\n - Advertise routes to only localAS peers. \nNoAdvertise\n - Don't advertise routes to any peer.\nNoExport\n - Don't export to next AS. \nGShut\n - Graceful Shutdown (GSHUT) withdraw routes before terminating BGP connection\ncommunityMembers List the communityMembers of the 65535:65535 True IP community. The expected formats are "AA:nn" >> example "65535:65535", <integer32> >> example 4294967040. The possible values of "AA:nn" is 0-65535, and of <integer32> 1-4294967040.\n  Note\nEither \nwellKnownCommunities\n or \ncommunityMembers\n parameter has to be passed for\ncreating an IP community resource."}
{"text": "Create IP community\nThis command creates an IP community resource:\nAzure CLI\naz nf ipcommunity create  \  --resource-group   "ResourceGroupName"  \  --resource-name   "ipcommunity-2701"  \  --location   "eastus"  \  --action   "Permit"  \  --well-known-communities   "Internet"   "LocalAS"   "GShut"  \  --community-members   "65500:12701"  \nExpected output:\nOutput\n{    "action": "Permit", \n  "annotation": null,    "communityMembers": [      "65500:12701"    ],    "id": "/subscriptions/9531faa8-8c39-4165-b033- 48697fe943db/resourceGroups/ResourceGroupName/providers/Microsoft.ManagedNet workFabric/ipCommunities/ipcommunity-2701",    "location": "eastus",    "name": "ipcommunity-2701",    "provisioningState": "Succeeded",    "resourceGroup": "ResourceGroupName",    "systemData": {      "createdAt": "2023-XX-XXT09:48:15.472935+00:00",      "createdBy": "user@address.com",      "createdByType": "User",      "lastModifiedAt": "2023-XX-XXT09:48:15.472935+00:00",      "lastModifiedBy": "user@address.com",      "lastModifiedByType": "User"    },    "tags": null,    "type": "microsoft.managednetworkfabric/ipcommunities",    "wellKnownCommunities": [      "Internet",      "LocalAS",      "GShut"    ]  }"}
{"text": "Show IP community\nThis command displays an IP community resource:\nAzure CLI\naz nf ipcommunity show  --resource-group   "ResourceGroupName"   --resource-name   "ipcommunity-2701"  \nExpected output:\nOutput\n{    "action": "Permit",    "annotation": null,    "communityMembers": [      "65500:12701"    ],    "id": "/subscriptions/9531faa8-8c39-4165-b033- 48697fe943db/resourceGroups/ResourceGroupName/providers/Microsoft.ManagedNet\nworkFabric/ipCommunities/ipcommunity-2701",    "location": "eastus",    "name": "ipcommunity-2701",    "provisioningState": "Succeeded",    "resourceGroup": "ResourceGroupName",    "systemData": {      "createdAt": "2023-XX-XXT09:48:15.472935+00:00",      "createdBy": "user@address.com",      "createdByType": "User",      "lastModifiedAt": "2023-XX-XXT09:48:15.472935+00:00",      "lastModifiedBy": "user@address.com",      "lastModifiedByType": "User"    },    "tags": null,    "type": "microsoft.managednetworkfabric/ipcommunities",    "wellKnownCommunities": [      "Internet",      "LocalAS",      "GShut"    ]  }"}
{"text": "IP extended community\nThe \nIPExtendedCommunity\nresource allows operators to manipulate routes based on route\ntargets. Operators use it to specify conditions and actions for adding/removing routes as they're propagated up-stream/down-stream or tag them with specific extended community values. The operator must create an ARM resource of the type\nI\nPExtendedCommunityList` by providing a list of community values and specific\nproperties. ExtendedCommunityLists are used in specifying match conditions and the action properties for route policies."}
{"text": "Parameters for IP extended community\nParameter Description Example Required\nresource- Use an appropriate resource group name ResourceGroupName True group specifically for your IP prefix\nresource- Resource Name of the ipPrefix ipprefixv4-1204-cn1 True name\nlocation AzON Azure Region used during NFC Creation eastus True\naction Action to be taken for the IP extended Deny or Permit True community  Permit\nParameter Description Example Required\nrouteTargets Route Target List. The expected formats are "1234:5678" True "ASN(plain):nn" >> example "4294967294:50", "ASN.ASN:nn" >> example "65533.65333:40", "IP-address:nn" >> example "10.10.10.10:65535". The possible values of "nn" are within "0-65535" range, and "ASN(plain)" within "0-4294967295" range."}
{"text": "Create IP extended community\nThis command creates an IP extended community resource:\nAzure CLI\naz nf ipextendedcommunity create  \  --resource-group   "ResourceGroupName"  \  --resource-name   "ipextcommunity-2701"  \  --location   "eastus"   \  --action   "Permit"  \  --route-targets   "65046:45678"  \nExpected output:\nOutput\n{    "action": "Permit",    "annotation": null,    "id": "/subscriptions/9531faa8-8c39-4165-b033- 48697fe943db/resourceGroups/ResourceGroupName/providers/Microsoft.ManagedNet workFabric/ipExtendedCommunities/ipextcommunity-2701",    "location": "eastus",    "name": "ipextcommunity-2701",    "provisioningState": "Succeeded",    "resourceGroup": "ResourceGroupName",    "routeTargets": [      "65046:45678"    ],    "systemData": {      "createdAt": "2023-XX-XXT09:52:30.385929+00:00",      "createdBy": "user@address.com",      "createdByType": "User",      "lastModifiedAt": "2023-XX-XXT09:52:30.385929+00:00",      "lastModifiedBy": "user@address.com",      "lastModifiedByType": "User"    },    "tags": null, \n  "type": "microsoft.managednetworkfabric/ipextendedcommunities"  }"}
{"text": "Show IP extended community\nThis command displays an IP extended community resource:\nAzure CLI\naz nf ipextendedcommunity show  --resource-group   "ResourceGroupName"   -- resource-name   "ipextcommunity-2701"  \nExpected output:\nOutput\n{    "action": "Permit",    "annotation": null,    "id": "/subscriptions/9531faa8-8c39-4165-b033- 48697fe943db/resourceGroups/ResourceGroupName/providers/Microsoft.ManagedNet workFabric/ipExtendedCommunities/ipextcommunity-2701",    "location": "eastus",    "name": "ipextcommunity-2701",    "provisioningState": "Succeeded",    "resourceGroup": "ResourceGroupName",    "routeTargets": [      "65046:45678"    ],    "systemData": {      "createdAt": "2023-XX-XXT09:52:30.385929+00:00",      "createdBy": "user@address.com",      "createdByType": "User",      "lastModifiedAt": "2023-XX-XXT09:52:30.385929+00:00",      "lastModifiedBy": "user@address.com",      "lastModifiedByType": "User"    },    "tags": null,    "type": "microsoft.managednetworkfabric/ipextendedcommunities"  }"}
{"text": "Route policy\nRoute policy resource enables an operator to specify conditions and actions based on IP prefixes, IP community list and IP extended community lists. Each route policy consists of multiple statements. Each statement consists of a sequence number, conditions, and\nactions. The conditions can be combinations of IP prefixes, IP communities, and IP extended communities and are applied in ascending order of sequence numbers. The action corresponding to the first matched condition is executed. If the conditions that matched has deny as action, the route is discarded and no further processing takes place. If the action in the Route policy corresponding to the matched condition is "Permit", the following combinations of actions are allowed:\nUpdating local preference Add/delete or Set of IpCommunityLists Add/delete or Set of IpExtendedCommunityLists"}
{"text": "Parameters for Route policy\nParameter Description Example Required\nresource-group Use an appropriate resource ResourceGroupName True group name specifically for your IP prefix\nresource-name Resource Name of the IP-Prefix ipprefixv4-1204-cn1 True\nlocation AzON Azure Region used during eastus True NFC Creation\nstatements List of one or more route Policy True statements\nsequenceNumber Sequence in which route policy 1 True statements are processed. Statements are evaluated starting with the lowest sequence number and continue down the list until a match condition is met. Once a match is made, the action is applied and the rest of the list is ignored\nParameter Description Example Required\ncondition Route policy condition 1234:5678 True properties. That contains a list of IP community ARM IDs or ipExtendedCommmunicty ARM IDs or ipPrefix ARM ID. One of the three(ipCommunityIds, ipCommunityIds, ipPrefixId) is required in a condition. If more than one is specified, the condition is matched if any one of the resources has a match.\nipCommunityIds List of IP community resource False IDs\nipExtendedCommunityIds List of IPExtendedCommunity False resource IDs\nipPrefixId Arm Resource ID of IpPrefix False\naction Route policy action properties. Permit True This property describes the action to be performed if there's a match of the condition in the statement. At least one of localPreference, ipCommunityProperties, or ipExtendedCommunityProperties needs to be enabled\nlocalPreference Local preference to be set as 10 False part of action\nipCommunityProperties Details of IP communities that False need to be added, removed, or set as part of action\nadd Applicable when the action is to add IP communities or IP extended communities\ndelete Applicable when the action is to delete IP communities or IP extended communities\nset Applicable when the action is to set IP communities or IP extended communities\nParameter Description Example Required\nipCommunityIds IP community ARM resource Ids that need to be added or deleted or set\nipExtendedCommunityProperties Details of IP Extended communities that need to be added, removed, or set as part of action\nipExtendedCommunityIDs IP extended community ARM resource Ids that need to be added or deleted or set"}
{"text": "Create route policy\nThis command creates route policies:\nAzure CLI\naz nf routepolicy create  \  --resource-group   "ResourceGroupName"   \  --resource-name   "rcf-Fab3-l3domain-v6-connsubnet-ext-policy"  \  --location   "eastus"  \  --statements   '[ \{"sequenceNumber": 10, "condition":{"ipPrefixId":  "/subscriptions/<subscription- id>/resourceGroups/ResourceGroupName/providers/Microsoft.ManagedNetworkFabri c/ipPrefixes/ipprefixv6-2701-staticsubnet"}, \   "action": {"actionType": "Permit", "ipCommunityProperties": {"set": \     {"ipCommunityIds": ["/subscriptions/<subscription- id>/resourceGroups/ResourceGroupName/providers/Microsoft.ManagedNetworkFabri c/ipCommunities/ipcommunity-2701-staticsubnet"]}}}}, \   {"sequenceNumber": 30, "condition":{"ipPrefixId":  "/subscriptions/<subscription- id>/resourceGroups/ResourceGroupName/providers/Microsoft.ManagedNetworkFabri c/ipPrefixes/ipprefixv6-2701-connsubnet"},  \   "action": {"actionType": "Permit", "ipCommunityProperties": {"set":  \   {"ipCommunityIds": ["/subscriptions/<subscription- id>/resourceGroups/ResourceGroupName/providers/Microsoft.ManagedNetworkFabri c/ipCommunities/ipcommunity-connsubnet-2701"]}}}},\  ]'   \nExpected output:\nOutput\n{    "annotation": null, \n  "id": "/subscriptions/9531faa8-8c39-4165-b033- 48697fe943db/resourceGroups/ResourceGroupName/providers/Microsoft.ManagedNet workFabric/routePolicies/rcf-Fab3-l3domain-v6-connsubnet-ext-policy",    "location": "eastus",    "name": "rcf-Fab3-l3domain-v6-connsubnet-ext-policy",    "provisioningState": "Accepted",    "resourceGroup": "ResourceGroupName",    "statements": [      {        "action": {          "actionType": "Permit",          "ipCommunityProperties": {            "add": null,            "delete": null,            "set": {              "ipCommunityIds": [                "/subscriptions/<subscription- id>/resourceGroups/ResourceGroupName/providers/Microsoft.ManagedNetworkFabri c/ipCommunities/ipcommunity-2701-staticsubnet"              ]            }          },          "ipExtendedCommunityProperties": null,          "localPreference": null        },        "annotation": null,        "condition": {          "ipCommunityIds": null,          "ipExtendedCommunityIds": null,          "ipPrefixId": "/subscriptions/<subscription- id>/resourceGroups/ResourceGroupName/providers/Microsoft.ManagedNetworkFabri c/ipPrefixes/ipprefixv6-2701-staticsubnet"        },        "sequenceNumber": 10      },      {        "action": {          "actionType": "Permit",          "ipCommunityProperties": {            "add": null,            "delete": null,            "set": {              "ipCommunityIds": [                "/subscriptions/<subscription- id>/resourceGroups/ResourceGroupName/providers/Microsoft.ManagedNetworkFabri c/ipCommunities/ipcommunity-connsubnet-2701"              ]            }          },          "ipExtendedCommunityProperties": null,          "localPreference": null        },        "annotation": null,        "condition": {          "ipCommunityIds": null, \n        "ipExtendedCommunityIds": null,          "ipPrefixId": "/subscriptions/<subscription- id>/resourceGroups/ResourceGroupName/providers/Microsoft.ManagedNetworkFabri c/ipPrefixes/ipprefixv6-2701-connsubnet"        },        "sequenceNumber": 30      }    ],    "systemData": {      "createdAt": "2023-XX-XXT10:10:21.123560+00:00",      "createdBy": "user@address.com",      "createdByType": "User",      "lastModifiedAt": "2023-XX-XXT10:10:21.123560+00:00",      "lastModifiedBy": "user@address.com",      "lastModifiedByType": "User"    },    "tags": null,    "type": "microsoft.managednetworkfabric/routepolicies"  }"}
{"text": "Show route policy\nThis command displays route policies:\nAzurecli\naz nf routepolicy show  --resource-group   "ResourceGroupName"   --resource-name   "rcf-Fab3-l3domain-v6-connsubnet-ext-policy"  \nExpected output:\nOutput\n{    "annotation": null,    "id": "/subscriptions/9531faa8-8c39-4165-b033- 48697fe943db/resourceGroups/ResourceGroupName/providers/Microsoft.ManagedNet workFabric/routePolicies/rcf-Fab3-l3domain-v6-connsubnet-ext-policy",    "location": "eastus",    "name": "rcf-Fab3-l3domain-v6-connsubnet-ext-policy",    "provisioningState": "Succeeded",    "resourceGroup": "ResourceGroupName",    "statements": [      {        "action": {          "actionType": "Permit",          "ipCommunityProperties": {            "add": null,            "delete": null,            "set": { \n            "ipCommunityIds": [                "/subscriptions/<subscription- id>/resourceGroups/ResourceGroupName/providers/Microsoft.ManagedNetworkFabri c/ipCommunities/ipcommunity-2701-staticsubnet"              ]            }          },          "ipExtendedCommunityProperties": null,          "localPreference": null        },        "annotation": null,        "condition": {          "ipCommunityIds": null,          "ipExtendedCommunityIds": null,          "ipPrefixId": "/subscriptions/<subscription- id>/resourceGroups/ResourceGroupName/providers/Microsoft.ManagedNetworkFabri c/ipPrefixes/ipprefixv6-2701-staticsubnet"        },        "sequenceNumber": 10      },      {        "action": {          "actionType": "Permit",          "ipCommunityProperties": {            "add": null,            "delete": null,            "set": {              "ipCommunityIds": [                "/subscriptions/<subscription- id>/resourceGroups/ResourceGroupName/providers/Microsoft.ManagedNetworkFabri c/ipCommunities/ipcommunity-connsubnet-2701"              ]            }          },          "ipExtendedCommunityProperties": null,          "localPreference": null        },        "annotation": null,        "condition": {          "ipCommunityIds": null,          "ipExtendedCommunityIds": null,          "ipPrefixId": "/subscriptions/<subscription- id>/resourceGroups/ResourceGroupName/providers/Microsoft.ManagedNetworkFabri c/ipPrefixes/ipprefixv6-2701-connsubnet"        },        "sequenceNumber": 30      }    ],    "systemData": {      "createdAt": "2023-XX-XXT10:10:21.123560+00:00",      "createdBy": "user@address.com",      "createdByType": "User",      "lastModifiedAt": "2023-XX-XXT10:10:21.123560+00:00",      "lastModifiedBy": "user@addresscom",      "lastModifiedByType": "User" \n  },    "tags": null,    "type": "microsoft.managednetworkfabric/routepolicies"  }"}
{"text": "Working with agent pools in Nexus"}
{"text": "Kubernetes clusters\nArticle  06/28/2023\nIn this article, you learn how to work with agent pools in a Nexus Kubernetes cluster. Agent pools serve as groups of nodes with the same configuration and play a key role in managing your applications.\nNexus Kubernetes clusters offer two types of agent pools.\nSystem agent pools are designed for hosting critical system pods like CoreDNS and metrics-server. User agent pools are designed for hosting your application pods.\nApplication pods can be scheduled on system node pools if you wish to only have one pool in your Kubernetes cluster. Nexus Kubernetes cluster must contain at least one system node pool with at least one node."}
{"text": "Prerequisites\nBefore proceeding with this how-to guide, it's recommended that you:\nRefer to the Nexus Kubernetes cluster  quickStart guide  for a comprehensive overview and steps involved. Ensure that you meet the outlined prerequisites to ensure smooth implementation of the guide."}
{"text": "Limitations\nYou can delete system node pools, provided you have another system node pool to take its place in the Nexus Kubernetes cluster. System pools must contain at least one node. You can't change the VM size of a node pool after you create it. System node pools require a VM SKU of at least 2 vCPUs and 4-GB memory. A minimum of two nodes 4 vCPUs is recommended (for example, NC_G4_v1), especially for large clusters. Each Nexus Kubernetes cluster requires at least one system node pool."}
{"text": "System pool\nFor a system node pool, Nexus Kubernetes automatically assigns the label\nkubernetes.azure.com/mode: system\n to its nodes. This label causes Nexus Kubernetes to\nprefer scheduling system pods on node pools that contain this label. This label doesn't prevent you from scheduling application pods on system node pools. However, we recommend you isolate critical system pods from your application pods to prevent misconfigured or rogue application pods from accidentally killing system pods.\nYou can enforce this behavior by creating a dedicated system node pool. Use the\nCriticalAddonsOnly=true:NoSchedule\n taint to prevent application pods from being\nscheduled on system node pools.\n  Important\nIf you run a single system node pool for your Nexus Kubernetes cluster in a production environment, we recommend you use at least three nodes for the node pool."}
{"text": "User pool\nThe user pool, on the other hand, is designed for your applications. This dedicated space allows you to run your applications separately from the system workloads. If you wish to ensure that your application PODs runs exclusively on the user pool, you can schedule your application PODs here."}
{"text": "Next steps\nChoosing how to utilize your system pool and user pool depends largely on your specific requirements and use case. Both dedicated and shared methods offer unique advantages. Dedicated pools can isolate workloads and provide guaranteed resources, while shared pools can optimize resource usage across the cluster.\nAlways consider your cluster's resource capacity, the nature of your workloads, and the required level of resiliency when making your decision. By managing and understanding these node pools effectively, you can optimize your Nexus Kubernetes cluster to best fit your operational needs.\nRefer to the  quickStart guide  to add new agent pools and experiment with configurations in your Nexus Kubernetes cluster."}
{"text": "Create a Nexus Kubernetes cluster with"}
{"text": "a customized huge-page configuration\nArticle  06/28/2023\nIn this article, you learn how to enable huge-page settings during the creation of your Nexus Kubernetes cluster. Enabling huge pages allows for larger memory allocations, reducing memory fragmentation and improving overall memory utilization.\nThis configuration is especially advantageous for data plane applications, as it enables the applications to efficiently handle larger datasets and perform memory-intensive operations. As a result, you can experience improved performance and optimize resource utilization for your data plane workloads.\n  Note\nHuge-page configuration does not apply to Kubernetes control plane nodes."}
{"text": "Prerequisites\nBefore proceeding with this how-to guide, it's recommended that you:\nRefer to the Nexus Kubernetes cluster  quickStart guide  for a comprehensive overview and steps involved. Ensure that you meet the outlined prerequisites to ensure smooth implementation of the guide."}
{"text": "Huge-page settings in Nexus Kubernetes"}
{"text": "cluster\nWhen configuring huge-pages for a Nexus Kubernetes cluster, you need to provide the following arguments:\nHugepageSize: Choose a huge-page size of either \n2M\n or \n1G\n.\nHugepageCount: Specify the number of huge-pages you want to allocate."}
{"text": "Limitations\nNexus Kubernetes cluster enforces the following constraints to ensure proper configuration:\nThe total size of huge-pages (HugepageSize multiplied by HugepageCount) must not exceed 80% of the VM's memory. At least 2 GB of memory must be left for the host kernel after allocating huge- pages. If the huge-page size is \n2M\n, the huge-page count must be a power of 2 (for\nexample, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192, etc.). Huge-page configurations must be done during agent pool creation, it can't be added or modified after."}
{"text": "Example: Create a Nexus Kubernetes cluster"}
{"text": "with huge-page settings\nRefer to the Nexus Kubernetes cluster  quickStart guide  for detailed instructions on creating your cluster. Additionally, remember to include the huge-page configurations in the \nkubernetes-deploy-parameters.json\n file that you created during the quickstart\nprocess. This configuration enables huge pages in your agent pool.\nJSON\n{     "$schema" :  "https://schema.management.azure.com/schemas/2019-04- 01/deploymentParameters.json#" ,     "contentVersion" :  "1.0.0.0" ,     "parameters" : {       "kubernetesClusterName" :{         "value" :  "hugepage-test"       },       "adminGroupObjectIds" : {         "value" : [           "00000000-0000-0000-0000-000000000000"         ]      },       "cniNetworkId" : {         "value" :  "/subscriptions/<subscription_id>/resourceGroups/<resource_group>/providers/ Microsoft.NetworkCloud/l3Networks/<l3Network-name>"       },       "cloudServicesNetworkId" : {         "value" :  "/subscriptions/<subscription_id>/resourceGroups/<resource_group>/providers/ Microsoft.NetworkCloud/cloudServicesNetworks/<csn-name>"       },       "extendedLocation" : {         "value" : \n"/subscriptions/<subscription_id>/resourceGroups/<managed_resource_group>/pr oviders/microsoft.extendedlocation/customlocations/<custom-location-name>"       },       "location" : {         "value" :  "eastus"       },       "sshPublicKey" : {         "value" :  "ssh-rsa AAAAB...."       },       "initialPoolAgentOptions" : {         "value" : { "hugepagesCount" : 512, "hugepagesSize" :  "2M" }      }    }  }"}
{"text": "Example: Add an agent pool to Nexus"}
{"text": "Kubernetes cluster with huge-page settings.\nRefer to the Nexus Kubernetes cluster  quickStart guide  for instructions on adding an agent pool to your cluster. Additionally, remember to include the huge-page configurations in the \nkubernetes-nodepool-parameters.json\n file that you created during\nthe quickstart process. This configuration enables huge pages in your agent pool.\nJSON\n{       "$schema" :  "https://schema.management.azure.com/schemas/2019-04- 01/deploymentParameters.json#" ,       "contentVersion" :  "1.0.0.0" ,       "parameters" : {         "kubernetesClusterName" :{           "value" :  "hugepage-test"         },         "extendedLocation" : {           "value" :  "/subscriptions/<subscription_id>/resourceGroups/<resource_group>/providers/ microsoft.extendedlocation/customlocations/<custom-location-name>"         },         "agentOptions" : {          "value" : { "hugepagesCount" : 512, "hugepagesSize" :  "2M" }        }      }  }"}
{"text": "Next steps\nRefer to the  quickStart guide  to add new agent pools and experiment with configurations in your Nexus Kubernetes cluster."}
{"text": "Configure service load-balancer in"}
{"text": "Azure Operator Nexus Kubernetes"}
{"text": "service\nArticle  06/28/2023\nIn this article, you learn how to configure a service load balancer in a Nexus Kubernetes cluster. The load balancer allows external services to access the services running within the cluster. The focus of this guide is on the configuration aspects, providing examples to help you understand the process. By following this guide, you're able to effectively configure service load balancers in your Nexus Kubernetes cluster."}
{"text": "Prerequisites\nBefore proceeding with this how-to guide, it's recommended that you:\nRefer to the Nexus Kubernetes cluster  quickStart guide  for a comprehensive overview and steps involved. Ensure that you meet the outlined prerequisites to ensure smooth implementation of the guide. Knowledge of Kubernetes concepts, including deployments and services. Contact your network administrator to acquire an IP address range that can be used for the load-balancer IP pool."}
{"text": "Limitations\nIP pool configuration is immutable: Once set, it can't be modified in a Nexus Kubernetes cluster. IP pool names must start with a lowercase letter or a digit and end with a lowercase letter or digit. IP pool names shouldn't exceed 63 characters to avoid potential issues or restrictions. IP address pools shouldn't overlap with existing POD CIDR, Service CIDR, or CNI prefix to prevent conflicts and networking problems within the cluster.\n  Important\nThese instructions are for creating a new Nexus Kubernetes cluster. Avoid applying the Bicep template to an existing cluster, as IP pool configuration is immutable.\nOnce a cluster is created with the IP pool configuration, it cannot be modified."}
{"text": "Configuration options\nBefore configuring the IP address pool for the service load-balancer, it's important to understand the various configuration options available. These options allow you to define the behavior and parameters of the IP address pool according to your specific requirements.\nLet's explore the configuration options for the IP address pool."}
{"text": "Required parameters\nThe IP address pool configuration requires the presence of two fields: \naddresses\n and\nname\n. These fields are essential for defining the IP address range and identifying the\npool.\nThe \naddresses\n field specifies the list of IP address ranges that can be used for\nallocation within the pool. You can define each range as a subnet in CIDR format or as an explicit start-end range of IP addresses. The \nname\n field serves as a unique identifier for the IP address pool. It helps\nassociate the pool with a BGP (Border Gateway Protocol) advertisement, enabling effective communication within the cluster."}
{"text": "Optional parameters\nIn addition to the required fields, there are also optional fields available for further customization of the IP address pool configuration.\nThe \nautoAssign\n field determines whether IP addresses are automatically assigned\nfrom the pool. This field is a \nstring\n type with a default value of \nTrue\n. You can set\nit to either \nTrue\n or \nFalse\n based on your preference.\nThe \nonlyUseHostIps\n field controls the use of IP addresses ending with \n.0\n and\n.255\n within the pool. Enabling this option restricts the usage to IP addresses\nbetween \n.1\n and \n.254\n (inclusive), excluding the reserved network and broadcast\naddresses."}
{"text": "Bicep template parameters for IP address pool"}
{"text": "configuration\nThe following JSON snippet shows the parameters required for configuring the IP address pool in the Bicep template.\nJSON\n"ipAddressPools" : {     "value" : [      {         "addresses" : [ "<IP>/<CIDR>" ],         "name" :  "<pool-name>" ,         "autoAssign" :  "True" ,  /*  "True" / "False"  */         "onlyUseHostIps" :  "True"   /*  "True" / "False"  */      }    ]  } \nTo add the IP pool configuration to your cluster, you need to update the \nkubernetes-\ndeploy-parameters.json\n file that you created during the  quickStart . Include the IP pool\nconfiguration in this file according to your desired settings.\nAfter adding the IP pool configuration to your parameter file, you can proceed with deploying the Bicep template. This action sets up your new cluster with the specified IP address pool configuration, allowing you to utilize the IP pool as intended.\nBy following these instructions, you can create a new Nexus Kubernetes cluster with the desired IP pool configuration and take advantage of the IP address pool for your cluster services."}
{"text": "Example parameters\nThis parameter file is intended to be used with the  quickStart guide  Bicep template for creating a cluster with BGP load balancer enabled. It contains the necessary configuration settings to set up the cluster with BGP load balancer functionality. By using this parameter file with the Bicep template, you can create a cluster with the desired BGP load balancer capabilities.\nJSON\n{     "$schema" :  "https://schema.management.azure.com/schemas/2019-04- 01/deploymentParameters.json#" ,     "contentVersion" :  "1.0.0.0" , \n   "parameters" : {       "kubernetesClusterName" :{         "value" :  "lb-test-cluster"       },       "adminGroupObjectIds" : {         "value" : [           "00000000-0000-0000-0000-000000000000"         ]      },       "cniNetworkId" : {         "value" :  "/subscriptions/<subscription_id>/resourceGroups/<resource_group>/providers/ Microsoft.NetworkCloud/l3Networks/<l3Network-name>"       },       "cloudServicesNetworkId" : {         "value" :  "/subscriptions/<subscription_id>/resourceGroups/<resource_group>/providers/ Microsoft.NetworkCloud/cloudServicesNetworks/<csn-name>"       },       "extendedLocation" : {         "value" :  "/subscriptions/<subscription_id>/resourceGroups/<managed_resource_group>/pr oviders/microsoft.extendedlocation/customlocations/<custom-location-name>"       },       "location" : {         "value" :  "eastus"       },       "sshPublicKey" : {         "value" :  "ssh-rsa AAAAB...."       },       "ipAddressPools" : {         "value" : [          {             "addresses" : [ "<IP>/<CIDR>" ],             "name" :  "<pool-name>" ,             "autoAssign" :  "True" ,             "onlyUseHostIps" :  "True"           }        ]      }    }  } \n  Note\nIf you intend to create a DualStack service, ensure that the address pool contains both an IPv4 CIDR and an IPv6 CIDR. This allows for simultaneous support of both IPv4 and IPv6 addresses in your load balancer configuration."}
{"text": "Example: Static IP address allocation for a"}
{"text": "service\nTo allocate a static IP address for a service, you can use the following commands."}
{"text": "Create a deployment\nBash\nkubectl create deployment nginx --image=nginx --port 80"}
{"text": "Static IP allocation (LoadBalancerIP)\nBash\nkubectl expose deployment nginx \     --name nginx-loadbalancer-pool1-static \      -- type  LoadBalancer \      --load-balancer-ip <IP from pool-1> \nReplace \n<IP from pool-1>\n with the desired IP address from the IP pool."}
{"text": "Static IP allocation (ExternalIP)\nBash\nkubectl expose deployment nginx \     --name nginx-clusterip-pool1-static \      -- type  ClusterIP \      --external-ip <IP from pool-1> \nReplace \n<IP from pool-1>\n with the desired IP address from the IP pool."}
{"text": "Example: IP address allocation for a service"}
{"text": "from specific IP pool\nTo allocate an IP address for a service from a specific IP pool, you can use the following command.\nBash\nkubectl expose deployment nginx \     --name nginx-loadbalancer-pool2-auto \      -- type  LoadBalancer \      --overrides  '{"metadata":{"annotations":{"metallb.universe.tf/address- pool":"pool-2"}}}'  \nThis command assigns an IP address to the service from the IP pool \npool-2\n. Adjust the\npool name as needed. Before trying out these examples, ensure that you have already created a Nexus Kubernetes cluster with two different IP address pools. If you haven't done so, follow the necessary steps to create the cluster and configure the IP pools accordingly.\n  Note\nThe IP address pool name is case-sensitive. Make sure to use the correct case when specifying the pool name."}
{"text": "Next steps\nYou can try deploying a network function (NF) within your Nexus Kubernetes cluster utilizing the newly configured load balancer. This configuration allows you to test the load balancing capabilities and observe how traffic is distributed among the instances of your NF."}
{"text": "Role-based access control in Azure"}
{"text": "Operator Nexus Kubernetes clusters\nArticle  07/05/2023\nThis article provides a comprehensive guide on how to manage access to Nexus Kubernetes clusters using Azure Active Directory (Azure AD). Specifically, we're focusing on role-based access control, which allows you to grant permissions to users based on their roles or responsibilities within your organization."}
{"text": "Before you begin\n1. To begin, create an Azure AD group for your cluster administrators and assign members to it. Azure AD allows access to be granted to the group as a whole, rather than managing permissions for each user individually. 2. Use the group ID you created as the value for 'adminGroupObjectIds' when creating the Nexus Kubernetes cluster to ensure that the members of the group get permissions to manage the cluster. Refer to the  QuickStart  guide for instructions on how to create and access the Nexus Kubernetes cluster."}
{"text": "Administrator access to the cluster\nNexus creates a Kubernetes cluster role binding with the default Kubernetes role\ncluster-admin\n and the Azure AD groups you specified as \nadminGroupObjectIds\n. The\ncluster administrators have full access to the cluster and can perform all operations on the cluster. The cluster administrators can also grant access to other users by assigning them to the appropriate Azure AD group.\n  Note\nWhen you create a Nexus Kubernetes cluster, Nexus automatically creates a managed resource group dedicated to storing the cluster resources, within this group, the Arc connected cluster resource is established.\nTo access your cluster, you need to set up the cluster connect \nkubeconfig\n. After logging\ninto Azure CLI with the relevant Azure AD entity, you can obtain the \nkubeconfig\nnecessary to communicate with the cluster from anywhere, even outside the firewall that surrounds it.\n1. Set \nCLUSTER_NAME\n, \nRESOURCE_GROUP\n and \nSUBSCRIPTION_ID\n variables.\nBash\nCLUSTER_NAME= "myNexusAKSCluster"   RESOURCE_GROUP= "myResourceGroup"   SUBSCRIPTION_ID=< set  the correct subscription_id> \n2. Query managed resource group with \naz\n and store in \nMANAGED_RESOURCE_GROUP\nAzure CLI\n az account set  -s  $SUBSCRIPTION_ID   MANAGED_RESOURCE_GROUP=$(az networkcloud kubernetescluster show  -n   $CLUSTER_NAME  -g  $RESOURCE_GROUP  --output  tsv  --query   managedResourceGroupConfiguration.name) \n3. The following command starts a connectedk8s proxy that allows you to connect to the Kubernetes API server for the specified Nexus Kubernetes cluster.\nAzure CLI\naz connectedk8s proxy  -n  $CLUSTER_NAME   -g  $MANAGED_RESOURCE_GROUP & \n4. Use \nkubectl\n to send requests to the cluster:\nConsole\nkubectl get nodes \nYou should now see a response from the cluster containing the list of all nodes.\n  Note\nIf you see the error message "Failed to post access token to client proxyFailed to connect to MSI", you may need to perform an \naz login\n to re-authenticate with\nAzure."}
{"text": "Role-based access control\nAs an administrator, you can provide role-based access control to the cluster by creating a role binding with Azure AD group object ID. For users who only need 'view'\npermissions, you can accomplish the task by adding them to an Azure AD group that's tied to the 'view' role.\n1. Create an Azure AD group for users who need 'view' access, referring to the default Kubernetes role called \nview\n. This role is just an example, and if necessary, you can\ncreate custom roles and use them instead. For more information on user-facing roles in Kubernetes, you can refer to the official documentation at  Kubernetes roll- based access roles .\n2. Take note of the Azure AD group object ID generated upon creation.\n3. Use the kubectl command to create a clusterrolebinding with the 'view' role and associate it with the Azure AD group. Replace \nAZURE_AD_GROUP_OBJECT_ID\n with the\nobject ID of your Azure AD group.\nBash\nkubectl create clusterrolebinding nexus-read-only-users --clusterrole  view --group=AZURE_AD_GROUP_OBJECT_ID \nThis command creates a cluster role binding named \nnexus-read-only-users\n that\nassigns the \nview\n role to the members of the specified Azure AD group.\n4. Verify that the role binding was created successfully.\nBash\nkubectl get clusterrolebinding nexus-read-only-users \n5. Now the users in the Azure AD group have 'view' access to the cluster. They can access the cluster using \naz connectedk8s proxy\n to view the resources, but can't\nmake any changes"}
{"text": "Next steps\nYou can further fine-tune access control by creating custom roles with specific permissions. The creation of these roles involves Kubernetes native RoleBinding or ClusterRoleBinding resources. You can check the official  Kubernetes documentation for detailed guidance on creating more custom roles and role bindings as per your requirements."}
{"text": "Create image for Azure Operator Nexus"}
{"text": "virtual machine\nArticle  07/12/2023\nIn this article, you learn how to create a container image that can be used to create a virtual machine in Operator Nexus. Specifically, you learn how to add a virtual disk to the container image. Once the container image is built and pushed to an Azure container registry, it can be used to create a virtual machine in Operator Nexus."}
{"text": "Prerequisites\nBefore you begin creating a virtual machine (VM) image, ensure you have the following prerequisites in place:\nInstall the latest version of the  necessary Azure CLI extensions .\nThis article requires version 2.49.0 or later of the Azure CLI. If using Azure Cloud Shell, the latest version is already installed.\nAzure Container Registry (ACR): Set up a working Azure Container Registry to store and manage your container images. ACR provides a secure and private registry for storing Docker images used in your VM image creation process. You can create an ACR by following the official documentation at  Azure Container Registry documentation.\nDocker: Install Docker on your local machine. Docker is a platform that enables you to build, package, and distribute applications as lightweight containers. You use Docker to build and package your VM image. You can download Docker from Docker's  official website .\n  Note\naz login\n  command to authenticate with Azure, and the script will You can use the \nautomatically perform the ACR login using the provided ACR name and subscription ID. If you don't have the Azure CLI installed on your machine, you can provide your username and password for ACR login instead.\nEnsure you have an operational Azure Container Registry (ACR) and Docker installed on your machine before proceeding with the creation of a VM image. Familiarize yourself\nwith the usage and functionality of ACR and Docker, as they're essential for managing your container images and building the VM image."}
{"text": "Virtual machine image requirements\nEnsure your Virtual Network Function (VNF) image is in qcow2 format that can boot with cloud-init.\nYou need to configure the bootloader, kernel, and init system in your image to enable a text-based serial console. This configuration is required to enable console support for your virtual machine (VM). Make sure the serial port settings on your system and terminal match to establish proper communication.\nYou need to ensure your VM image supports cloud-init version 2, enabling advanced configuration options during the VM initialization process.\nYou need to ensure that your VM image includes cloud-init with the \nnocloud\ndatasource. \nnocloud\n  datasource allows for initial configuration and customization\nduring VM provisioning.\nDisks must be placed into the \n/disk\n  directory inside the container.\nRaw and qcow2 formats are supported. Qcow2 is recommended in order to reduce the container image's size.\nContainer disks should be based on the \nscratch\n  image, which is an empty base\nimage that contains no files or directories other than the image itself. Using\nscratch\n  as the base image ensures that the container image is as small as possible\nand only includes the necessary files for the VNF."}
{"text": "Steps to create image for Operator Nexus"}
{"text": "virtual machine\nYou can create an image for your VNF by using the provided script. It generates a Dockerfile that copies the VNF disk image file into the container's \n/disk\n  directory.\n  Note\nThe following script is provided as an example. If you prefer, you can create and push the container image manually instead of following the script.\nThe following environment variables are used to configure the script for creating a virtual machine (VM) image for your VNF. Modify and export these variables with your own values before executing the script:\nBash\n# Azure subscription ID (provide if not using username-password) export  SUBSCRIPTION= "your_subscription_id"\n# (Mandatory) Azure Container Registry name export  ACR_NAME= "your_acr_name"\n# (Mandatory) Name of the container image export  CONTAINER_IMAGE_NAME= "your_container_image_name"\n# (Mandatory) Tag for the container image export  CONTAINER_IMAGE_TAG= "your_container_image_tag"\n# (Mandatory) VNF image (URL, local file, or full local path) export  VNF_IMAGE= "your_vnf_image"\n# (Optional) ACR URL (leave empty to derive from ACR_NAME) export  ACR_URL= ""\n# (Optional) ACR login username (provide if not using subscription) export  USERNAME= ""\n# (Optional) ACR login password (provide if not using subscription) export  PASSWORD= ""\nTo create a VM image for your Virtual Network Function (VNF), save the provided script as \ncreate-container-disk.sh\n , set the required environment variables, and execute the\nscript.\nBash\n#!/bin/bash\n# Define the required environment variables required_vars=(      "ACR_NAME"                    # Azure Container Registry name      "CONTAINER_IMAGE_NAME"        # Name of the container image      "CONTAINER_IMAGE_TAG"         # Tag for the container image      "VNF_IMAGE"                   # VNF image (URL or file path) )\n# Verify if required environment variables are set for  var  in   "${required_vars[@]}" ;  do      if  [ -z  "${!var}"  ];  then          echo   "Error: $var environment variable is not set."\n         exit  1      fi done\n# Check if either SUBSCRIPTION or USERNAME with PASSWORD is provided if  [ -z  "$SUBSCRIPTION"  ] && [ -z  "$USERNAME"  ] && [ -z  "$PASSWORD"  ];  then      echo   "Error: Either provide SUBSCRIPTION or USERNAME with PASSWORD."      exit  1 fi\n# Set default value for DOCKERFILE_NAME if not set if  [ -z  "$DOCKERFILE_NAME"  ];  then     DOCKERFILE_NAME= "nexus-vm-img-dockerfile" fi\n# Check if ACR_URL is already set by the user if  [ -z  "$ACR_URL"  ];  then      # Derive the ACR URL from the ACR_NAME     ACR_URL= "$ACR_NAME.azurecr.io" fi\n# Initialize variables for downloaded/copied files downloaded_files=()\n# Function to clean up downloaded files cleanup () {      for  file  in   "${downloaded_files[@]}" ;  do          if  [ -f  "$file"  ];  then             rm  "$file"          fi      done }\n# Register the cleanup function to be called on exit trap  cleanup EXIT\n# Check if the VNF image is a URL or a local file if  [[  "$VNF_IMAGE"  == http* ]];  then      # Use curl to download the file     filename=$(basename  "$VNF_IMAGE" )      # Download the VNF image file and save the output to a file     curl -f -Lo  "$filename"   "$VNF_IMAGE"      if  [ $? -ne 0 ];  then          echo   "Error: Failed to download file."          exit  1      fi      # Add the downloaded file to the list for cleanup     downloaded_files+=( "$filename" ) elif  [[  "$VNF_IMAGE"  == /* ]];  then      # Use the provided full local path     filename=$(basename  "$VNF_IMAGE" )      # Copy the VNF image file to the current directory for cleanup     cp  "$VNF_IMAGE"   "./$filename"      # Add the copied file to the list for cleanup     downloaded_files+=( "$filename" )\nelse      # Assume it's a local file in the current directory     filename= "$VNF_IMAGE" fi\n# Check if the file exists if  [ ! -f  "$filename"  ];  then      echo   "Error: File $filename does not exist."      exit  1 fi\n# Create a Dockerfile that copies the VNF image file into the container's  /disk directory # The containerDisk needs to be readable for the user with the UID 107  (qemu). cat <<EOF >  "$DOCKERFILE_NAME" FROM scratch ADD --chown=107:107  "$filename"  /disk/ EOF\n# Build the Docker image and tag it to the Azure Container Registry docker build -f  "$DOCKERFILE_NAME"  -t  "$CONTAINER_IMAGE_NAME:$CONTAINER_IMAGE_TAG"  .\n# Log in to Azure Container Registry if  [ -n  "$USERNAME"  ] && [ -n  "$PASSWORD"  ];  then     docker login  "$ACR_NAME.azurecr.io"  -u  "$USERNAME"  -p  "$PASSWORD" else     az acr login --name  "$ACR_NAME"  --subscription  "$SUBSCRIPTION" fi\ndocker tag  "$CONTAINER_IMAGE_NAME:$CONTAINER_IMAGE_TAG"   "$ACR_URL/$CONTAINER_IMAGE_NAME:$CONTAINER_IMAGE_TAG" docker push  "$ACR_URL/$CONTAINER_IMAGE_NAME:$CONTAINER_IMAGE_TAG"\n# Remove the downloaded/copied files cleanup\nrm  "$DOCKERFILE_NAME"\necho   "VNF image $ACR_URL/$CONTAINER_IMAGE_NAME:$CONTAINER_IMAGE_TAG created  successfully!"\nAfter executing the script, you'll have a VM image tailored for your Virtual Network Function (VNF). You can use this image to deploy your VNF."}
{"text": "Example usage\n1. Set the required environment variables.\nBash\nexport  SUBSCRIPTION= "" 00000000-0000-0000-0000-000000000000 "" export  ACR_NAME= "myvnfacr" export  CONTAINER_IMAGE_NAME= "ubuntu" export  CONTAINER_IMAGE_TAG= "20.04" export  VNF_IMAGE= "https://cloud- images.ubuntu.com/releases/focal/release/ubuntu-20.04-server-cloudimg- amd64.img"\n2. Save the provided script as \ncreate-container-disk.sh\n  and make it executable.\nBash\nchmod +x create-container-disk.sh\n3. Execute the script.\nBash\n$ ./create-container-disk.sh   % Total    % Received % Xferd  Average Speed   Time    Time     Time   Current                                  Dload  Upload   Total   Spent    Left   Speed 100  622M  100  622M    0     0  24.7M      0  0:00:25  0:00:25 --:--:- - 26.5M [+] Building 36.6s (5/5) FINISHED  => [internal] load .dockerignore                                          0.1s  => => transferring context: 2B                                            0.0s  => [internal] load build definition from nexus-vm-img-dockerfile          0.1s  => => transferring dockerfile: 137B                                       0.0s  => [internal] load build context                                          36.4s  => => transferring context: 652.33MB                                      36.3s  => CACHED [1/1] ADD --chown=107:107 ubuntu-20.04-server-cloudimg- amd64.img /disk/             0.0s  => exporting to image                                                     0.0s  => => exporting layers                                                    0.0s  => => writing image  sha256:5b5f531c132cdbba202136b5ec41c9bfe9d91beeb5acee617c1ef902df4ca772    0.0s  => => naming to docker.io/library/ubuntu:20.04                            0.0s\nLogin Succeeded The push refers to repository [myvnfacr.azurecr.io/ubuntu] b86efae7de58: Layer already exists 20.04: digest:  sha256:d514547ee28d9ed252167d0943d4e711547fda95161a3728c44a275f5d9669a8  size: 529 VNF image myvnfacr.azurecr.io/ubuntu:20.04 created successfully!"}
{"text": "Next steps\nRefer to the  QuickStart guide  to deploy a VNF using the image you created."}
{"text": "Introduction to the Virtual Machine"}
{"text": "console service\nArticle  07/18/2023\nThe Virtual Machine (VM) console service provides managed SSH access to a VM hosted in an Operator Nexus Instance. It relies on the Azure Private Link Service (PLS) to establish a private network connection between the user's network and the Azure Operator Nexus Cluster Manager's private network."}
{"text": "For more information about networking resources that enables private connectivity to an Operator Nexus Instance, see  Introduction to Azure Private Link .\nThis document provides guided instructions of how to use the VM Console service to establish an SSH session with a Virtual Machine in an Operator Nexus Instance.\nThis guide helps you to:\n1. Establish a secure private network connectivity between your network and the Cluster Manager's private network 2. Create a Console resource with \naz networkcloud virtualmachine console\n  CLI\ncommand 3. Initiate an SSH session with a Virtual Machine\n  Note\nIn order to avoid passing the \n--subscription\n  parameter to each Azure CLI\ncommand, execute the following command:\nAzure CLI\n  az account set  --subscription   "your-subscription-ID""}
{"text": "Before you begin\n1. Install the latest version of the  appropriate CLI extensions"}
{"text": "Setting variables\nTo help set up the environment for SSHing to Virtual Machines, define these environment variables that are used throughout this guide.\n  Note\nThese environment variable values do not reflect a real deployment and users MUST change them to match their environments.\nBash\n     # Cluster Manager environment variables      export  CM_CLUSTER_NAME= "contorso-cluster-manager-1234"      export  CM_MANAGED_RESOURCE_GROUP= "contorso-cluster-manager-1234-rg"      export   CM_EXTENDED_LOCATION= "/subscriptions/subscriptionId/resourceGroups/resourceG roupName/providers/Microsoft.ExtendedLocation/customLocations/clusterManager ExtendedLocationName"      export  CM_HOSTED_RESOURCES_RESOURCE_GROUP= "my-contorso-console-rg"\n     # Your Console resource enviroment variables      export  VIRTUAL_MACHINE_NAME= "my-undercloud-vm"      export  CONSOLE_PUBLIC_KEY= "xxxx-xxxx-xxxxxx-xxxx"      export  CONSOLE_EXPIRATION_TIME= "2023-06-01T01:27:03.008Z"\n     # your environment variables      export  PRIVATE_ENDPOINT_RG= "my-work-env-rg"      export  PRIVATE_ENDPOINT_NAME= "my-work-env-ple"      export  PRIVATE_ENDPOINT_REGION= "eastus"      export  PRIVATE_ENDPOINT_VNET= "my-work-env-ple-vnet"      export  PRIVATE_ENDPOINT_SUBNET= "my-work-env-ple-subnet"      export  PRIVATE_ENDPOINT_CONNECTION_NAME= "my-contorse-ple-pls-connection""}
{"text": "Establishing Private Network Connectivity\nIn order to establish a secure SSH session with a Virtual Machine, you need to establish a private network connectivity between your network and the Cluster Manager's private network.\nThis private network relies on the Azure Private Link Endpoint (PLE) and the Azure Private Link Service (PLS).\nThe Cluster Manager automatically creates a PLS so that you can establish a private network connection between your network and the Cluster Manager's private network.\nThis section provides a step-by-step guide to help you to establish a private network connectivity."}
{"text": "1. You need to retrieve the resource identifier for the PLS associated to the VM Console service running in the Cluster Manager.\nBash\n     # retrieve the infrastructure resource group of the AKS cluster      export  pls_resource_group=$(az aks show --name ${CM_CLUSTER_NAME} - g ${CM_MANAGED_RESOURCE_GROUP} --query  "nodeResourceGroup"  -o tsv)\n     # retrieve the Private Link Service resource id      export  pls_resourceid=$(az network private-link-service show \         --name console-pls \         --resource-group ${pls_resource_group} \         --query id \         --output tsv)\n2. Create the PLE for establishing a private and secure connection between your network and the Cluster Manager's private network. You need the PLS resource ID obtained in  Creating Console Resource .\nBash\n    az network private-endpoint create \         --connection-name  "${PRIVATE_ENDPOINT_CONNECTION_NAME}"  \         --name  "${PRIVATE_ENDPOINT_NAME}"  \         --private-connection-resource-id  "${pls_resourceid}"  \         --resource-group  "${PRIVATE_ENDPOINT_RG}"  \         --vnet-name  "${PRIVATE_ENDPOINT_VNET}"  \         --subnet  "${PRIVATE_ENDPOINT_SUBNET}"  \         --manual-request  false\n3. Retrieve the private IP address allocated to the PLE, which you need when establishing the \nssh\n  session.\nBash\n    ple_interface_id=$(az network private-endpoint list --resource- group ${PRIVATE_ENDPOINT_NAME}-rg --query  "[0].networkInterfaces[0].id"   -o tsv)\n    sshmux_ple_ip=$(az network nic show --ids $ple_interface_id --query  'ipConfigurations[0].privateIPAddress'  -o tsv)\n     echo   "sshmux_ple_ip: ${sshmux_ple_ip}""}
{"text": "Creating Console Resource\nThe Console resource provides the information about the VM such as VM name, public SSH key, expiration date for the SSH session, etc.\nThis section provides step-by-step guide to help you to create a Console resource using Azure CLI commands."}
{"text": "1. The first thing before you can establish an SSH session with a VM is to create a Console  resource in the Cluster Manager.\nBash\n    az networkcloud virtualmachine console create \         --virtual-machine-name  "${VIRTUAL_MACHINE_NAME}"  \         --resource-group  "${CM_HOSTED_RESOURCES_RESOURCE_GROUP}"  \         --extended-location name= "${CM_EXTENDED_LOCATION}"   type = "CustomLocation"  \         --enabled True \         --key-data  "${CONSOLE_PUBLIC_KEY}"  \        [--expiration  "${CONSOLE_EXPIRATION_TIME}" ]\nIf you omit the \n--expiration\n  parameter, the Cluster Manager will automatically set\nthe expiration to one day after the creation of the Console resource. Also note that the  must  comply with RFC3339 otherwise the\nexpiration\n  date & time format \ncreation of the Console resource fails.\n  Note\nFor a complete synopsis for this command, invoke \naz networkcloud console\ncreate --help\n .\n2. Upon successful creation of the Console resource, retrieve the VM Access ID. You must use this unique identifier as \nuser\n  of the \nssh\n  session.\nBash\n    virtual_machine_access_id=$(az networkcloud virtualmachine console  show \         --virtual-machine-name  "${VIRTUAL_MACHINE_NAME}"  \         --resource-group  "${CM_HOSTED_RESOURCES_RESOURCE_GROUP}"  \         --query  "virtualMachineAccessId" )\n  Note\naz networkcloud virtualmachine\nFor a complete synopsis for this command, invoke \nconsole show --help\n ."}
{"text": "Establishing an SSH session with Virtual"}
{"text": "Machine\nssh\n  session to the VM, that At this point, you have all the info needed for establishing a \nis, \nvirtual_machine_access_id\n  and \nsshmux_ple_ip\n .\nThe VM Console service is a \nssh\n  server that "relays" the session to the designated VM.\nsshmux_ple_ip\n  indirectly references the VM Console service and the The \nvirtual_machine_access_id\n  the identifier for the VM.\n  Important\nThe VM Console service listens to port  must  specify this port\n2222\n , therefore you \nnumber in the \nssh\n  command.\nBash\n   SSH [-i path-to-private-SSH-key] -p 2222  $virtual_machine_access_id@$sshmux_ple_ip"}
{"text": "ssh\n  session per Virtual The VM Console service was designed to allow  only  one \nMachine. Anyone establishing a successful \nssh\n  session to a VM closes an existing\nsession, if any.\n  Important\nThe private SSH key used for authenticating the \nssh\n  session (default:\n$HOME/.ssh/id_rsa\n ) MUST match the public SSH key passed as parameter when\ncreating the Console resource."}
{"text": "Updating Console Resource\nYou can disable \nssh\n  session to a given VM by updating the expiration date/time and/or\nupdate the public SSH key.\nBash\n    az networkcloud virtualmachine console update \         --virtual-machine-name  "${VIRTUAL_MACHINE_NAME}"  \         --resource-group  "${CM_HOSTED_RESOURCES_RESOURCE_GROUP}"  \         [--enabled True | False] \         [--key-data  "${CONSOLE_PUBLIC_KEY}" ] \         [--expiration  "${CONSOLE_EXPIRATION_TIME}" ]\nssh\n  access to a VM, you need to update the Console resource If you want to disable \nwith the parameter \nenabled False\n . This update closes any \nssh\n  session and restricts any\nsubsequent sessions.\n  Note\nssh\n  session to a VM, the corresponding Console Before anyone can create a \nresource  must  be set to \n--enabled True\n .\n--expiration\n  time expires, it closes any \nssh\n  session corresponding the When a Console \nConsole resource. You'll need to update the expiration time with a future value so that you can establish a new session.\nssh\nWhen you update the Console's public SSH key, the VM Console service closes any \nsession referenced by the Console resource. You have to provide a matching private SSH\nssh\n  session. key matching the new public key when you establish a"}
{"text": "Cleaning Up (Optional)\nTo clean up your VM Console environment setup, you need to delete the Console resource and your Private Link Endpoint.\n1. Deleting your Console resource\nBash\n    az networkcloud virtualmachine console delete \         --virtual-machine-name  "${VIRTUAL_MACHINE_NAME}"  \         --resource-group  "${CM_HOSTED_RESOURCES_RESOURCE_GROUP}"\n2. Deleting the Private Link Endpoint\nBash\n    az network private-endpoint delete \     --name ${PRIVATE_ENDPOINT_NAME}-ple \     --resource-group ${PRIVATE_ENDPOINT_NAME}-rg"}
{"text": "Monitor Nexus Kubernetes cluster\nArticle  07/10/2023\nEach Nexus Kubernetes cluster consists of multiple layers:\nVirtual Machines (VMs) Kubernetes layer Application pods\nFigure: Sample Nexus Kubernetes cluster\nOn an instance, Nexus Kubernetes clusters are delivered with an  optional   Container Insights  observability solution. Container Insights captures the logs and metrics from Nexus Kubernetes clusters and workloads. It's solely your discretion whether to enable this tooling or deploy your own telemetry stack.\nThe Nexus Kubernetes cluster with Azure monitoring tool looks like:\nFigure: Nexus Kubernetes cluster with Monitoring Tools"}
{"text": "Extension onboarding with CLI using managed"}
{"text": "identity auth\nDocumentation for starting with  Azure CLI , how to install it across  multiple operating systems , and how to install  CLI extensions .\nInstall latest version of the  necessary CLI extensions ."}
{"text": "Monitor Nexus Kubernetes cluster  VM layer\nThis how-to guide provides steps and utility scripts to  Arc connect  the Nexus Kubernetes cluster Virtual Machines to Azure and enable monitoring agents for the collection of System logs from these VMs using  Azure Monitoring Agent . The instructions further capture details on how to set up log data collection into a Log Analytics workspace.\nThe following resources provide you with support:\narc-connect.env\n : use this template file to create environment variables needed by\nincluded scripts\nBash\nexport  SUBSCRIPTION_ID= ""\nexport  SERVICE_PRINCIPAL_ID= "" export  SERVICE_PRINCIPAL_SECRET= "" export  RESOURCE_GROUP= "" export  TENANT_ID= "" export  LOCATION= "" export  INSTALL_AZURE_MONITOR_AGENT= "true" export  PROXY_URL= "" export  NAMESPACE= ""\ndcr.sh\n : use this script to create a Data Collection Rule (DCR) to configure syslog collection\nBash\n#!/bin/bash set  -e\nSUBSCRIPTION_ID= "${SUBSCRIPTION_ID:?SUBSCRIPTION_ID must be set}" SERVICE_PRINCIPAL_ID= "${SERVICE_PRINCIPAL_ID:?SERVICE_PRINCIPAL_ID must be  set}" SERVICE_PRINCIPAL_SECRET= "${SERVICE_PRINCIPAL_SECRET:? SERVICE_PRINCIPAL_SECRET must be set}" RESOURCE_GROUP= "${RESOURCE_GROUP:?RESOURCE_GROUP must be set}" TENANT_ID= "${TENANT_ID:?TENANT_ID must be set}" LOCATION= "${LOCATION:?LOCATION must be set}" LAW_RESOURCE_ID= "${LAW_RESOURCE_ID:?LAW_RESOURCE_ID must be set}" DCR_NAME=${DCR_NAME:-${RESOURCE_GROUP}-syslog-dcr}\naz login --service-principal -u  "${SERVICE_PRINCIPAL_ID}"  -p  "${SERVICE_PRINCIPAL_SECRET}"  -t  "${TENANT_ID}"\naz account  set  -s  "${SUBSCRIPTION_ID}"\naz extension add --name monitor-control-service\nRULEFILE=$(mktemp) tee  "${RULEFILE}"  <<EOF {    "location" :  "${LOCATION}" ,    "properties" : {      "dataSources" : {        "syslog" : [         {            "name" :  "syslog" ,            "streams" : [              "Microsoft-Syslog"           ],            "facilityNames" : [              "auth" ,              "authpriv" ,              "cron" ,              "daemon" ,\n             "mark" ,              "kern" ,              "local0" ,              "local1" ,              "local2" ,              "local3" ,              "local4" ,              "local5" ,              "local6" ,              "local7" ,              "lpr" ,              "mail" ,              "news" ,              "syslog" ,              "user" ,              "uucp"           ],            "logLevels" : [              "Info" ,              "Notice" ,              "Warning" ,              "Error" ,              "Critical" ,              "Alert" ,              "Emergency"           ]         }       ]     },      "destinations" : {        "logAnalytics" : [         {            "workspaceResourceId" :  "${LAW_RESOURCE_ID}" ,            "name" :  "centralWorkspace"         }       ]     },      "dataFlows" : [       {          "streams" : [            "Microsoft-Syslog"         ],          "destinations" : [            "centralWorkspace"         ]       }     ]   } }\nEOF\naz monitor data-collection rule create --name  "${DCR_NAME}"  --resource-group  "${RESOURCE_GROUP}"  --location  "${LOCATION}"  --rule-file  "${RULEFILE}"  -o  tsv --query id\nrm -rf  "${RULEFILE}"\nassign.sh\n : use the script to create a policy to associate the DCR with all Arc-\nenabled servers in a resource group\nBash\n#!/bin/bash set  -e\nSUBSCRIPTION_ID= "${SUBSCRIPTION_ID:?SUBSCRIPTION_ID must be set}" SERVICE_PRINCIPAL_ID= "${SERVICE_PRINCIPAL_ID:?SERVICE_PRINCIPAL_ID must be  set}" SERVICE_PRINCIPAL_SECRET= "${SERVICE_PRINCIPAL_SECRET:? SERVICE_PRINCIPAL_SECRET must be set}" RESOURCE_GROUP= "${RESOURCE_GROUP:?RESOURCE_GROUP must be set}" TENANT_ID= "${TENANT_ID:?TENANT_ID must be set}" LOCATION= "${LOCATION:?LOCATION must be set}" DCR_NAME=${DCR_NAME:-${RESOURCE_GROUP}-syslog-dcr} POLICY_NAME=${POLICY_NAME:-${DCR_NAME}-policy}\naz login --service-principal -u  "${SERVICE_PRINCIPAL_ID}"  -p  "${SERVICE_PRINCIPAL_SECRET}"  -t  "${TENANT_ID}"\naz account  set  -s  "${SUBSCRIPTION_ID}"\nDCR=$(az monitor data-collection rule show --name  "${DCR_NAME}"  --resource- group  "${RESOURCE_GROUP}"  -o tsv --query id)\nPRINCIPAL=$(az policy assignment create \   --name  "${POLICY_NAME}"  \   --display-name  "${POLICY_NAME}"  \   --resource-group  "${RESOURCE_GROUP}"  \   --location  "${LOCATION}"  \   --policy  "d5c37ce1-5f52-4523-b949-f19bf945b73a"  \   --assign-identity \   -p  "{\"dcrResourceId\":{\"value\":\"${DCR}\"}}"  \   -o tsv --query identity.principalId)\nrequired_roles=$(az policy definition show -n  "d5c37ce1-5f52-4523-b949- f19bf945b73a"  --query policyRule.then.details.roleDefinitionIds -o tsv) for  roleId  in  $( echo   "$required_roles" );  do   az role assignment create \     --role  "${roleId##*/}"  \     --assignee-object-id  "${PRINCIPAL}"  \     --assignee-principal-type  "ServicePrincipal"  \     --scope  /subscriptions/ "$SUBSCRIPTION_ID" /resourceGroups/ "$RESOURCE_GROUP" done\ninstall.sh\n : Arc-enable Nexus Kubernetes cluster VMs and install Azure\nMonitoring Agent on each VM\nBash\n#!/bin/bash set  -e\nfunction   create_secret () {   kubectl apply -f - -n  "${NAMESPACE}"  <<EOF apiVersion: v1 kind: Secret metadata:   name: naks-vm-telemetry type : Opaque stringData:   SUBSCRIPTION_ID:  "${SUBSCRIPTION_ID}"   SERVICE_PRINCIPAL_ID:  "${SERVICE_PRINCIPAL_ID}"   SERVICE_PRINCIPAL_SECRET:  "${SERVICE_PRINCIPAL_SECRET}"   RESOURCE_GROUP:  "${RESOURCE_GROUP}"   TENANT_ID:  "${TENANT_ID}"   LOCATION:  "${LOCATION}"   PROXY_URL:  "${PROXY_URL}"   INSTALL_AZURE_MONITOR_AGENT:  "${INSTALL_AZURE_MONITOR_AGENT}" EOF }\nfunction   create_daemonset () {   kubectl apply -f - -n  "${NAMESPACE}"  <<EOF apiVersion: apps/v1 kind: DaemonSet metadata:   name: naks-vm-telemetry   labels:     k8s-app: naks-vm-telemetry spec:   selector:     matchLabels:       name: naks-vm-telemetry   template:     metadata:       labels:         name: naks-vm-telemetry     spec:       hostNetwork:  true       hostPID:  true       containers:         - name: naks-vm-telemetry           image: mcr.microsoft.com/oss/mirror/docker.io/library/ubuntu:20.04           env:             - name: SUBSCRIPTION_ID               valueFrom:                 secretKeyRef:\n                  name: naks-vm-telemetry                   key: SUBSCRIPTION_ID             - name: SERVICE_PRINCIPAL_ID               valueFrom:                 secretKeyRef:                   name: naks-vm-telemetry                   key: SERVICE_PRINCIPAL_ID             - name: SERVICE_PRINCIPAL_SECRET               valueFrom:                 secretKeyRef:                   name: naks-vm-telemetry                   key: SERVICE_PRINCIPAL_SECRET             - name: RESOURCE_GROUP               valueFrom:                 secretKeyRef:                   name: naks-vm-telemetry                   key: RESOURCE_GROUP             - name: TENANT_ID               valueFrom:                 secretKeyRef:                   name: naks-vm-telemetry                   key: TENANT_ID             - name: LOCATION               valueFrom:                 secretKeyRef:                   name: naks-vm-telemetry                   key: LOCATION             - name: PROXY_URL               valueFrom:                 secretKeyRef:                   name: naks-vm-telemetry                   key: PROXY_URL             - name: INSTALL_AZURE_MONITOR_AGENT               valueFrom:                 secretKeyRef:                   name: naks-vm-telemetry                   key: INSTALL_AZURE_MONITOR_AGENT           securityContext:             privileged:  true            command :             - /bin/bash             - -c             - |                set  -e               WORKDIR=\$(nsenter -t1 -m -u -n -i mktemp -d)                trap   'nsenter -t1 -m -u -n -i rm -rf "\${WORKDIR}"; echo  "Azure Monitor Configuration Failed"'  ERR               nsenter -t1 -m -u -n -i  mkdir  -p  "\${WORKDIR}" /telemetry\n              nsenter -t1 -m -u -n -i tee  "\${WORKDIR}" /telemetry/telemetry_common.py > /dev/null <<EOF                #!/usr/bin/python3               import json               import logging               import os\n              import socket               import subprocess               import sys\n              arc_config_file =  "\${WORKDIR}/telemetry/arc-connect.json"\n              class AgentryResult:                   CONNECTED =  "Connected"                   CREATING =  "Creating"                   DISCONNECTED =  "Disconnected"                   FAILED =  "Failed"                   SUCCEEDED =  "Succeeded"\n              class OnboardingMessage:                   COMPLETED =  "Onboarding completed"                   STILL_CREATING =  "Azure still creating"                   STILL_TRYING =  "Service still trying"\n              def get_logger(logger_name):                   logger = logging.getLogger(logger_name)                   logger.setLevel(logging.DEBUG)                   handler = logging.StreamHandler(stream=sys.stdout)                   format = logging.Formatter(fmt= "%(name)s - %(levelname)s -  %(message)s" )                   handler.setFormatter(format)                   logger.addHandler(handler)                    return  logger\n              def az_cli_cm_ext_install(logger, config):                   logger.info( "Install az CLI connectedmachine extension" )                   proxy_url = config.get( "PROXY_URL" )                    if  proxy_url is not None:                       os.environ[ "HTTP_PROXY" ] = proxy_url                       os.environ[ "HTTPS_PROXY" ] = proxy_url                   run_cmd(logger,  "/usr/bin/az extension add --name  connectedmachine --yes" )\n              def get_cm_properties(logger, config):                   hostname = socket.gethostname()                   resource_group = config.get( "RESOURCE_GROUP" )\n                  logger.info(f "Getting arc enrollment properties for  {hostname}..." )\n                  az_login(logger, config)\n                  property_cmd = f '/usr/bin/az connectedmachine show -- machine-name "{hostname}" --resource-group "{resource_group}"'\n                  try:\n                      raw_property = run_cmd(logger, property_cmd)                       cm_json = json.loads(raw_property.stdout)                       provisioning_state = cm_json[ "properties" ] [ "provisioningState" ]                       status = cm_json[ "properties" ][ "status" ]                   except:                       logger.warning( "Connectedmachine not yet present" )                       provisioning_state =  "NOT_PROVISIONED"                       status =  "NOT_CONNECTED"                   finally:                       az_logout(logger)\n                  logger.info(                       f 'Connected machine "{hostname}" provisioningState is  "{provisioning_state}" and status is "{status}"'                   )\n                   return  provisioning_state, status\n              def get_cm_extension_state(logger, config, extension_name):                   resource_group = config.get( "RESOURCE_GROUP" )                   hostname = socket.gethostname()\n                  logger.info(f "Getting {extension_name} state for  {hostname}..." )\n                  az_login(logger, config)\n                  state_cmd = f '/usr/bin/az connectedmachine extension show  --name "{extension_name}" --machine-name "{hostname}" --resource-group " {resource_group}"'\n                  try:                       raw_state = run_cmd(logger, state_cmd)                       cme_json = json.loads(raw_state.stdout)                       provisioning_state = cme_json[ "properties" ] [ "provisioningState" ]                   except:                       logger.warning( "Connectedmachine extension not yet  present" )                       provisioning_state =  "NOT_PROVISIONED"                   finally:                       az_logout(logger)\n                  logger.info(                       f 'Connected machine "{hostname}" extenstion " {extension_name}" provisioningState is "{provisioning_state}"'                   )\n                   return  provisioning_state\n              def run_cmd(logger, cmd, check_result=True, echo_output=True):                   res = subprocess.run("}
{"text": "cmd,                       shell=True,                       stdout=subprocess.PIPE,                       stderr=subprocess.PIPE,                       universal_newlines=True,                   )\n                   if  res.stdout:                        if  echo_output:                           logger.info(f "[OUT] {res.stdout}" )\n                   if  res.stderr:                        if  echo_output:                           logger.info(f "[ERR] {res.stderr}" )\n                   if  check_result:                       res.check_returncode()\n                   return  res   # can parse out res.stdout and res.returncode\n              def az_login(logger, config):                   logger.info( "Login to Azure account..." )                   proxy_url = config.get( "PROXY_URL" )                    if  proxy_url is not None:                       os.environ[ "HTTP_PROXY" ] = proxy_url                       os.environ[ "HTTPS_PROXY" ] = proxy_url\n                  service_principal_id = config.get( "SERVICE_PRINCIPAL_ID" )                   service_principal_secret =  config.get( "SERVICE_PRINCIPAL_SECRET" )                   tenant_id = config.get( "TENANT_ID" )                   subscription_id = config.get( "SUBSCRIPTION_ID" )                   cmd = f '/usr/bin/az login --service-principal --username " {service_principal_id}" --password "{service_principal_secret}" --tenant " {tenant_id}"'                   run_cmd(logger, cmd)                   logger.info(f "Set Subscription...{subscription_id}" )                   set_sub = f '/usr/bin/az account set --subscription " {subscription_id}"'                   run_cmd(logger, set_sub)\n              def az_logout(logger):                   logger.info( "Logout of Azure account..." )                   run_cmd(logger,  "/usr/bin/az logout --verbose" ,  check_result=False)\n              EOF\n              nsenter -t1 -m -u -n -i tee  "\${WORKDIR}" /telemetry/setup_arc_for_servers.py > /dev/null <<EOF                #!/usr/bin/python3               import json               import time\n              import telemetry_common\n              def run_connect(logger, arc_config):                   logger.info( "Connect machine to Azure Arc..." )                   service_principal_id =  arc_config.get( "SERVICE_PRINCIPAL_ID" )                   service_principal_secret =  arc_config.get( "SERVICE_PRINCIPAL_SECRET" )                   resource_group = arc_config.get( "RESOURCE_GROUP" )                   tenant_id = arc_config.get( "TENANT_ID" )                   subscription_id = arc_config.get( "SUBSCRIPTION_ID" )                   location = arc_config.get( "LOCATION" )                   connect_cmd = f 'azcmagent connect --service-principal-id " {service_principal_id}" --service-principal-secret " {service_principal_secret}" --resource-group "{resource_group}" --tenant-id  "{tenant_id}" --subscription-id "{subscription_id}" --location "{location}"'\n                  cloudtype = arc_config.get( "CLOUDTYPE" )                    if  cloudtype is not None:                       connect_cmd += f ' --cloud "{cloudtype}"'\n                  tags = arc_config.get( "TAGS" )                    if  tags is not None:                       connect_cmd += f ' --tags "{tags}"'\n                  correlation_id = arc_config.get( "CORRELATION_ID" )                    if  correlation_id is not None:                       connect_cmd += f ' --correlation-id "{correlation_id}"'\n                  proxy_url = arc_config.get( "PROXY_URL" )                    if  proxy_url is not None:                       set_proxy_cmd = f '/usr/bin/azcmagent config set  proxy.url "{proxy_url}"'                       telemetry_common.run_cmd(logger, set_proxy_cmd)\n                   # Hardcoding allowed extensions as these will only vary if  NC team adds new extensions                   allowed_extensions =  "Microsoft.Azure.Monitor/AzureMonitorLinuxAgent,Microsoft.Azure.AzureDefende rForServers/MDE.Linux"                   set_extensions_cmd = (                       f '/usr/bin/azcmagent config set extensions.allowlist " {allowed_extensions}"'                   )                   telemetry_common.run_cmd(logger, set_extensions_cmd)\n                  telemetry_common.az_login(logger, arc_config)                   logger.info( "Connecting machine to Azure Arc..." )\n                  try:                       telemetry_common.run_cmd(logger, connect_cmd)                   except:                       logger.info( "Trying to connect machine to Azure \nArc..." )                   finally:                       telemetry_common.az_logout(logger)\n              def run_disconnect(logger, arc_config):                   logger.info( "Disconnect machine from Azure Arc..." )                   service_principal_id =  arc_config.get( "SERVICE_PRINCIPAL_ID" )                   service_principal_secret =  arc_config.get( "SERVICE_PRINCIPAL_SECRET" )\n                  cmd = f '/usr/bin/azcmagent disconnect --service-principal- id "{service_principal_id}" --service-principal-secret " {service_principal_secret}"'\n                  telemetry_common.az_login(logger, arc_config)\n                  try:                       telemetry_common.run_cmd(logger, cmd)                   except:                       logger.info( "Trying to disconnect machine from Azure  Arc..." )                   finally:                       telemetry_common.az_logout(logger)\n              def arc_enrollment(logger, arc_config):                   logger.info( "Executing Arc enrollment..." )\n                  telemetry_common.az_cli_cm_ext_install(logger, arc_config)\n                   # Get connected machine properties                   cm_provisioning_state, cm_status =  telemetry_common.get_cm_properties(                       logger, arc_config                   )\n                   if  (                       cm_provisioning_state ==  telemetry_common.AgentryResult.SUCCEEDED                       and cm_status ==  telemetry_common.AgentryResult.CONNECTED                   ):                        logger.info(telemetry_common.OnboardingMessage.COMPLETED)                        return  True                    elif  cm_provisioning_state  in  [                       telemetry_common.AgentryResult.FAILED,                       telemetry_common.AgentryResult.DISCONNECTED,                   ]:                       run_disconnect(logger, arc_config)                        logger.warning(telemetry_common.OnboardingMessage.STILL_TRYING)                        return  False\n                   elif  cm_provisioning_state ==  telemetry_common.AgentryResult.CREATING:                        logger.warning(telemetry_common.OnboardingMessage.STILL_CREATING)                        return  False                    else :                       run_connect(logger, arc_config)                        logger.warning(telemetry_common.OnboardingMessage.STILL_TRYING)                        return  False\n              def main():                   timeout = 300   #  TODO:  increase when executed via systemd  unit                   start_time = time.time()                   end_time = start_time + timeout\n                  config_file = telemetry_common.arc_config_file\n                  logger = telemetry_common.get_logger(__name__)\n                  logger.info( "Running setup_arc_for_servers.py..." )\n                   if  config_file is None:                       raise Exception( "config file is expected" )\n                  arc_config = {}\n                  with open(config_file,  "r" ) as file:                       arc_config = json.load(file)\n                  arc_enrolled = False\n                   while  time.time() < end_time:                       logger.info( "Arc enrolling the server..." )                       try:                           arc_enrolled = arc_enrollment(logger, arc_config)                       except Exception as e:                           logger.error(f "Could not arc enroll server: {e}" )                        if  arc_enrolled:                            break                       logger.info( "Sleeping 30s..." )   # retry for Azure info                       time.sleep(30)\n               if  __name__ ==  "__main__" :                   main()\n              EOF\n              nsenter -t1 -m -u -n -i tee  "\${WORKDIR}" /telemetry/setup_azure_monitor_agent.py > /dev/null <<EOF                #!/usr/bin/python3               import json\n              import os               import socket               import time\n              import telemetry_common\n              def run_install(logger, ama_config):                   logger.info( "Install Azure Monitor agent..." )                   resource_group = ama_config.get( "RESOURCE_GROUP" )                   location = ama_config.get( "LOCATION" )                   proxy_url = ama_config.get( "PROXY_URL" )                   hostname = socket.gethostname()                    if  proxy_url is not None:                       os.environ[ "HTTP_PROXY" ] = proxy_url                       os.environ[ "HTTPS_PROXY" ] = proxy_url                       settings = (                            '{"proxy":{"mode":"application","address":"'                           + proxy_url                           +  '","auth": "false"}}'                       )                       cmd = f '/usr/bin/az connectedmachine extension create  --no-wait --name "AzureMonitorLinuxAgent" --publisher  "Microsoft.Azure.Monitor" --type "AzureMonitorLinuxAgent" --machine-name " {hostname}" --resource-group "{resource_group}" --location "{location}" -- verbose --settings \' {settings}\ ''                    else :                       cmd = f '/usr/bin/az connectedmachine extension create  --no-wait --name "AzureMonitorLinuxAgent" --publisher  "Microsoft.Azure.Monitor" --type "AzureMonitorLinuxAgent" --machine-name " {hostname}" --resource-group "{resource_group}" --location "{location}" -- verbose'\n                  version = ama_config.get( "VERSION" )                    if  version is not None:                       cmd += f ' --type-handler-version "{version}"'\n                  logger.info( "Installing Azure Monitor agent..." )                   telemetry_common.az_login(logger, ama_config)\n                  try:                       telemetry_common.run_cmd(logger, cmd)                   except:                       logger.info( "Trying to install Azure Monitor  agent..." )                   finally:                       telemetry_common.az_logout(logger)\n              def run_uninstall(logger, ama_config):                   logger.info( "Uninstall Azure Monitor agent..." )                   resource_group = ama_config.get( "RESOURCE_GROUP" )                   hostname = socket.gethostname()                   cmd = f '/usr/bin/az connectedmachine extension delete -- name "AzureMonitorLinuxAgent" --machine-name "{hostname}" --resource-group "\n{resource_group}" --yes --verbose'\n                  telemetry_common.az_login(logger, ama_config)                   logger.info( "Uninstalling Azure Monitor agent..." )\n                  try:                       telemetry_common.run_cmd(logger, cmd)                   except:                        print ( "Trying to uninstall Azure Monitor agent..." )                   finally:                       telemetry_common.az_logout(logger)\n              def ama_installation(logger, ama_config):                   logger.info( "Executing AMA extenstion installation..." )                   telemetry_common.az_cli_cm_ext_install(logger, ama_config)\n                   # Get connected machine properties                   cm_provisioning_state, cm_status =  telemetry_common.get_cm_properties(                       logger, ama_config                   )\n                   if  (                       cm_provisioning_state ==  telemetry_common.AgentryResult.SUCCEEDED                       and cm_status ==  telemetry_common.AgentryResult.CONNECTED                   ):                        # Get AzureMonitorLinuxAgent extension status                       ext_provisioning_state =  telemetry_common.get_cm_extension_state(                           logger, ama_config,  "AzureMonitorLinuxAgent"                       )\n                       if  ext_provisioning_state ==  telemetry_common.AgentryResult.SUCCEEDED:                            logger.info(telemetry_common.OnboardingMessage.COMPLETED)                            return  True                        elif  ext_provisioning_state ==  telemetry_common.AgentryResult.FAILED:                           run_uninstall(logger, ama_config)                            logger.warning(telemetry_common.OnboardingMessage.STILL_TRYING)                            return  False                        elif  ext_provisioning_state ==  telemetry_common.AgentryResult.CREATING:                            logger.warning(telemetry_common.OnboardingMessage.STILL_CREATING)                            return  False                        else :                           run_install(logger, ama_config)                            logger.warning(telemetry_common.OnboardingMessage.STILL_TRYING)\n                           return  False                    else :                       logger.error( "Server not arc enrolled, enroll the  server and retry" )                        return  False\n              def main():                   timeout = 60   #  TODO:  increase when executed via systemd  unit                   start_time = time.time()                   end_time = start_time + timeout\n                  config_file = telemetry_common.arc_config_file\n                  logger = telemetry_common.get_logger(__name__)\n                  logger.info( "Running setup_azure_monitor_agent.py..." )\n                   if  config_file is None:                       raise Exception( "config file is expected" )\n                  ama_config = {}\n                  with open(config_file,  "r" ) as file:                       ama_config = json.load(file)\n                  ama_installed = False\n                   while  time.time() < end_time:                       logger.info( "Installing AMA extension..." )                       try:                           ama_installed = ama_installation(logger,  ama_config)                       except Exception as e:                           logger.error(f "Could not install AMA extension:  {e}" )                        if  ama_installed:                            break                       logger.info( "Sleeping 30s..." )   # retry for Azure info                       time.sleep(30)\n               if  __name__ ==  "__main__" :                   main()\n              EOF\n              nsenter -t1 -m -u -n -i tee  "\${WORKDIR}" /arc-connect.sh >  /dev/null <<EOF                #!/bin/bash                set  -e\n               echo   "{\"SUBSCRIPTION_ID\": \"\${SUBSCRIPTION_ID}\", \n\"SERVICE_PRINCIPAL_ID\": \"\${SERVICE_PRINCIPAL_ID}\",  \"SERVICE_PRINCIPAL_SECRET\": \"\${SERVICE_PRINCIPAL_SECRET}\",  \"RESOURCE_GROUP\": \"\${RESOURCE_GROUP}\", \"TENANT_ID\":  \"\${TENANT_ID}\", \"LOCATION\": \"\${LOCATION}\", \"PROXY_URL\":  \"\${PROXY_URL}\"}"  >  "\${WORKDIR}" /telemetry/arc-connect.json\n               echo   "Connecting machine to Azure Arc..."               /usr/bin/python3  "\${WORKDIR}" /telemetry/setup_arc_for_servers.py >  "\${WORKDIR}" /setup_arc_for_servers.out               cat  "\${WORKDIR}" /setup_arc_for_servers.out                if  grep  "Could not arc enroll server"   "\${WORKDIR}" /setup_arc_for_servers.out > /dev/null;  then                  exit  1                fi"}
{"text": "/usr/bin/azcmagent config  set  incomingconnections.ports 22               /usr/bin/azcmagent config  set  extensions.allowlist  Microsoft.Azure.Monitor/AzureMonitorLinuxAgent,Microsoft.Azure.AzureDefender ForServers/MDE.Linux,Microsoft.Azure.ActiveDirectory/AADSSHLoginForLinux\n               if  [  "\${INSTALL_AZURE_MONITOR_AGENT}"  =  "true"  ];  then                  echo   "Installing Azure Monitor agent..."                 /usr/bin/python3  "\${WORKDIR}" /telemetry/setup_azure_monitor_agent.py >  "\${WORKDIR}" /setup_azure_monitor_agent.out                 cat  "\${WORKDIR}" /setup_azure_monitor_agent.out                  if  grep  "Could not install AMA extension"   "\${WORKDIR}" /setup_azure_monitor_agent.out > /dev/null;  then                    exit  1                  fi                fi               EOF\n              nsenter -t1 -m -u -n -i sh  "\${WORKDIR}" /arc-connect.sh               nsenter -t1 -m -u -n -i rm -rf  "\${WORKDIR}"                echo   "Server monitoring configured successfully"               tail -f /dev/null           livenessProbe:             initialDelaySeconds: 600             periodSeconds: 60             timeoutSeconds: 30              exec :                command :                 - /bin/bash                 - -c                 - |                    set  -e                   WORKDIR=\$(nsenter -t1 -m -u -n -i mktemp -d)                    trap   'nsenter -t1 -m -u -n -i rm -rf "\${WORKDIR}"'  ERR  EXIT                   nsenter -t1 -m -u -n -i tee  "\${WORKDIR}" /liveness.sh >  /dev/null <<EOF                    #!/bin/bash                    set  -e\n                   # Check AMA processes                   ps -ef | grep  "\\\s/opt/microsoft/azuremonitoragent/bin/agentlauncher\\\s"                   ps -ef | grep  "\\\s/opt/microsoft/azuremonitoragent/bin/mdsd\\\s"                   ps -ef | grep  "\\\s/opt/microsoft/azuremonitoragent/bin/amacoreagent\\\s"\n                   # Check Arc server agent is Connected                   AGENTSTATUS= "\\\$(azcmagent show -j)"                    if  [[ \\\$( echo   "\\\${AGENTSTATUS}"  | jq -r .status) !=  "Connected"  ]];  then                      echo   "azcmagent is not connected"                      echo   "\\\${AGENTSTATUS}"                      exit  1                    fi\n                   # Verify dependent services are running                    while  IFS=  read  -r status;  do                      if  [[  "\\\${status}"  !=  "active"  ]];  then                        echo   "one or more azcmagent services not active"                        echo   "\\\${AGENTSTATUS}"                        exit  1                      fi                    done  < <(jq -r  '.services[] | (.status)'   <<<\\\${AGENTSTATUS})\n                   # Run connectivity tests                   RESULT= "\\\$(azcmagent check -j)"                    while  IFS=  read  -r reachable;  do                      if  [[ ! \\\${reachable} ]];  then                        echo   "one or more connectivity tests failed"                        echo   "\\\${RESULT}"                        exit  1                      fi                    done  < <(jq -r  '.[] | (.reachable)'  <<<\\\${RESULT})\n                  EOF\n                  nsenter -t1 -m -u -n -i sh  "\${WORKDIR}" /liveness.sh                   nsenter -t1 -m -u -n -i rm -rf  "\${WORKDIR}"                    echo   "Liveness check succeeded"\n      tolerations:         - operator:  "Exists"           effect:  "NoSchedule"\nEOF }\nSUBSCRIPTION_ID= "${SUBSCRIPTION_ID:?SUBSCRIPTION_ID must be set}" SERVICE_PRINCIPAL_ID= "${SERVICE_PRINCIPAL_ID:?SERVICE_PRINCIPAL_ID must be  set}" SERVICE_PRINCIPAL_SECRET= "${SERVICE_PRINCIPAL_SECRET:?\nSERVICE_PRINCIPAL_SECRET must be set}" RESOURCE_GROUP= "${RESOURCE_GROUP:?RESOURCE_GROUP must be set}" TENANT_ID= "${TENANT_ID:?TENANT_ID must be set}" LOCATION= "${LOCATION:?LOCATION must be set}" PROXY_URL= "${PROXY_URL:?PROXY_URL must be set}" INSTALL_AZURE_MONITOR_AGENT= "${INSTALL_AZURE_MONITOR_AGENT:? INSTALL_AZURE_MONITOR_AGENT must be true/false}" NAMESPACE= "${NAMESPACE:?NAMESPACE must be set}"\ncreate_secret create_daemonset"}
{"text": "Prerequisites-VM\nCluster administrator access to the Nexus Kubernetes cluster. See  documentation on connecting to the Nexus Kubernetes cluster.\nTo use Azure Arc-enabled servers, register the following Azure resource providers in your subscription: Microsoft.HybridCompute Microsoft.GuestConfiguration Microsoft.HybridConnectivity\nRegister these resource providers, if not done previously:\nAzure CLI\naz account set  --subscription   "{the Subscription Name}" az provider register  --namespace   'Microsoft.HybridCompute' az provider register  --namespace   'Microsoft.GuestConfiguration' az provider register  --namespace   'Microsoft.HybridConnectivity'\nAssign an Azure service principal to the following Azure built-in roles, as needed. Assign the service principal to the Azure resource group that has the machines to be connected:\nRole Needed to\nAzure Connected Machine Resource Connect Arc-enabled Nexus Kubernetes cluster VM Administrator   or  Contributor server in the resource group and install the Azure Monitoring Agent (AMA)\nMonitoring Contributor  or  Contributor Create a  Data Collection Rule (DCR)  in the resource group and associate Arc-enabled servers to it\nUser Access Administrator , and  Resource Needed if you want to use Azure policy Policy Contributor  or  Contributor assignment(s) to ensure that a DCR is associated\nRole Needed to\nwith  Arc-enabled machines\nKubernetes Extension Contributor Needed to deploy the K8s extension for Container Insights"}
{"text": "Environment setup\nCopy and run the included scripts. You can run them from an  Azure Cloud Shell , in the Azure portal. Or you can run them from a Linux command prompt where the Kubernetes command line tool (kubectl) and Azure CLI are installed.\nPrior to running the included scripts, define the following environment variables:\nEnvironment Variable Description\nSUBSCRIPTION_ID The ID of the Azure subscription that contains the resource group\nRESOURCE_GROUP The resource group name where Arc-enabled server and associated resources are created\nLOCATION The Azure Region where the Arc-enabled servers and associated resources are created\nSERVICE_PRINCIPAL_ID The appId of the Azure service principal with appropriate role assignment(s)\nSERVICE_PRINCIPAL_SECRET The authentication password for the Azure service principal\nTENANT_ID The ID of the tenant directory where the service principal exists\nPROXY_URL The proxy URL to use for connecting to Azure services\nNAMESPACE The namespace where the Kubernetes artifacts are created\narc-connect.env\n , to set the For convenience, you can modify the template file, \nenvironment variable values.\nBash\n# Apply the modified values to the environment  ./arc-connect.env"}
{"text": "Add a data collection rule (DCR)\nAssociate the Arc-enabled servers with a DCR to enable the collection of log data into a Log Analytics workspace. You can create the DCR via the Azure portal or CLI. Information on creating a DCR to collect data from the VMs is available  here .\ndcr.sh\n  script creates a DCR, in the specified resource group, that will The included \nconfigure log collection.\n1. Ensure proper  environment setup  and role  prerequisites  for the service principal. The DCR is created in the specified resource group.\n2. Create or identify a Log Analytics workspace for log data ingestion as per the DCR. Set an environment variable, LAW_RESOURCE_ID to its resource ID. Retrieve the resource ID for a known Log Analytics workspace name:\nBash\nexport  LAW_RESOURCE_ID=$(az monitor  log -analytics workspace show -g  "${RESOURCE_GROUP}"  -n <law name> --query id -o tsv)\n3. Run the dcr.sh script. It creates a DCR in the specified resource group with name ${RESOURCE_GROUP}-syslog-dcr\nBash\n./dcr.sh\nView/manage the DCR from the Azure portal or  CLI . By default, the Linux Syslog log level is set to "INFO". You can change the log level as needed.\n  Note\nManually, or via a policy, associate servers created prior to the DCR's creation. See remediation task ."}
{"text": "Associate Arc-enabled server resources to DCR\nAssociate the Arc-enabled server resources to the created DCR for logs to flow to the Log Analytics workspace. There are options for associating servers with DCRs."}
{"text": "Use Azure portal or CLI to associate selected Arc-enabled servers to"}
{"text": "DCR\nIn Azure portal, add Arc-enabled server resource to the DCR using its Resources section.\nUse this  link  for information about associating the resources via the Azure CLI."}
{"text": "Use Azure policy to manage DCR associations\nAssign a policy to the resource group to enforce the association. There's a built-in policy definition, to associate  Linux Arc Machines with a DCR . Assign the policy to the resource group with DCR as a parameter. It ensures association of all Arc-enabled servers, within the resource group, with the same DCR.\nIn the Azure portal, select the  policy definition  page.\nAssign\n  button from the \nFor convenience, the provided \nassign.sh\n  script assigns the built-in policy to the\ndcr.sh\n  script. specified resource group and DCR created with the \n1. Ensure proper  environment setup  and role  prerequisites  for the service principal to do policy and role assignments. 2. Create the DCR, in the resource group, using  Adding\ndcr.sh\n  script as described in \na Data Collection Rule  section.\nassign.sh\n  script. It creates the policy assignment and necessary role 3. Run the \nassignments.\nBash\n./assign.sh"}
{"text": "Connect Arc-enabled servers and install Azure monitoring agent\nUse the included \ninstall.sh\n  script to Arc-enroll all server VMs that represent the nodes\nof the Nexus Kubernetes cluster. This script creates a Kubernetes daemonSet on the Nexus Kubernetes cluster. It deploys a pod to each cluster node, connecting each VM to Arc-enabled servers and installing the Azure Monitoring Agent (AMA). The \ndaemonSet\nalso includes a liveness probe that monitors the server connection and AMA processes.\n1. Set the environment as specified in  Environment Setup . Set the current \nkubeconfig\ncontext for the Nexus Kubernetes cluster VMs. 2. Permit \nKubectl\n  access to the Nexus Kubernetes cluster.\n  Note\nWhen you create a Nexus Kubernetes cluster, Nexus automatically creates a managed resource group dedicated to storing the cluster resources, within this group, the Arc connected cluster resource is established.\nkubeconfig\n . After To access your cluster, you need to set up the cluster connect \nlogging into Azure CLI with the relevant Azure AD entity, you can obtain the\nkubeconfig\n  necessary to communicate with the cluster from anywhere, even\noutside the firewall that surrounds it.\na. Set \nCLUSTER_NAME\n , \nRESOURCE_GROUP\n  and \nSUBSCRIPTION_ID\n  variables.\nBash\nCLUSTER_NAME= "myNexusAKSCluster" RESOURCE_GROUP= "myResourceGroup" SUBSCRIPTION_ID=< set  the correct subscription_id>\naz\n  and store in \nMANAGED_RESOURCE_GROUP\nb. Query managed resource group with \nAzure CLI\n az account set  -s  $SUBSCRIPTION_ID  MANAGED_RESOURCE_GROUP=$(az networkcloud kubernetescluster show  -n   $CLUSTER_NAME  -g  $RESOURCE_GROUP  --output  tsv  --query   managedResourceGroupConfiguration.name)\nc. The following command starts a connectedk8s proxy that allows you to connect to the Kubernetes API server for the specified Nexus Kubernetes cluster.\nAzure CLI\naz connectedk8s proxy  -n  $CLUSTER_NAME   -g  $MANAGED_RESOURCE_GROUP &\nd. Use \nkubectl\n  to send requests to the cluster:\nConsole\nkubectl get nodes\nYou should now see a response from the cluster containing the list of all nodes.\n  Note\nIf you see the error message "Failed to post access token to client proxyFailed\naz login\n  to re-authenticate to connect to MSI", you may need to perform an \nwith Azure.\ninstall.sh\n  script from the command prompt with kubectl access to the 3. Run the \nNexus Kubernetes cluster.\nThe script deploys the \ndaemonSet\n  to the cluster. Monitor the progress as follows:\nBash\n# Run the install script and observe results ./install.sh kubectl get pod --selector= 'name=naks-vm-telemetry' kubectl logs <podname>\nOn completion, the system logs the message "Server monitoring configured successfully". At that point, the Arc-enabled servers appear as resources within the selected resource group.\n  Note\nAssociate these connected servers to the  DCR . After you configure a policy, there may be some delay to observe the logs in Azure Log Analytics Workspace"}
{"text": "Monitor Nexus Kubernetes cluster  K8s layer"}
{"text": "Prerequisites-Kubernetes\nThere are certain prerequisites the operator should ensure to configure the monitoring tools on Nexus Kubernetes Clusters.\nContainer Insights stores its data in a  Log Analytics workspace . Log data flows into the workspace whose Resource ID you provided during the initial scripts covered in the "Add a data collection rule (DCR)"  section. Else, data funnels into a default workspace in the Resource group associated with your subscription (based on Azure location).\nAn example for East US may look like follows:\nLog Analytics workspace Name: DefaultWorkspace-<GUID>-EUS Resource group name: DefaultResourceGroup-EUS\nRun the following command to get a pre-existing  Log Analytics workspace Resource ID :\nAzure CLI\naz login\naz account set  --subscription   "<Subscription Name or ID the Log Analytics  workspace is in>"\naz monitor log-analytics workspace show  --workspace-name   "<Log Analytics  workspace Name>"  \    --resource-group   "<Log Analytics workspace Resource Group>"  \    -o  tsv  --query  id\nTo deploy Container Insights and view data in the applicable Log Analytics workspace requires certain role assignments in your account. For example, the "Contributor" role assignment. See the instructions for  assigning required roles :\nLog Analytics Contributor  role: necessary permissions to enable container monitoring on a CNF (provisioned) cluster. Log Analytics Reader  role: non-members of the Log Analytics Contributor role, receive permissions to view data in the Log Analytics workspace once you enable container monitoring."}
{"text": "Install the cluster extension\nSign-in into the  Azure Cloud Shell  to access the cluster:\nAzure CLI\naz login\naz account set  --subscription   "<Subscription Name or ID the Provisioned  Cluster is in>"\nNow, deploy Container Insights extension on a provisioned Nexus Kubernetes cluster using either of the next two commands:"}
{"text": "With customer pre-created Log analytics workspace\nAzure CLI\naz k8s-extension create  --name  azuremonitor -containers  \    --cluster-name   "<Nexus Kubernetes cluster Name>"  \    --resource-group   "<Nexus Kubernetes cluster Resource Group>"  \    --cluster-type  connectedClusters \    --extension-type  Microsoft.AzureMonitor.Containers \    --release-train  preview \\n   --configuration-settings  logAnalyticsWorkspaceResourceID= "<Log Analytics  workspace Resource ID>"  \   amalogsagent.useAADA uth= true"}
{"text": "Use the default Log analytics workspace\nAzure CLI\naz k8s-extension create  --name  azuremonitor -containers  \    --cluster-name   "<Nexus Kubernetes cluster Name>"  \    --resource-group   "<Nexus Kubernetes cluster Resource Group>"  \    --cluster-type  connectedClusters \    --extension-type  Microsoft.AzureMonitor.Containers \    --release-train  preview \    --configuration-settings  amalogsagent.useAADA uth= true"}
{"text": "Validate Cluster extension\nValidate the successful deployment of monitoring agents enablement on Nexus Kubernetes Clusters using the following command:\nAzure CLI\naz k8s-extension show  --name  azuremonitor -containers  \    --cluster-name   "<Nexus Kubernetes cluster Name>"  \    --resource-group   "<Nexus Kubernetes cluster Resource Group>"  \    --cluster-type  conectedClusters\nLook for a Provisioning State of "Succeeded" for the extension. The "k8s-extension create" command may have also returned the status."}
{"text": "Customize logs & metrics collection\nContainer Insights provides end-users functionality to fine-tune the collection of logs and metrics from Nexus Kubernetes Clusters-- Configure Container insights agent data collection ."}
{"text": "Extra resources\nReview  workbooks documentation  and then you may use Operator Nexus telemetry  sample Operator Nexus workbooks .\nReview  Azure Monitor Alerts , how to create  Azure Monitor Alert rules , and use sample Operator Nexus Alert templates ."}
{"text": "Monitoring virtual machines (for"}
{"text": "virtualized network function)\nArticle  03/14/2023\nThis section discusses the optional tooling available for telecom operators to monitor the Virtualized Network Functions (VNF) workloads. With Azure Monitoring Agent (AMA), logs and performance metrics can be collected from the Virtual Machines (VM) running VNFs. One of the pre-requisites for AMA is Arc connectivity back to Azure (using Azure Arc for Servers)."}
{"text": "Extension onboarding with CLI using managed"}
{"text": "identity auth\nWhen enabling Monitoring agents on VMs using CLI, ensure appropriate versions of CLI are installed:\nazure-cli: 2.39.0+ azure-cli-core: 2.39.0+ Resource-graph: 2.1.0+\nDocumentation for starting with  Azure CLI , how to install it across  multiple operating systems , and how to install  CLI extensions ."}
{"text": "Arc connectivity\nAzure Arc-enabled servers let you manage Linux physical servers and Virtual Machines hosted outside of Azure, such as on-premises cloud environment like Operator Nexus. A hybrid machine is any machine not running in Azure. When a hybrid machine is connected to Azure, it becomes a connected machine, treated as a resource in Azure. Each connected machine has a Resource ID enabling the machine to be included in a resource group."}
{"text": "Prerequisites\nBefore you start, be sure to review the  prerequisites  and verify that your subscription, and resources meet the requirements. Some of the prerequisites are:\nYour VNF VM is connected to CloudServicesNetwork (the network that the VM uses to communicate with Operator Nexus services).\nYou have SSH access to your VNF VM. Proxies & wget install: Ensure wget is installed. To set the proxy as an environment variable run:\nAzure CLI\necho  "http\_proxy=http://169.254.0.11:3128"  \>\> /etc/environment  echo  "https\_proxy=http://169.254.0.11:3128"  \>\> /etc/environment\nYou have appropriate permissions on VNF VM to be able to run scripts, install package dependencies etc. For more information visit  link  for more details. To use Azure Arc-enabled servers, the following Azure resource providers must be registered in your subscription: Microsoft.HybridCompute Microsoft.GuestConfiguration Microsoft.HybridConnectivity\nIf these resource providers aren't already registered, you can register them using the following commands:\nAzure CLI\naz account set  --subscription   "{Your Subscription Name}"  \naz provider register  --namespace   'Microsoft.HybridCompute'  \naz provider register  --namespace   'Microsoft.GuestConfiguration'  \naz provider register  --namespace   'Microsoft.HybridConnectivity'"}
{"text": "Deployment\nYou can Arc connect servers in your environment by performing a set of steps manually. The VNF VM can be connected to Azure using a deployment script. Or you can use an automated method by running a template script. The script can be used to automate the download and installation of the agent.\nThis method requires that you have administrator permissions on the machine to install and configure the agent. On Linux machine, you can deploy the required agentry by using the root account.\nThe script to automate the download and installation, and to establish the connection with Azure Arc, is available from the Azure portal. To complete the process, take the\nfollowing steps:\n1. From your browser, go to the  Azure portal .\n2. On the  Select a method  page, select the  Add a single server  tile, and then select Generate script .\n3. On the  Prerequisites  page, select  Next   .\n4. On the  Resource details  page, provide the following information:\n5. In the Subscription drop-down list, select the subscription the machine will be managed in.\n6. In the  Resource group  drop-down list, select the resource group the machine will be managed from.\n7. In the  Region  drop-down list, select the Azure region to store the server's metadata.\n8. In the  Operating system  drop-down list, select the operating system of your VNF VM.\n9. If the machine is communicating through a proxy server to connect to the internet, specify the proxy server IP address. If a name and port number is used, specify that information.\n10. Select  Next: Tags .\n11. On the  Tags  page, review the default  Physical location tags  suggested and enter a value, or specify one or more  Custom tags  to support your standards.\n12. Select  Next: Download and run script .\n13. On the  Download and run script  page, review the summary information, and then select  Download . If you still need to make changes, select  Previous .\nNote:\n1. Set the exit on error flag up at the top of the script to make sure it fails fast and doesn't give you false success in the end. For example, in Shell script use "set -e" at the top of the script. 2. Add export http_proxy=<PROXY_URL> and export https_proxy=<PROXY_URL> to the script along with export statements in the Arc connectivity script. (Proxy IP - 169.254.0.11:3128).\nTo deploy the \nazcmagent\n on the server, sign-in to the server with an account that has\nroot access. Change to the folder that you copied the script to and execute it on the server by running the ./OnboardingScript.sh script.\nIf the agent fails to start after setup is finished, check the logs for detailed error information. The log directory is \n/var/opt/azcmagent/log\n.\nAfter you install the agent and configure it to connect to Azure Arc-enabled servers, verify that the server is successfully connected at  Azure portal .\nFigure: Sample Arc-Enrolled VM"}
{"text": "Troubleshooting\nNote:  If you see errors while running script, then fix the errors and rerun the script before moving to the next steps.\nSome common reasons for errors:\n1. You don't have the required permissions on the VM. 2. wget package isn't installed on the VM. 3. If it fails to install package dependencies, it's because proxy doesn't have the required domains added to the allowed URLs. For example, on Ubuntu, apt fails to install dependencies because it can't reach ".ubuntu.com". Add the required egress endpoints to the proxy."}
{"text": "Azure monitor agent\nThe Azure Monitor Agent is implemented as an  Azure VM extension  ver Arc connected Machines. It also lists the options to create  associations with Data Collection Rules  that\ndefine which data the agent should collect. Installing, upgrading, or uninstalling the Azure Monitor Agent won't require you to restart your server.\nEnsure that you configure collection of logs and metrics using the Data Collection Rule.\nFigure: DCR adding source\nNote:  The metrics configured with DCR should have destination set to Log Analytics Workspace as it's not supported on Azure Monitor Metrics yet.\nFigure: DCR adding destination"}
{"text": "Pre-requisites\nThe following prerequisites must be met prior to installing the Azure Monitor Agent:\nPermissions  : For methods other than using the Azure portal, you must have the following role assignments to install the agent:\nBuilt-in role Scopes Reason\nVirtual Machine Contributor   Azure Azure Arc-enabled To deploy the agent Connected Machine Resource Administrator servers\nAny role that includes the action Subscription To deploy Azure Microsoft.Resources/deployments/ * and/orResource group Resource Manager and/or templates"}
{"text": "Installing Azure Monitoring Agent\nOnce, the Virtual Machines are Arc connected, ensure that you create a local file from your  Azure Cloud Shell  with name "settings.json" to provide the proxy information:\nFigure: settings.json file\nThen use the following command to install the Azure Monitoring agent on these Azure Arc-enabled servers:\nAzure CLI\naz connectedmachine extension create  --name  AzureMonitorLinuxAgent  -- publisher  Microsoft.Azure.Monitor  --type  AzureMonitorLinuxAgent  --machine- name  \ <arc-server-name\>   --resource-group  \ <resource-group-name\>   --location   \ <arc-server-location\>   --type-handler-version   "1.21.1"   --settings   settings.json \nTo collect data from virtual machines by using Azure Monitor Agent, you'll need to:\n1. Create  Data Collection Rules (DCRs) that define which data Azure Monitor Agent sends to which destinations.\n2. Associate the Data Collection Rule to specific Virtual Machines."}
{"text": "Data Collection Rule via Portal\nThe steps to create a DCR and associate it to a Log Analytics Workspace can be found here .\nLastly verify if you're getting the logs in the Log Analytics Workspace specified."}
{"text": "Data collection rule via CLI\nFollowing are the commands to create and associate DCR to enable collection of logs and metrics from these Virtual Machines.\nCreate DCR:\nAzure CLI\naz monitor data-collection rule create  --name  \ <name-for-dcr\>   --resource- group  \ <resource-group-name\>   --location  \ <location-for-dcr\>   --rule-file  \ <rules-file\>   [--description]   [--tags]  \nAn example rules-file:\nAssociate DCR:\nAzure CLI\naz monitor data-collection rule association create  --name  \ <name-for-dcr- association\>   --resource  \ <connected-machine-resource-id\>   --rule-id  \ <dcr- resource-id\>   [--description]"}
{"text": "Additional resources\nReview  workbooks documentation  and then you may use Operator Nexus telemetry  sample Operator Nexus workbooks . Review  Azure Monitor Alerts , how to create  Azure Monitor Alert rules , and use sample Operator Nexus Alert templates ."}
{"text": "Cluster metrics configuration\nArticle  05/04/2023\nWhen the user deploys a Cluster, a standard set of metrics gets enabled for collection. For the list of metrics, see  List of Metrics Collected .\nUsers can't control the behavior (enable or disable) for collection of these included standard metrics. Though, users can control the collection of some optional metrics that aren't part of the link to the list. To enable this experience, users have to create and update a MetricsConfiguration resource for a cluster. By default, creation of this MetricsConfiguration resource doesn't change the collection of metrics. User has to update the resource to enable or disable these optional metrics collection.\n  Note\nFor a cluster, at max, only one MetricsConfiguration resource can be created.\nUsers need to create a MetricsConfiguration resource to check a list of\noptional metrics that can be controlled.\nDeletion of the MetricsConfiguration resource results in the standard set of\nmetrics being restored."}
{"text": "How to manage cluster metrics configuration\nTo support the lifecycle of cluster metrics configurations, the following interactions allow for the creation and management of a cluster's metrics configurations."}
{"text": "Creating a metrics configuration\nUse the \naz network cluster metricsconfiguration create\n command to create metrics\nconfiguration for cluster. If you have multiple Azure subscriptions, user must pass subscription ID either using a flag \n--subscription <SUBSCRIPTION_ID>\n to the CLI\ncommand or select the appropriate subscription ID using the  az account set  command.\nAzure CLI\naz networkcloud cluster metricsconfiguration create  \    --cluster-name   "<CLUSTER>"  \    --extended-location   name= "<CLUSTER_EXTENDED_LOCATION_ID>"   type= "CustomLocation"  \    --location   "<LOCATION>"  \ \n  --collection-interval   <COLLECTION_INTERVAL (1-1440)>  \    --enabled-metrics   "<METRIC_TO_ENABLE_1>"   "<METRIC_TO_ENABLE_2>"  \    --tags   <TAG_KEY1> = "<TAG_VALUE1>"   <TAG_KEY2> = "<TAG_VALUE2>"  \    --resource-group   "<RESOURCE_GROUP>"  \nReplace values within \n<\n \n>\n with your specific information.\nQuery the cluster resource and find the value of \n<CLUSTER-EXTENDED-LOCATION-ID>\nin the \nproperties.clusterExtendedLocation\nThe \ncollection-interval\n field is a mandatory field, and \nenabled-metrics\n is an\noptional field.\nAlternatively, operators can provide the list of enabled metrics via json or yaml file.\nExample: enabled-metrics.json file\nJSON\n[      "metric_1" ,      "metric_2"   ] \nExample: enabled-metrics.yaml file\nYAML\n-   "metric_1"   -   "metric_2"  \nExample command to use enabled-metrics json/yaml file:\nAzure CLI\naz networkcloud cluster metricsconfiguration create  \    --cluster-name   "<CLUSTER>"  \    --extended-location   name= "<CLUSTER_EXTENDED_LOCATION_ID>"   type= "CustomLocation"  \    --location   "<LOCATION>"  \    --collection-interval   <COLLECTION_INTERVAL (1-1440)>  \    --enabled-metrics   <path-to-yaml-or-json-file>  \    --tags   <TAG_KEY1> = "<TAG_VALUE1>"   <TAG_KEY2> = "<TAG_VALUE2>"  \    --resource-group   "<RESOURCE_GROUP>"  \nHere, <path-to-yaml-or-json-file> can be ./enabled-metrics.json or ./enabled- metrics.yaml (place the file under current working directory) before performing the action.\nTo see all available parameters and their description run the command:\nAzure CLI\naz networkcloud cluster metricsconfiguration create  --help"}
{"text": "Metrics configuration elements\nParameter name Description\nCLUSTER Resource Name of Cluster\nLOCATION The Azure Region where the Cluster is deployed\nCLUSTER_EXTENDED_LOCATION_ID The Cluster extended Location from Azure portal\nCOLLECTION_INTERVAL The collection frequency for default standard metrics\nRESOURCE_GROUP The Cluster resource group name\nTAG_KEY1 Optional tag1 to pass to MetricsConfiguration create\nTAG_VALUE1 Optional tag1 value to pass to MetricsConfiguration create\nTAG_KEY2 Optional tag2 to pass to MetricsConfiguration create\nTAG_VALUE2 Optional tag2 value to pass to MetricsConfiguration create\nMETRIC_TO_ENABLE_1 Optional metric "METRIC_TO_ENABLE_1" enabled in addition to the default metrics\nMETRIC_TO_ENABLE_2 Optional metric "METRIC_TO_ENABLE_2" enabled in addition to the default metrics\nSpecifying \n--no-wait --debug\n options in az command results in the execution of this\ncommand asynchronously. For more information, see  how to track asynchronous operations .\n  Note\nThe default metrics collection interval for standard set of metrics is set to\nevery 5 minutes. Changing the \ncollectionInterval\n will also impact the\ncollection frequency for default standard metrics.\nThere can be only one set of metrics configuration defined per cluster. The\nresource is created with the name \ndefault\n."}
{"text": "List the metrics configuration\nYou can check the metrics configuration resource for a specific cluster by using \naz\nnetworkcloud cluster metricsconfiguration list\n command:\nAzure CLI\naz networkcloud cluster metricsconfiguration list  \    --cluster-name   "<CLUSTER>"  \    --resource-group   "<RESOURCE_GROUP>""}
{"text": "Retrieving a metrics configuration\nAfter a metrics configuration gets created, Operators can check the details for resource using \naz networkcloud cluster metricsconfiguration show\n command:\nAzure CLI\naz networkcloud cluster metricsconfiguration show  \    --cluster-name   "<CLUSTER>"  \    --resource-group   "<RESOURCE_GROUP>"  \nThis command returns a JSON representation of the metrics configuration. You can observe the list of enabled and disabled metrics in addition to the collection frequency as an output for this command."}
{"text": "Updating a metrics configuration\nMuch like the creation of a metrics configuration, Operators can perform an update action to change the configuration or update the tags assigned to the metrics configuration.\nAzure CLI\naz networkcloud cluster metricsconfiguration update  \    --cluster-name   "<CLUSTER>"  \    --collection-interval   <COLLECTION_INTERVAL (1-1440)>  \    --enabled-metrics   "<METRIC_TO_ENABLE_1>"   "<METRIC_TO_ENABLE_2>"  \    --tags   <TAG_KEY1> = "<TAG_VALUE1>"   <TAG_KEY2> = "<TAG_VALUE2>"  \    --resource-group   "<RESOURCE_GROUP>"  \nOperators can update \ncollection-interval\n independent of \nenabled-metrics\n list. Omit\nfields that aren't getting changed.\nSpecifying \n--no-wait --debug\n options in az command results in the execution of this\ncommand asynchronously. For more information, see  how to track asynchronous operations ."}
{"text": "Deleting a metrics configuration\nDeletion of the metrics configuration returns the cluster to an unaltered configuration. To delete a metrics configuration, use the command:\nAzure CLI\naz networkcloud cluster metricsconfiguration delete  \    --cluster-name   "<CLUSTER>"  \    --resource-group   "<RESOURCE_GROUP>"  \nSpecifying \n--no-wait --debug\n options in az command results in the execution of this\ncommand asynchronously. For more information, see  how to track asynchronous operations ."}
{"text": "Tracking asynchronous operations using"}
{"text": "Azure CLI\nArticle  04/05/2023\nSome Azure CLI operations are asynchronous. To track the status of an asynchronous operation, the \noperationStatuses\n resource can be used. Asynchronous commands can\nbe run with a \n--debug\n flag enabled. When \n--debug\n is specified, the progress of the\nrequest can be monitored. The operation status URL can be found by examining the\nAzure-AsyncOperation\n or \nLocation\n header on the HTTP response to the creation\nrequest.\nOutput\n... many lines of logged information ... \nurllib3.connectionpool: https://management.azure.com:443 "PUT  /subscriptions/.../resourceGroups/.../providers/Microsoft.NetworkCloud/clust ers/.../metricsConfigurations/default?api-version=2022-12-12-preview  HTTP/1.1" 201 926  cli.azure.cli.core.util: Response status: 201  cli.azure.cli.core.util: Response headers: \n... several lines of http headers of the response ... \ncli.azure.cli.core.util:     'Azure-AsyncOperation':  'https://management.azure.com/subscriptions/.../providers/Microsoft.NetworkC loud/locations/EASTUS/operationStatuses/12312312-1231-1231-1231- 123123123123*99399E995...?api-version=2022-12-12-preview' \n... remaining http headers of the response and more lines of logging ... \nUsing the value from before:\nhttps://management.azure.com/subscriptions/.../providers/Microsoft.NetworkCloud/loc\nations/EASTUS/operationStatuses/12312312-1231-1231-1231-123123123123*99399E995...?\napi-version=2022-12-12-preview\n, an Azure CLI \naz rest\n call can be issued to retrieve the\noperation status.\nsh\naz rest -m get -u  "https://management.azure.com/subscriptions/.../providers/Microsoft.NetworkC loud/locations/EASTUS/operationStatuses/12312312-1231-1231-1231- 123123123123*99399E995...?api-version=2022-12-12-preview"  \nThis request will return an operation status result that can be requeried using the same command until the status reaches a final state of \nSucceeded\n or \nFailed\n. At this point, the\nrequested operation has ceased.\nJSON\n{     "endTime" :  "2023-02-08T17:38:31.2042934Z" ,     "error" : {},     "id" :  "subscriptions/.../providers/Microsoft.NetworkCloud/locations/EASTUS/operati onStatuses/12312312-1231-1231-1231-123123123123*99399E995...?api- version=2022-12-12-preview" ,     "name" :  "12312312-1231-1231-1231-123123123123*99399E995..." ,     "properties" :  null ,     "resourceId" :  "subscriptions/.../resourceGroups/.../providers/Microsoft.NetworkCloud/clust ers/.../metricsConfigurations/default?api-version=2022-12-12-preview" ,     "startTime" :  "2023-02-08T17:38:24.7576911Z" ,     "status" :  "Succeeded"   }"}
{"text": "How to pre-certify network functions on"}
{"text": "Azure Operator Nexus\nArticle  03/14/2023\nThe pre-certification of network functions accelerates deployment of network services. The pre-certified network functions can be managed like any other Azure resource. The lifecycle of these pre-certified network functions is managed by Azure Network Function Manager (ANFM) or an NFM of your choice.\nIn this section, we'll describe the process and the steps for network function pre- certification"}
{"text": "Pre-certification of network function for"}
{"text": "operators\nThe goal is make available a catalog of network functions that conform to the Operator Nexus specifications. NF partners onboarding to pre-certification program and ANFM service won't be required to change the commercial licensing arrangement with the operators."}
{"text": "Pre-certification process\nThis section outlines the pre-certification process for Network Function deployment. Microsoft uses this process with Network Equipment Providers (NEP) that provide network function(s). This process guides the partner through onboarding the network function onto Operator Nexus and certifies the network function deployment methods using Azure deployment services. The goal of this program is to ensure that the partner's network function deployment process is predictable and repeatable on the Operator Nexus platform. Microsoft provides a pre-certification environment for the partners to validate the deployment of their network function. As a result, the partners' network functions will be published in the Microsoft catalog of network functions. This catalog will be available to operators using the Operator Nexus platform.\nIf the NF partner is interested in listing their offer in the Azure Marketplace, Microsoft will work with the partner to enable this offering in the marketplace."}
{"text": "Azure Network Function Manager\nThe Azure  Network Function Manager (ANFM)  provides a cloud native orchestration and managed experience for pre-certified network functions (from the Azure Marketplace). The ANFM provides consistent Azure managed applications experience for network functions."}
{"text": "Pre-certification steps\nHere are the steps of the NF Deployment pre-certification\nFigure: Pre-Certification (precert) Process"}
{"text": "Prerequisites and process for partner on-"}
{"text": "boarding to the pre-cert lab\nTo ensure an efficient and effective onboarding process for the partner there are perquisites to pre-certification lab entry.\n1. The partners start the Azure Marketplace agreement and  create a partner center account . The partner can then publish the network function offers in the marketplace. The marketplace agreement doesn't have to be completed prior to precert lab entry. However, it's an important step before the helm charts and images on-boarded to Azure Network Function Manager (ANFM) service are added to the pre-certified catalog.\n2. Microsoft will conduct several sessions on key topics with the partner:\na. Technical discussions describing the Operator Nexus architecture with focus on run time specification:\nCompute dimensions for Kubernetes master and worker nodes, memory, storage requirements, and compute capabilities\nNUMA alignment huge page support hyperthreading VM Networking/Kubernetes networking requirements: SR-IOV DPDK Review \ncloudinit\n support for VM based network functions\nMicrosoft AKS-Hybrid support for tenant workloads, CNI versions for Calico and Cultus\nb. The Operator Nexus platform includes a managed fabric automation service. With an agreement from the partner regarding the network function requirements, Microsoft will engage with the partner and review:\nthe network fabric architecture and fabric automation APIs for the creation of L2/L3 isolation-domains L3 route policies that will extend the network connectivity from the node to the TOR/CE router.\nThe fabric deep dive sessions will identify the peering requirements, route policies, and filters that need to be configured in the fabric for testing the network function.\nc. Microsoft will work with the NEPs to onboard the helm charts and container images (CNFs) or VM images (VNFs) to the Azure Network Function Manager service (ANFM). Microsoft will consult with the partner to validate the supported versions of the helm charts for deployment using the ANFM service.\nd. Microsoft will work with the partner to identify test tool requirements for the specific network function and prepare the lab prior to entry. Microsoft will provide basic traffic simulation tools such as Developing Solutions' dsTest or Spirent Landslide in the precert lab. The partners can also deploy other test tools of their choice during the precert testing.\n3. On reviewing the requirements (2a  2d) and prior to smoke testing, Microsoft will work with the partners to define Azure Resource Manager (ARM) templates:\nfor network fabric automation components  L2/L3 isolation-domains, L3 route policies if any for the end-to-end test set-up for AKS-Hybrid (CNF), VM instance (VNF), workload networks and management networks that describe subcluster networking onboard the helm charts and images to ANFM service and create a vendor image version for deployment into precert lab\nfor deployment of NFs using ANFM with the user data container pods/ VMs configuration properties for deployment of NEP specific config management application for deployment of other CNF/VNF test tools."}
{"text": "Operator Nexus technical specification documents"}
{"text": "provided to NF partners\nAs a part of technical engagement, the documents that Microsoft will provide to the partner are:\nOperator Nexus runtime specification document and the Azure Resource Manager (ARM) API specification document for creating AKS-Hybrid clusters (CNFs) and VM instances (VNFs) on an Operator Nexus cluster Azure Resource Manager API specification document for creating fabric automation components that are required for tenant networking. ARM API specification document for onboarding to ANFM service. The specification will also define the customer facing APIs for deployment of network functions using the ANFM service."}
{"text": "Scheduling/partner engagement\nThe Network Function deployment pre-certification lab will be a shared infrastructure where multiple partners will be testing simultaneously. Based on the available capacity in the lab Microsoft will allocate resources for various partners to complete the Network Function deployment pre-certification activities in a timely and limited window.\nIf the partner has a dedicated Operator Nexus environment in their facility, Microsoft will work with them to enable updated version of Operator Nexus software to complete Network Function deployment pre-certification."}
{"text": "Deployment testing\nScope of testing in the pre-certification lab\nWith the ARM templates defined in the previous section, Microsoft will work with the partners to identify the appropriate lab environment to perform the testing. Microsoft will enable the appropriate Subscription ID and Resource Groups so that the partners can deploy all the resources from the Azure Portal/CLI into the target lab environment. Microsoft will also enable jump box access for the partners to perform troubleshooting\nor remote connectivity to the test tools/config management tools. The following verification will be performed by the partners and results reviewed by Microsoft:\n1. Verify that the resources in the ARM template, corresponding to the tenant cluster definition, include:\nAKS-Hybrid cluster/ VM instance managed fabric resources for isolation-domain and L3 route policies workload networks and management network resources for Kubernetes networks/ VM networks\n2. Verify the basic network connectivity between all the end points of test set-up is working.\n3. After the tenant cluster set-up is validated, verify that the ANFM network function is working as designed. Verify that the container pods are in a running state.\n4. Verify that NF works with supported versions of Kubernetes, CNI versions.\n5. Verify that the helm-based upgrade operations on the NF application using ANFM service is working as designed.\n6. Perform interface testing after the NF has been deployed and configured. This testing will validate that the networking is working as designed. And it validates Kubernetes cluster's internal network and connection to a source/sink (simulator).\n7. Perform a low-volume traffic simulation on the application.  To validate the internal routing, deploy the test tool application inside the Operator Nexus cluster. To test the routing across the CE, based on the application characteristics, deploy the test tool application outside the Operator Nexus cluster.\n8. Azure PaaS integration (optional): Microsoft precert environment will be connected to an Azure region using ExpressRoute. The NEP partner can also integrate Azure PaaS services with their application. They can verify that the PaaS functionality is working as designed.\n9. After testing is complete, delete NF resources in the ANFM service and verify that the container pods are removed from the subcluster.\n10. Verify all the tenant cluster and fabric components are deleted. Validate that deleting the network fabric resources removes the corresponding configuration on the Network devices.\nTesting tools in pre-certification lab\nMicrosoft will provide basic traffic simulation tools such as Developing Solutions' dsTest or Spirent Landslide in the precert lab. These tools can be used to validate packet flow patterns for the network function. The partners can also deploy other test tools of their choice during the precert testing."}
{"text": "Test results from deployment precertification testing\nMicrosoft will review the test results, provided by the partner, for the application being precertified. The objective of the Network Function Deployment Precertification process is to ensure that the test cases defined, and test results produced comprehensively validate the deployment of the application on the Operator Nexus platform. For interface testing using a test tool such as Spirent, Microsoft will work with the partners to identify the test scenarios. After the deployment validation and smoke testing are completed in the precertification lab, Microsoft will review the test results with the NEPS to confirm that the test results meet the scope of deployment pre-certification. Microsoft will then work with the partner to graduate the NF application to the ANFM service catalog. The partners are free to share the results of the pre-certification/re- certification testing."}
{"text": "Recertification\nMicrosoft will enable the preview/ update versions of Operator Nexus platform and ANFM service releases in precert lab. Microsoft will coordinate with partners to recertify."}
{"text": "Prepare to install Azure CLI extensions\nArticle  07/14/2023\nThis how-to guide explains the steps for installing the required az CLI and extensions required to interact with Operator Nexus.\nInstallations of the following CLI extensions are required: \nnetworkcloud\n  (for\nmanagednetworkfabric\n  (for Microsoft.NetworkCloud APIs), \nMicrosoft.ManagedNetworkFabric APIs) and \nhybridaks\n  (for AKS-Hybrid APIs).\nIf you haven't already installed Azure CLI:  Install Azure CLI . The aka.ms links download the latest available version of the extension."}
{"text": "networkcloud"}
{"text": "CLI extension Install \nRemove any previously installed version of the extension\nAzure CLI\naz extension remove  --name  networkcloud\nInstall and test the \nnetworkcloud\n  CLI extension\nAzure CLI\naz extension add  --name  networkcloud az networkcloud  --help"}
{"text": "managednetworkfabric"}
{"text": "CLI extension Install \nRemove any previously installed version of the extension\nAzure CLI\naz extension remove  --name  managednetworkfabric\nInstall and test the \nmanagednetworkfabric\n  CLI extension\nAzure CLI\naz extension add  --name  managednetworkfabric az networkfabric  --help"}
{"text": "Install AKS-Hybrid ("}
{"text": "hybridaks"}
{"text": ") CLI extension\nRemove any previously installed version of the extension\nAzure CLI\naz extension remove  --name  hybridaks\nDownload the \nhybridaks\n  python wheel\nLinux / macOS / WSL\nsh\n    curl -L  "https://aka.ms/nexus-hybridaks-cli"  --output  "hybridaks- 0.0.0-py3-none-any.whl"\nhybridaks\n  CLI extension Install and test the \nAzure CLI\naz extension add  --source  hybridaks - 0.0.0 -py 3 -none-any .whl az hybridaks  --help"}
{"text": "Install other Azure extensions\nAzure CLI\naz extension add  --yes   --upgrade   --name  customlocation az extension add  --yes   --upgrade   --name  k8s -extension az extension add  --yes   --upgrade   --name  k8s -configuration az extension add  --yes   --upgrade   --name  arcappliance az extension add  --yes   --upgrade   --name  connectedmachine az extension add  --yes   --upgrade   --name  monitor -control-service   --version   0.2.0 az extension add  --yes   --upgrade   --name  ssh az extension add  --yes   --upgrade   --name  connectedk8s\nList installed CLI extensions and versions\nList the extension version running:\nAzure CLI\naz extension list  --query   "[].{Name:name,Version:version}"   -o  table\nExample output:\nOutput\nName                     Version -----------------------  ------------- arcappliance             0.2.31 monitor-control-service  0.2.0 connectedmachine         0.5.1 connectedk8s             1.3.20 k8s-extension            1.4.2 networkcloud             0.4.0.post94 k8s-configuration        1.7.0 managednetworkfabric     0.1.0.post45 customlocation           0.1.3 hybridaks                0.2.1 ssh                      1.1.6"}
{"text": "Troubleshoot Azure Operator Nexus"}
{"text": "server problems\nArticle  06/27/2023\nThis article describes how to troubleshoot server problems by using restart, reimage, and replace (three Rs) actions on Azure Operator Nexus bare-metal machines (BMMs). You might need to take these actions on your server for maintenance reasons, which causes a brief disruption to specific BMMs.\nThe time required to complete each of these actions is similar. Restarting is the fastest, whereas replacing takes slightly longer. All three actions are simple and efficient methods for troubleshooting."}
{"text": "Prerequisites\nFamiliarize yourself with the capabilities referenced in this article by reviewing the BMM actions . Gather the following information: Name of the resource group for the BMM Name of the BMM that requires a lifecycle management operation"}
{"text": "Identify the corrective action\nWhen you're troubleshooting a BMM for failures and determining the best corrective action, it's important to understand the available options. Restarting or reimaging a BMM can be an efficient and effective way to fix problems or simply restore the software to a known-good place. This article provides direction on the best practices for each of the three Rs.\nTroubleshooting technical problems requires a systematic approach. One effective method is to start with the simplest and least invasive solution and work your way up to more complex and drastic measures, if necessary.\nThe first step in troubleshooting is often to try restarting the device or system. Restarting can help to clear any temporary glitches or errors that might be causing the problem. If restarting doesn't solve the problem, the next step might be to try reimaging the device or system.\nIf reimaging doesn't solve the problem, the final step might be to replace the faulty hardware component. Replacement can be a more drastic measure, but it might be\nnecessary if the problem is related to a hardware malfunction.\nKeep in mind that these troubleshooting methods might not always be effective, and other factors in play might require a different approach."}
{"text": "Troubleshoot with a restart action\nRestarting a BMM is a process of restarting the server through a simple API call. This action can be useful for troubleshooting problems when tenant virtual machines on the host aren't responsive or are otherwise stuck.\nThe restart typically is the starting point for mitigating a problem."}
{"text": "Troubleshoot with a reimage action\nReimaging a BMM is a process that you use to redeploy the image on the OS disk, without impact to the tenant data. This action executes the steps to rejoin the cluster with the same identifiers.\nThe reimage action can be useful for troubleshooting problems by restoring the OS to a known-good working state. Common causes that can be resolved through reimaging include recovery due to doubt of host integrity, suspected or confirmed security compromise, or "break glass" write activity.\nA reimage action is the best practice for lowest operational risk to ensure the integrity of the BMM."}
{"text": "Troubleshoot with a replace action\nServers contain many physical components that can fail over time. It's important to understand which physical repairs require BMM replacement and when BMM replacement is recommended but not required.\nA hardware validation process is invoked to ensure the integrity of the physical host in advance of deploying the OS image. Like the reimage action, the tenant data isn't modified during replacement.\nAs a best practice, cordon off and shut down the BMM in advance of physical repairs. When you're performing the following physical repair, a replace action isn't required because the BMM host will continue to function normally after the repair:\nHot swappable power supply\nWhen you're performing the following physical repairs, we recommend a replace action, though it isn't necessary to bring the BMM back into service:\nCPU DIMM Fan Expansion board riser Transceiver Ethernet or fiber cable replacement\nWhen you're performing the following physical repairs, a replace action is required to bring the BMM back into service:\nBackplane System board SSD disk PERC/RAID adapter Mellanox NIC Broadcom embedded NIC"}
{"text": "Summary\nRestarting, reimaging, and replacing are effective troubleshooting methods that you can use to address technical problems. However, it's important to have a systematic approach and to consider other factors before you try any drastic measures.\nIf you still have questions,  contact support ."}
{"text": "Troubleshoot AKS hybrid cluster"}
{"text": "provisioning failures\nArticle  06/27/2023\nTo gather the data needed to diagnose Azure Kubernetes Service (AKS) hybrid cluster creation or management problems for Azure Operator Nexus, you first need to  check the status of your installation ."}
{"text": "If  Status  isn't  Connected  and  Provisioning state  isn't  Succeeded , the installation failed. This article can help you troubleshoot the failure.\nFor more information, see  Manage an AKS hybrid cluster ."}
{"text": "Prerequisites\nInstall the latest version of the  appropriate Azure CLI extensions . Gather this information: Tenant ID Subscription ID Cluster name and resource group Network fabric controller and resource group Network fabric instances and resource group AKS hybrid cluster name and resource group Prepare Azure CLI commands, Bicep templates, and Azure Resource Manager templates (ARM templates) that you use for resource creation."}
{"text": "What does an unhealthy AKS hybrid cluster"}
{"text": "look like?\nSeveral types of failures look similar to a user.\nIn the Azure portal, an unhealthy cluster might show:\nAn alert that says "This cluster isn't connected to Azure." A status of  Offline . A message that refers to certificate expiration time for a managed identity: "Couldn't display date/time, invalid format."\nIn the Azure CLI, check the output of the following command:\nAzure\naz hybridaks show  -g   <> --name   <>  \nAn unhealthy cluster might show:\nprovisioningState\n: \nFailed\n.\nprovisioningState\n: \nSucceeded\n, but null values for fields such as\nlastConnectivityTime\n and \nmanagedIdentityCertificateExpirationTime\n, or an\nerrorMessage\n field that isn't null."}
{"text": "Troubleshoot basic network requirements\nAt a minimum, every AKS hybrid cluster needs a default Container Network Interface (CNI) network and a cloud services network. Starting from the bottom up, consider managed network fabric resources, network cloud resources, and AKS hybrid resources."}
{"text": "Network fabric resources\nEach network cloud cluster can support up to 200 cloud services networks. The fabric must be configured with a Layer 3 (L3) isolation domain and an L3 internal network for use with the default CNI network. The VLAN range can be greater than 1,000 for the default CNI network. The L3 isolation domain must be successfully enabled."}
{"text": "Network cloud resources\nThe cloud services network must be created. Use the correct hybrid AKS extended location. You can get it from the respective site cluster while you're creating the AKS hybrid resources. The default CNI network must be created with an IPv4 prefix and a VLAN that matches an existing L3 isolation domain. The IPv4 prefix must be unique across all default CNI networks and Layer 3 networks. The networks must have a \nprovisioningState\n value of \nSucceeded\n.\nLearn how to connect a network cloud by using the Azure CLI ."}
{"text": "AKS hybrid resources\nTo be used by an AKS hybrid cluster, each network cloud network must be "wrapped" in an AKS hybrid virtual network.  Learn how to configure an AKS hybrid virtual network by using the Azure CLI ."}
{"text": "Troubleshoot common problems\nAny of the following problems can cause the AKS hybrid cluster to fail to be fully provisioned."}
{"text": "AKS hybrid clusters might fail or time out when they're"}
{"text": "created concurrently\nThe Azure Arc appliance can handle creating only one AKS hybrid cluster at a time within an instance. After you create a single AKS hybrid cluster, you must wait for its provisioning status to be \nSucceeded\n and for the cluster status to appear as  Connected\nor  Online  in the Azure portal.\nIf you tried to create several at once and have them in a \nFailed\n state, delete all failed\nclusters and any partially succeeded clusters. Anything that isn't a fully successful cluster should be deleted.\nAfter all clusters and artifacts are deleted, wait a few minutes for the Azure Arc appliance and cluster operators to reconcile. Then try to create a single new AKS hybrid cluster. Wait for that to come up successfully and report as  Connected  or  Online . You should now be able to continue creating AKS hybrid clusters, one at a time."}
{"text": "Case mismatch between an AKS hybrid virtual network"}
{"text": "and a network cloud network\nFor you to configure an AKS hybrid virtual network, the resource IDs for the network cloud network must precisely match the Azure Resource Manager resource IDs. To ensure that the IDs have identical uppercase and lowercase letters, ensure that you use the correct casing when you're setting up the network.\nIf you're using the Azure CLI, use the \n--aods-vnet-id*\n parameter. If you're using Azure\nResource Manager, Bicep, or a manual Azure REST API call, use the value of\n.properties.infraVnetProfile.networkCloud.networkId\n.\nThe most reliable way to obtain the correct value for creating the virtual network is to query the object for its ID. For example:\nBash\naz networkcloud cloudservices show -g  "example-rg"  -n  "csn-name"  -o tsv -- query id  az networkcloud defaultcninetwork show -g  "example-rg"  -n  "dcn-name"  -o tsv  --query id  az networkcloud l3network show -g  "example-rg"  -n  "l3n-name"  -o tsv --query  id"}
{"text": "L3 isolation domain or L2 isolation domain isn't enabled\nAt a high level, the steps to create isolation domains are:\n1. Create the L3 isolation domain.\n2. Add one or more internal networks.\n3. Add one external network (optional, if northbound connectivity is required).\n4. Enable the L3 isolation domain by using the following command:\nBash\naz nf l3domain update-admin-state --resource-group  "RESOURCE_GROUP_NAME"  --resource-name  "L3ISOLATIONDOMAIN_NAME"  --state  "Enable"  \nIt's important to check that the fabric resources achieve an \nadministrativeState\n value\nof \nEnabled\n, and that the \nprovisioningState\n value is \nSucceeded\n. If the \nupdate-admin-\nstate\n step is skipped or unsuccessful, the networks can't operate. You can use \nshow\ncommands to check the values. For example:\nBash\naz nf l3domain show -g  "example-rg"  --resource-name  "l2domainname"  -o table  az nf l2domain show -g  "example-rg"  --resource-name  "l3domainname"  -o table"}
{"text": "Network cloud network status is Failed\nWhen you create networks, ensure that they come up successfully. In particular, pay attention to the following constraints when you're creating default CNI networks:\nThe IPv4 prefix and VLAN need to match the internal network in the referenced L3 isolation domain. The IPv4 prefix must be unique across default CNI networks (and Layer 3 networks) in the network cloud cluster.\nIf you're using the Azure CLI to create these resources, the \n--debug\n option is helpful.\nThe output includes an operation status URL, which you can query by using \naz rest\n.\nDepending on the mechanism used for creation (Azure portal, Azure CLI, Azure Resource Manager), it's sometimes hard to see why resources are \nFailed\n.\nOne useful tool to help surface errors is the  az monitor activity-log  command. You can use it to show activities for a specific resource ID, resource group, or correlation ID. (You can also get this information in the  Activity  area of the Azure portal.)\nFor example, to see why a default CNI network failed, use the following code:\nBash\nRESOURCE_ID= "/subscriptions/$subscriptionsid/resourceGroups/example- rg/providers/Microsoft.NetworkCloud/defaultcninetworks/example-duplicate- prefix-dcn"      az monitor activity-log list --resource-id  "${RESOURCE_ID}"  -o tsv --query  '[].properties.statusMessage'  | jq \nHere's the result:\nOutput\n{    "status": "Failed", \n  "error": {      "code": "ResourceOperationFailure",      "message": "The resource operation completed with terminal provisioning  state 'Failed'.",      "details": [        {          "code": "Specified IPv4Connected Prefix 10.0.88.0/24 overlaps with  existing prefix 10.0.88.0/24 from example-dcn",          "message": "admission webhook \"vdefaultcninetwork.kb.io\" denied  the request: Specified IPv4Connected Prefix 10.0.88.0/24 overlaps with  existing prefix 10.0.88.0/24 from example-dcn"        }      ]    }  }"}
{"text": "Memory saturation on an AKS hybrid node\nThere have been incidents where workloads for cloud-native network functions (CNFs) can't start because of resource constraints on the AKS hybrid node that the CNF workload is scheduled on. It has happened on nodes that have Azure Arc pods that are consuming many compute resources. To reduce memory saturation, use effective monitoring tools and apply best practices.\nFor more information, see  Troubleshoot memory saturation in AKS clusters .\nTo access further details in the logs, see  Log Analytics workspace .\nIf you still have questions,  contact support ."}
{"text": "Troubleshoot isolation domain"}
{"text": "provisioning failures\nArticle  06/27/2023\nThe use of isolation domains allows for the establishment of connectivity between network functions at both Layer 2 and Layer 3 in the cluster and network fabric. As a result, workloads can communicate within and across racks.\nUse the information in this article to gather the data that you need to diagnose problems with isolation domain creation or management in Azure Operator Nexus by using the Azure CLI.\nFor more information, see  Configure L2 and L3 isolation domains by using a managed network fabric ."}
{"text": "Prerequisites\nInstall the latest version of the  appropriate Azure CLI extensions , including setting up the \nManagedNetworkFabric\n Azure CLI extension by using a WHL file.\nGather this information: Tenant ID Subscription ID Cluster name and resource group Network fabric controller and resource group Network fabric instances and resource group"}
{"text": "Configuration problems\nIf you're having general configuration problems, contact the network administrators within the organization for more details."}
{"text": "Error while enabling isolation domains\nThe fabric ASN value is no longer a mandatory value, which is defined based on the SKU that the payload uses. The peer ASN value can be anywhere from 0 through 65535.\nFor further instructions, see  Change the administrative state of an L3 isolation domain ."}
{"text": "Reserved range for VLAN IDs (Option A)\nWhen you're creating an isolation domain, VLAN IDs below 500 are reserved for infrastructure purposes and shouldn't be used. Instead, establish an external network with a VLAN ID higher than 500 on the partner end (PE) side to enable peering between the customer end (CE) and the PE (Option A peering).\nFor further instructions, see  Create external networks ."}
{"text": "Isolation domain stuck in a disabled state"}
{"text": "(Option A)\nIf you're using Option A, your isolation domain might get stuck in a disabled state when you try to create an external network. If you make any modifications to the IPv6 subnet payload, you must disable and enable the isolation domain to ensure successful provisioning."}
{"text": "Inability to ping an IP address\nIf you can't ping 107.xx.xx.x, the process of disabling and enabling the isolation domain can help you re-establish successful connectivity."}
{"text": "Terminal state provisioning error\nIf you get a terminal state provisioning error, the reason might be a failure in creating an external or internal network because the VLAN ID is already in use."}
{"text": "Isolation domain stuck in a deleting state\nIf your isolation domain is stuck in a deleting state for longer than normal, make sure that you first deleted one or two observed dependent consuming resources. That's a requirement before you try to delete an isolation domain."}
{"text": "Terminal provisioning state of Failed after a"}
{"text": "resource operation\nIf you get a terminal provisioning state of \nFailed\n after finishing a resource operation,\none potential explanation is a loss of access for the resource to retrieve secret or\ncertificate information from the key vault."}
{"text": "No network attached to the isolation domain\nBefore you enable isolation, it's necessary to create one or more internal or external networks.\nTo access further details in the logs, see  Log Analytics workspace .\nIf you still have questions,  contact support ."}
{"text": "Troubleshoot accessing a CSN-"}
{"text": "connected internet host name within an"}
{"text": "AKS hybrid cluster\nArticle  06/27/2023\nThis article outlines troubleshooting for scenarios where you're having problems reaching an internet host name that's part of the cloud services network (CSN) attached to an Azure Kubernetes Service (AKS) hybrid cluster."}
{"text": "Prerequisites\nGather this information: Subscription ID Cluster name and resource group AKS hybrid cluster name and resource group Become familiar with the procedures in  Connect to the AKS hybrid cluster ."}
{"text": "Common scenarios\nAssume that you're logged in to the jump server. You used SSH to access the AKS hybrid virtual machine (VM) by using the IP address of the worker nodes or control plane VMs. From the AKS hybrid VM, you can't reach any egress endpoints that you obtained when you created an AKS hybrid VM that uses the CSN.\nYou're encountering an error when trying to access the fully qualified domain name (FQDN) of internet host names:\nBash\ncurl -vk [http://www.ubuntu.com](http://www.ubuntu.com) \nOutput\n\*   Trying 192.xxx.xxx.xxx:xx...    \* TCP_NODELAY set    \*   Trying 2607:f8b0:xxxx:c17::xx:xx...    \* TCP_NODELAY set    \* Immediate connect fail for 2607:f8b0:xxxx:c17::xx: Network is  unreachable    \*   Trying 2607:f8b0:xxxx:c17::xx:xx...   \n\* TCP_NODELAY set    \* Immediate connect fail for 2607:f8b0:xxxx:c17::xx: Network is  unreachable    \*   Trying 2607:f8b0:xxxx:c17::xx:xx...    \* TCP_NODELAY set    \* Immediate connect fail for 2607:f8b0:xxxx:c17::xx: Network is  unreachable    \*   Trying 2607:f8b0:xxxx:c17::93:xx..."}
{"text": "Suggested solutions"}
{"text": "First attempt\nHere's the code for the first attempt at a workaround:\nBash\ncurl -x  "http_proxy=http://169.xxx.x.xx.xxxx"  -vk  "https://ubuntu.com"  \nhttps_proxy=<http://169.xxx.x.xx.xxxx> tdnf -y install openssh-clients \nHere's the output:\nOutput\nerror:**  \* Could not resolve proxy: http_proxy=http    \* Closing connection 0   \ncurl: (5) Could not resolve proxy: http_proxy=http  https_proxy=[http://169.xxx.x.xx.xxxx](http://169.xxx.x.xx.xxxx/) curl  -vk "https://ubuntu.com" shows connected but user get 403 Access denied."}
{"text": "Second attempt\nThe first option for another attempt is to set a proxy inline by using curl. The settings will go away after the command is complete.\nBash\ncurl -x  "http://169.xxx.x.xx.xxxx"  -vk  "https://ubuntu.com"  \nFor the following second option, the proxy setting is effective while the user remains in the shell:\nBash\nexport  https_proxy= "http://169.xxx.x.xx.xxxx"   export  HTTPS_PROXY= "http://169.xxx.x.xx.xxxx"   curl -vk <https://ubuntu.com> \nIf you're running an RPM package installation by using a shell script, be sure to set\nhttps_proxy\n locally inside a shell script explicitly. You can also try setting the proxy as an\noption inline on RPM with the \n--httpproxy\n and \n--httpport\n options.\nRPM has a proxy flag, which you must set:\nBash\nsudo rpm -- import <https://aglet.packages.cloudpassage.com/cloudpassage.packages.key>  --httpproxy 169.xxx.x.xx  --httpport 3128 \n  Note\nIf you set these flags system wide, they might lose their ability to run kubectl locally. Set them inline within the script first to help minimize the effects.\nFor more information, see the  Xmodulo article about installing RPM packages behind a proxy ."}
{"text": "Troubleshoot VM problems after"}
{"text": "cordoning off and restarting bare-metal"}
{"text": "machines\nArticle  06/27/2023\nFollow this troubleshooting guide after you cordon off and restart bare metal machine (BMMs) for Azure Operator Nexus if:\nYou encounter virtual machines (VMs) with an error status on the Azure portal after an upgrade. Traditional methods such as powering off and restarting the VMs don't work."}
{"text": "Prerequisites\nInstall the latest version of the  appropriate Azure CLI extensions . Familiarize yourself with the capabilities referenced in this article by reviewing the BMM actions . Gather the following information: Subscription ID Cluster name and resource group Virtual machine name Make sure that the virtual machine has a provisioning state of  Succeeded  and a power state of  On ."}
{"text": "Symptoms\nDuring BMM restart or upgrade testing, the VM is in an error state. After the restart, or after powering off and powering back on, the BMM is no longer cordoned off. Although the virtual network function (VNF) successfully came up, established its BGP sessions, and started routing traffic, the VM status in the portal consistently shows an error. Despite this discrepancy, the application remains healthy and continues to function properly. The portal actions and Azure CLI APIs for the NC VM resource itself are no longer achieving the intent. For example: Selecting  Power Off  (or using the Azure CLI to power off) doesn't actually power off the VM anymore.\nSelecting  Restart (or using the Azure CLI to restart)doesn't actually restart the VM anymore. The platform has lost the ability to manage this VM resource."}
{"text": ""}
{"text": "Troubleshooting steps\n1. Gather the VM details and validate the VM status in the portal. Ensure that the VM isn't connected and is powered off. 2. Validate the status of the virtual machine before and after restart or upgrade. 3. Check the BGP session and traffic flow before and after restart or upgrade of the VNF.\nFor more troubleshooting, see  Troubleshoot Azure Operator Nexus server problems ."}
{"text": "Procedure\nThere's a problem with the status update on the VM after the upgrade. Although the upgrade and the VM itself are fine, the status is being reported incorrectly, leading to actions being ignored.\nPerform the following Azure CLI update on any affected VMs with dummy tag values (the use of \ntag1\n and \nvalue1\n):\nBash\n   az networkcloud virtualmachine update --ids <VMresourceId>--tags  tag1=value1 \nThis process restores the VM to an online state."}
{"text": ""}
{"text": "Manage emergency access to a bare"}
{"text": "metal machine using the"}
{"text": "az"}
{"text": "networkcloud cluster"}
{"text": "baremetalmachinekeyset\nArticle  06/16/2023\n  Caution\nPlease note this process is used in emergency situations when all other troubleshooting options using Azure have been exhausted. SSH access to these bare metal machines is restricted to users managed via this method from the specified jump host list.\nThere are rare situations where a user needs to investigate & resolve issues with a bare metal machine and all other ways have been exhausted via Azure. Azure Operator Nexus provides the \naz networkcloud cluster baremetalmachinekeyset\n command so users can\nmanage SSH access to these bare metal machines.\nWhen the command runs, it executes on each bare metal machine in the Cluster. If a bare metal machine is unavailable or powered off at the time of command execution, the status of the command reflects which bare metal machines couldn't have the command executed. There's a reconciliation process that runs periodically that retries the command on any bare metal machine that wasn't available at the time of the original command. Multiple commands execute in the order received.\nThere's no limit to the number of users in a group.\n  Caution\nNotes for jump host IP addresses\nThe keyset create/update process adds the jump host IP addresses to the IP tables for the Cluster. The process adds these addresses to IP tables and restricts SSH access to only those IPs. It's important to specify the Cluster facing IP addresses for the jump hosts. These IP addresses may be different than the public facing IP address used to access the jump host.\nOnce added, users are able to access bare metal machines from any specified jump host IP including a jump host IP defined in another bare metal machine keyset group. Existing SSH access remains when adding the first bare metal machine keyset. However, the keyset command limits an existing user's SSH access to the specified jump host IPs in the keyset commands."}
{"text": "Prerequisites\nInstall the latest version of the  appropriate CLI extensions . The on-premises Cluster must have connectivity to Azure. Get the Resource Group name for the \nCluster\n resource.\nThe process applies keysets to all running bare metal machines. The added users must be part of an Azure Active Directory (Azure AD) group. For more information, see  How to Manage Groups . To restrict access for managing keysets, create a custom role. For more information, see  Azure Custom Roles . In this instance, add or exclude permissions for \nMicrosoft.NetworkCloud/clusters/bareMetalMachineKeySets\n. The options are\n/read\n, \n/write\n, and \n/delete\n.\n  Note\nWhen bare metal machine access is created, modified or deleted via the commands described in this article, a background process delivers those changes to the machines. This process is paused during Operator Nexus software upgrades. If an upgrade is known to be in progress, you can use the \n--no-wait\n option with the\ncommand to prevent the command prompt from waiting for the process to complete."}
{"text": "Creating a bare metal machine keyset\nThe \nbaremetalmachinekeyset create\n command creates SSH access to the bare metal\nmachine in a Cluster for a group of users.\nThe command syntax is:\nAzure CLI\naz networkcloud cluster baremetalmachinekeyset create  \     --name   <bare metal machine Keyset Name>  \ \n   --extended-location   name= <Extended Location ARM ID>  \       type= "CustomLocation"  \     --location   <Azure Region>  \     --azure-group-id   <Azure AAD Group ID>  \     --expiration   <Expiration Timestamp>  \     --jump-hosts-allowed   <List of jump server IP addresses>  \     --os-group-name   <Name of the Operating System Group>  \     --privilege-level   <"Standard" or "Superuser">  \     --user-list   '[{"description":"<User List Description>","azureUserName":" <User Name>",\      "sshPublicKey":{"keyData":"<SSH Public Key>"}}]'  \     --tags  key1= <Key Value>  key2= <Key Value>  \     --cluster-name   <Cluster Name>  \     --resource-group   <Resource Group>"}
{"text": "Create Arguments\nAzure CLI\n   --azure-group-id                              [Required]  : The object ID of  Azure Active Directory                                                             group that all  users in the list must                                                             be in for access  to be granted. Users                                                             that are not in  the group do not have                                                             access.     --bare-metal-machine-key-set-name   --name   -n   [Required]  : The name of the  bare metal machine key                                                             set.     --cluster-name                                [Required]  : The name of the  cluster.     --expiration                                  [Required]  : The date and time  after which the users                                                             in this key set  are removed from                                                             the bare metal  machines. Format is:                                                              "YYYY-MM- DDTHH:MM:SS.000Z"      --extended-location                           [Required]  : The extended  location of the cluster                                                             associated with  the resource.      Usage:  --extended-location   name= XX  type= XX        name: Required. The resource ID of the extended location on which the  resource is created.        type: Required. The extended location type:  "CustomLocation" .     --jump-hosts-allowed                          [Required]  : The list of IP  addresses of jump hosts                                                             with management \nnetwork access from                                                             which a login is  be allowed for the                                                             users. Supports  IPv4 or IPv6 addresses.     --privilege-level                             [Required]  : The access level  allowed for the users                                                             in this key set.   Allowed values:                                                              "Standard"  or  "Superuser" .     --resource-group   -g                           [Required]  : Name of resource  group. Optional if                                                             configuring the  default group using `az                                                             configure  -- defaults   group= <name> `.    --user-list                                   [Required]  : The unique list  of permitted users.      Usage:  --user-list  azure -user-name =XX  description= XX  key-data= XX        azure -user-name : Required. User name used to login to the server.        description: The free -form  description for this user.        key -data : Required. The public ssh key of the user. \n      Multiple users can be specified by using more than one  --user-list   argument.     --os-group-name                                         : The name of the  group that users are assigned                                                             to on the  operating system of the machines.    --tags                                                  : Space -separated   tags:  key[=value]                                                               [key[=value]   ...] . Use  ''  to clear                                                             existing tags.     --location   -l                                           : Azure Region.  Values from: `az account                                                             list -locations `.  You can configure the                                                             default location  using `az configure                                                              --defaults   location= <location> `.     --no-wait                                               : Do not wait for  the long -running                                                              operation to  finish."}
{"text": "Global Azure CLI arguments (applicable to all commands)\nAzure CLI\n   --debug                                                 : Increase logging  verbosity to show all                                                             debug logs.     --help   -h                                               : Show this help  message and exit.     --only-show-errors                                      : Only show errors,  suppressing warnings.     --output   -o                                             : Output format.   Allowed values: json,                                                             jsonc, none,  table, tsv, yaml, yamlc.                                                             Default: json.     --query                                                 : JMESPath query  string. See                                                              http://jmespath.org/ for more                                                             information and  examples.     --subscription                                [Required]  : Name or ID of  subscription. Optional if                                                             configuring the  default subscription                                                             using `az account  set  -s  NAME_OR_ID`.     --verbose                                               : Increase logging  verbosity. Use  --debug                                                              for full debug  logs. \nThis example creates a new keyset with two users that have standard access from two jump hosts.\nAzure CLI\naz networkcloud cluster baremetalmachinekeyset create  \     --name   "bareMetalMachineKeySetName"  \     --extended-location   name= "/subscriptions/subscriptionId/resourceGroups/resourceGroupName/provide rs/Microsoft.ExtendedLocation/customLocations/clusterExtendedLocationName"  \       type= "CustomLocation"  \     --location   "location"  \     --azure-group-id   "f110271b-XXXX-4163-9b99-214d91660f0e"  \     --expiration   "2022-12-31T23:59:59.008Z"  \     --jump-hosts-allowed   "192.0.2.1"   "192.0.2.5"  \     --os-group-name   "standardAccessGroup"  \     --privilege-level   "Standard"  \     --user-list   '[{"description":"Needs access for troubleshooting as a part  of the support team","azureUserName":"userABC", "sshPublicKey": {"keyData":"ssh-rsa   AAtsE3njSONzDYRIZv/WLjVuMfrUSByHp+jfaaOLHTIIB4fJvo6dQUZxE20w2iDHV3tEkmnTo84e ba97VMueQD6OzJPEyWZMRpz8UYWOd0IXeRqiFu1lawNblZhwNT/ojNZfpB3af/YDzwQCZgTcTRyN NhL4o/blKUmug0daSsSXISTRnIDpcf5qytjs1XoyYyJMvzLL59mhAyb3p/cD+Y3/s3WhAx+l0XOK\npzXnblrv9d3q4c2tWmm/SyFqthaqd0= admin@vm"}},\    {"description":"Needs access for troubleshooting as a part of the support  team","azureUserName":"userXYZ","sshPublicKey":{"keyData":"ssh-rsa   AAtsE3njSONzDYRIZv/WLjVuMfrUSByHp+jfaaOLHTIIB4fJvo6dQUZxE20w2iDHV3tEkmnTo84e ba97VMueQD6OzJPEyWZMRpz8UYWOd0IXeRqiFu1lawNblZhwNT/ojNZfpB3af/YDzwQCZgTcTRyN NhL4o/blKUmug0daSsSXTSTRnIDpcf5qytjs1XoyYyJMvzLL59mhAyb3p/cD+Y3/s3WhAx+l0XOK pzXnblrv9d3q4c2tWmm/SyFqthaqd0= admin@vm"}}]'  \     --tags  key1= "myvalue1"  key2= "myvalue2"  \     --cluster-name   "clusterName"      --resource-group   "resourceGroupName"  \nFor assistance in creating the \n--user-list\n structure, see  Azure CLI Shorthand ."}
{"text": "Deleting a bare metal machine keyset\nThe \nbaremetalmachinekeyset delete\n command removes SSH access to the bare metal\nmachine for a group of users. All members of the group no longer have SSH access to any of the bare metal machines in the Cluster.\nThe command syntax is:\nAzure CLI\naz networkcloud cluster baremetalmachinekeyset delete  \     --name   <bare metal machine Keyset Name>  \     --cluster-name   <Cluster Name>  \     --resource-group   <Resource Group Name>"}
{"text": "Delete Arguments\nAzure CLI\n     --bare-metal-machine-key-set-name   --name   -n   [Required]  : The name of the  bare metal machine key set to be                                                               deleted.       --cluster-name                                [Required]  : The name of the  cluster.       --resource-group   -g                           [Required]  : Name of  resource group. Optional if configuring the                                                               default group  using `az configure  --defaults                                                                 group= <name> `.       --no-wait                                               : Do not wait for  the long -running  operation to                                                               finish.       --yes   -y                                                : Do not prompt  for confirmation. \nThis example removes the "bareMetalMachineKeysetName" keyset group in the "clusterName" Cluster.\nAzure CLI\naz networkcloud cluster baremetalmachinekeyset delete  \     --name   "bareMetalMachineKeySetName"  \     --cluster-name   "clusterName"  \     --resource-group   "resourceGroupName""}
{"text": "Updating a Bare Metal Machine Keyset\nThe \nbaremetalmachinekeyset update\n command allows users to make changes to an\nexisting keyset group.\nThe command syntax is:\nAzure CLI\naz networkcloud cluster baremetalmachinekeyset update  \     --name   <bare metal machine Keyset Name>  \     --jump-hosts-allowed   <List of jump server IP addresses>  \     --privilege-level   <"Standard" or "Superuser">  \     --user-list   '[{"description":"<User List Description>","azureUserName":" <User Name>",\     "sshPublicKey":{"keyData":"<SSH Public Key>"}}]'  \     --tags  key1= <Key Value>  key2= <Key Value>  \     --cluster-name   <Cluster Name>  \     --resource-group   <Resource Group>"}
{"text": "Update Arguments\nAzure CLI\n   --bare-metal-machine-key-set-name   --name   -n   [Required]  : The name of the  bare metal machine key set.     --cluster-name                                [Required]  : The name of the  cluster.     --expiration                                            : The date and time  after which the users                                                             in this key set  are removed from                                                             the bare metal  machines. Format is:                                                              "YYYY-MM- DDTHH:MM:SS.000Z"      --jump-hosts-allowed                                    : The list of IP \naddresses of jump hosts                                                             with management  network access from                                                             which a login is  allowed for the                                                             users. Supports  IPv4 or IPv6 addresses.     --privilege-level                                       : The access level  allowed for the users                                                             in this key set.   Allowed values:                                                              "Standard"  or  "Superuser" .     --user-list                                             : The unique list  of permitted users.      Usage:  --user-list  azure -user-name =XX  description= XX  key-data= XX        azure -user-name : Required. User name used to login to the server.        description: The free -form  description for this user.        key -data : Required. The public SSH key of the user. \n      Multiple users can be specified by using more than one  --user-list   argument.     --resource-group   -g                           [Required]  : Name of resource  group. Optional if                                                             configuring the  default group using `az                                                             configure  -- defaults   group= <name> `.    --tags                                                  : Space -separated   tags:  key[=value]                                                               [key[=value]   ...] . Use  ''  to clear                                                             existing tags.     --no-wait                                               : Do not wait for  the long -running                                                              operation to  finish. \nThis example adds two new users to the "baremetalMachineKeySetName" group and changes the expiry time for the group.\nAzure CLI\naz networkcloud cluster baremetalmachinekeyset update  \     --name   "bareMetalMachineKeySetName"  \    --expiration   "2023-12-31T23:59:59.008Z"  \     --user-list   '[{"description":"Needs access for troubleshooting as a part  of the support team",\    "azureUserName":"userABC","sshPublicKey":{"keyData":"ssh-rsa   AAtsE3njSONzDYRIZv/WLjVuMfrUSByHp+jfaaOLHTIIB4fJvo6dQUZxE20w2iDHV3tEkmnTo84e ba97VMueQD6OzJPEyWZMRpz8UYWOd0IXeRqiFu1lawNblZhwNT/ojNZfpB3af/YDzwQCZgTcTRyN NhL4o/blKUmug0daSsSXISTRnIDpcf5qytjs1XoyYyJMvzLL59mhAyb3p/cD+Y3/s3WhAx+l0XOK pzXnblrv9d3q4c2tWmm/SyFqthaqd0= admin@vm"}},\ \n  {"description":"Needs access for troubleshooting as a part of the support  team",\    "azureUserName":"userXYZ","sshPublicKey":{"keyData":"ssh-rsa   AAtsE3njSONzDYRIZv/WLjVuMfrUSByHp+jfaaOLHTIIB4fJvo6dQUZxE20w2iDHV3tEkmnTo84e ba97VMueQD6OzJPEyWZMRpz8UYWOd0IXeRqiFu1lawNblZhwNT/ojNZfpB3af/YDzwQCZgTcTRyN NhL4o/blKUmug0daSsSXTSTRnIDpcf5qytjs1XoyYyJMvzLL59mhAyb3p/cD+Y3/s3WhAx+l0XOK pzXnblrv9d3q4c2tWmm/SyFqthaqd0= admin@vm"}}]'  \      --cluster-name   "clusterName"  \     --resource-group   "resourceGroupName""}
{"text": "Listing Bare Metal Machine Keysets\nThe \nbaremetalmachinekeyset list\n command allows users to see the existing keyset\ngroups in a Cluster.\nThe command syntax is:\nAzure CLI\naz networkcloud cluster baremetalmachinekeyset list  \     --cluster-name   <Cluster Name>  \     --resource-group   <Resource Group>"}
{"text": "List Arguments\nAzure CLI\n   --cluster-name                                [Required]  : The name of the  cluster.     --resource-group   -g                           [Required]  : Name of resource  group. Optional if                                                             configuring the  default group using `az                                                             configure  -- defaults   group= <name> `."}
{"text": "Show Bare Metal Machine Keyset Details\nThe \nbaremetalmachinekeyset show\n command allows users to see the details of an\nexisting keyset group in a Cluster.\nThe command syntax is:\nAzure CLI\naz networkcloud cluster baremetalmachinekeyset show  \     --cluster-name   <Cluster Name>  \     --resource-group   <Resource Group>"}
{"text": "Show Arguments\nAzure CLI\n   --bare-metal-machine-key-set-name   --name   -n   [Required]  : The name of the  bare metal machine key                                                             set.     --cluster-name                                [Required]  : The name of the  cluster.     --resource-group   -g                           [Required]  : Name of resource  group. You can                                                             configure the  default group using `az                                                             configure  -- defaults   group= <name> `."}
{"text": "Manage emergency access to a bare"}
{"text": "metal machine using the"}
{"text": "az"}
{"text": "networkcloud cluster bmckeyset\nArticle  06/16/2023\n  Caution\nPlease note this process is used in emergency situations when all other troubleshooting options via Azure have been exhausted. SSH access to these bare metal machines is restricted to users managed via this method from the specified jump host list.\nThere are rare situations where a user needs to investigate & resolve issues with a bare metal machine and all other ways using Azure have been exhausted. Operator Nexus provides the \naz networkcloud cluster bmckeyset\n command so users can manage SSH\naccess to the baseboard management controller (BMC) on these bare metal machines.\nWhen the command runs, it executes on each bare metal machine in the Cluster. If a bare metal machine is unavailable or powered off at the time of command execution, the status of the command reflects which bare metal machines couldn't have the command executed. There's a reconciliation process that runs periodically that retries the command on any bare metal machine that wasn't available at the time of the original command. Multiple commands execute in the order received.\nThe BMCs support a maximum number of 12 users. Users are defined on a per Cluster basis and applied to each bare metal machine. Attempts to add more than 12 users results in an error. Delete a user before adding another one when 12 already exists."}
{"text": "Prerequisites\nInstall the latest version of the  appropriate CLI extensions . The on-premises Cluster must have connectivity to Azure. Get the Resource Group name for the \nCluster\n resource.\nThe process applies keysets to all running bare metal machines. The users added must be part of an Azure Active Directory (Azure AD) group. For more information, see  How to Manage Groups . To restrict access for managing keysets, create a custom role. For more information, see  Azure Custom Roles . In this instance, add or exclude permissions\nfor \nMicrosoft.NetworkCloud/clusters/bmcKeySets\n. The options are \n/read\n, \n/write\n,\nand \n/delete\n.\n  Note\nWhen BMC access is created, modified or deleted via the commands described in this article, a background process delivers those changes to the machines. This process is paused during Operator Nexus software upgrades. If an upgrade is known to be in progress, you can use the \n--no-wait\n option with the command to\nprevent the command prompt from waiting for the process to complete."}
{"text": "Creating a BMC keyset\nThe \nbmckeyset create\n command creates SSH access to the bare metal machine in a\nCluster for a group of users.\nThe command syntax is:\nAzure CLI\naz networkcloud cluster bmckeyset create  \     --name   <BMC Keyset Name>  \     --extended-location   name= <Extended Location ARM ID>  \       type= "CustomLocation"  \     --location   <Azure Region>  \     --azure-group-id   <Azure AAD Group ID>  \     --expiration   <Expiration Timestamp>  \     --jump-hosts-allowed   <List of jump server IP addresses>  \     --privilege-level   <"Administrator" or "ReadOnly">  \     --user-list   '[{"description":"<User description>","azureUserName":"<User  Name>", \     "sshPublicKey":{"keyData":"<SSH Public Key>"}}]'  \     --tags  key1= <Key Value>  key2= <Key Value>  \     --cluster-name   <Cluster Name>  \     --resource-group   <Resource Group Name>"}
{"text": "Create Arguments\nAzure CLI\n   --azure-group-id                              [Required]  : The object ID of  Azure Active Directory                                                             group that all  users in the list must                                                             be in for access \nto be granted. Users                                                             that are not in  the group do not have                                                             access.     --bmc-key-set-name   --name   -n                  [Required]  : The name of the  BMC key set.     --cluster-name                                [Required]  : The name of the  cluster.     --expiration                                  [Required]  : The date and time  after which the users                                                             in this key set  are removed from                                                             the BMCs. The  limit is up to 1 year from creation.                                                             Format is  "YYYY- MM-DDTHH:MM:SS.000Z"      --extended-location                           [Required]  : The extended  location of the cluster                                                             associated with  the resource.      Usage:  --extended-location   name= XX  type= XX        name: Required. The resource ID of the extended location on which the  resource is created.        type: Required. The extended location type:  "CustomLocation" .     --privilege-level                             [Required]  : The access level  allowed for the users                                                             in this key set.   Allowed values:                                                              "Administrator"   or  "ReadOnly" .     --resource-group   -g                           [Required]  : Name of resource  group. Optional if                                                             configuring the  default group using `az                                                             configure  -- defaults   group= <name> `.    --user-list                                   [Required]  : The unique list  of permitted users.      Usage:  --user-list  azure -user-name =XX  description= XX  key-data= XX        azure -user-name : Required. User name used to login to the server.        description: The free -form  description for this user.        key -data : Required. The public ssh key of the user. \n      Multiple users can be specified by using more than one  --user-list   argument.     --tags                                                  : Space -separated   tags:  key[=value]                                                               [key[=value]   ...] . Use  ''  to clear                                                             existing tags.     --location   -l                                           : Azure Region.  Values from: `az account                                                             list -locations `.  You can configure the                                                             default location \nusing `az configure                                                              --defaults   location= <location> `.     --no-wait                                               : Do not wait for  the long -running                                                              operation to  finish."}
{"text": "Global Azure CLI arguments (applicable to all commands)\nAzure CLI\n   --debug                                                 : Increase logging  verbosity to show all                                                             debug logs.     --help   -h                                               : Show this help  message and exit.     --only-show-errors                                      : Only show errors,  suppressing warnings.     --output   -o                                             : Output format.   Allowed values: json,                                                             jsonc, none,  table, tsv, yaml, yamlc.                                                             Default: json.     --query                                                 : JMESPath query  string. See                                                              http://jmespath.org/ for more                                                             information and  examples.     --subscription                                [Required]  : Name or ID of  subscription. Optional if                                                             configuring the  default subscription                                                             using `az account  set  -s  NAME_OR_ID`.     --verbose                                               : Increase logging  verbosity. Use  --debug                                                              for full debug  logs. \nThis example creates a new keyset with two users that have standard access from two jump hosts.\nAzure CLI\naz networkcloud cluster bmckeyset create  \     --name   "bmcKeySetName"  \     --extended-location   name= "/subscriptions/subscriptionId/resourceGroups/resourceGroupName/provide\nrs/Microsoft.ExtendedLocation/customLocations/clusterExtendedLocationName"  \       type= "CustomLocation"  \     --location   "location"  \     --azure-group-id   "f110271b-XXXX-4163-9b99-214d91660f0e"  \     --expiration   "2023-12-31T23:59:59.008Z"  \     --privilege-level   "Standard"  \     --user-list   '[{"description":"Needs access for troubleshooting as a part  of the support team",\    "azureUserName":"userABC","sshPublicKey":{"keyData":"ssh-rsa   AAtsE3njSONzDYRIZv/WLjVuMfrUSByHp+jfaaOLHTIIB4fJvo6dQUZxE20w2iDHV3tEkmnTo84e ba97VMueQD6OzJPEyWZMRpz8UYWOd0IXeRqiFu1lawNblZhwNT/ojNZfpB3af/YDzwQCZgTcTRyN NhL4o/blKUmug0daSsSXISTRnIDpcf5qytjs1XoyYyJMvzLL59mhAyb3p/cD+Y3/s3WhAx+l0XOK pzXnblrv9d3q4c2tWmm/SyFqthaqd0= admin@vm"}},\    {"description":"Needs access for troubleshooting as a part of the support  team",\    "azureUserName":"userXYZ","sshPublicKey":{"keyData":"ssh-rsa   AAtsE3njSONzDYRIZv/WLjVuMfrUSByHp+jfaaOLHTIIB4fJvo6dQUZxE20w2iDHV3tEkmnTo84e ba97VMueQD6OzJPEyWZMRpz8UYWOd0IXeRqiFu1lawNblZhwNT/ojNZfpB3af/YDzwQCZgTcTRyN NhL4o/blKUmug0daSsSXTSTRnIDpcf5qytjs1XoyYyJMvzLL59mhAyb3p/cD+Y3/s3WhAx+l0XOK pzXnblrv9d3q4c2tWmm/SyFqthaqd0= admin@vm"}}]'  \     --tags  key1= "myvalue1"  key2= "myvalue2"  \     --cluster-name   "clusterName"  \     --resource-group   "resourceGroupName"  \nFor assistance in creating the \n--user-list\n structure, see  Azure CLI Shorthand ."}
{"text": "Deleting a BMC keyset\nThe \nbmckeyset delete\n command removes SSH access to the BMC for a group of users.\nAll members of the group lose SSH access to any of the BMCs in the Cluster.\nThe command syntax is:\nAzure CLI\naz networkcloud cluster bmckeyset delete  \     --name   <BMC Keyset Name>  \     --cluster-name   <Cluster Name>  \     --resource-group   <Resource Group Name>  \"}
{"text": "Delete Arguments\nAzure CLI\n   --bmc-key-set-name   --name   -n                  [Required]  : The name of the  BMC key set to be deleted.     --cluster-name                                [Required]  : The name of the  cluster. \n   --resource-group   -g                           [Required]  : Name of resource  group. Optional if configuring the                                                             default group  using `az configure  --defaults                                                               group= <name> `.     --no-wait                                               : Do not wait for  the long -running  operation to finish.     --yes   -y                                                : Do not prompt for  confirmation. \nThis example removes the "bmcKeysetName" keyset group in the "clusterName" Cluster.\nAzure CLI\naz networkcloud cluster bmckeyset delete  \     --name   "bmcKeySetName"  \     --cluster-name   "clusterName"  \     --resource-group   "resourceGroupName"  \"}
{"text": "Updating a BMC Keyset\nThe \nbmckeyset update\n command allows users to make changes to an existing keyset\ngroup.\nThe command syntax is:\nAzure CLI\naz networkcloud cluster bmckeyset update  \     --name   <BMC Keyset Name>  \     --jump-hosts-allowed   <List of jump server IP addresses>  \     --privilege-level   <"Standard" or "Superuser">  \     --user-list   '[{"description":"<User description>",\      "azureUserName":"<UserName>", \      "sshPublicKey":{"keyData":"<SSH Public Key>"}}]'  \     --tags  key1= <Key Value>  key2= <Key Value>  \     --cluster-name   <Cluster Name>  \     --resource-group   <Resource Group Name>"}
{"text": "Update Arguments\nAzure CLI\n   --bmc-key-set-name   --name   -n                  [Required]  : The name of the  BMC key set.     --cluster-name                                [Required]  : The name of the  cluster. \n   --expiration                                            : The date and time  after which the users                                                             in this key set  are removed from                                                             the BMCs. Format  is:                                                              "YYYY-MM- DDTHH:MM:SS.000Z"      --jump-hosts-allowed                                    : The list of IP  addresses of jump hosts                                                             with management  network access from                                                             which a login is  allowed for the                                                             users. Supports  IPv4 or IPv6 addresses.     --privilege-level                                       : The access level  allowed for the users                                                             in this key set.   Allowed values:                                                              "Administrator"   or  "ReadOnly" .     --user-list                                             : The unique list  of permitted users.      Usage:  --user-list  azure -user-name =XX  description= XX  key-data= XX        azure -user-name : Required. User name used to login to the server.        description: The free -form  description for this user.        key -data : Required. The public SSH key of the user. \n      Multiple users can be specified by using more than one  --user-list   argument.     --resource-group   -g                           [Required]  : Name of resource  group. Optional if                                                             configuring the  default group using `az                                                             configure  -- defaults   group= <name> `.    --tags                                                  : Space -separated   tags:  key[=value]                                                               [key[=value]   ...] . Use  ''  to clear                                                             existing tags.     --no-wait                                               : Do not wait for  the long -running                                                              operation to  finish. \nThis example adds two new users to the "bmcKeySetName" group and changes the expiry time for the group.\nAzure CLI\naz networkcloud cluster bmckeyset update  \     --name   "bmcKeySetName"  \     --expiration   "2023-12-31T23:59:59.008Z"  \     --user-list   '[{"description":"Needs access for troubleshooting as a part  of the support team",\    "azureUserName":"userDEF","sshPublicKey":{"keyData":"ssh-rsa   AAtsE3njSONzDYRIZv/WLjVuMfrUSByHp+jfaaOLHTIIB4fJvo6dQUZxE20w2iDHV3tEkmnTo84e ba97VMueQD6OzJPEyWZMRpz8UYWOd0IXeRqiFu1lawNblZhwNT/ojNZfpB3af/YDzwQCZgTcTRyN NhL4o/blKUmug0daSsSXISTRnIDpcf5qytjs1XoyYyJMvzLL59mhAyb3p/cD+Y3/s3WhAx+l0XOK pzXnblrv9d3q4c2tWmm/SyFqthaqd0= admin@vm"}}]\    --cluster-name "clusterName" \    --resource-group "resourceGroupName""}
{"text": "Listing BMC Keysets\nThe \nbmckeyset list\n command allows users to see the existing keyset groups in a\nCluster.\nThe command syntax is:\nAzure CLI\naz networkcloud cluster bmckeyset list  \     --cluster-name   <Cluster Name>  \     --resource-group   <Resource Group Name>"}
{"text": "List Arguments\nAzure CLI\n   --cluster-name                                [Required]  : The name of the  cluster.     --resource-group   -g                           [Required]  : Name of resource  group. Optional if                                                               configuring the  default group using `az                                                               configure  -- defaults   group= <name> `."}
{"text": "Show BMC Keyset Details\nThe \nbmckeyset show\n command allows users to see the details of an existing keyset\ngroup in a Cluster.\nThe command syntax is:\nAzure CLI\naz networkcloud cluster bmckeyset show  \     --cluster-name   <Cluster Name>  \     --resource-group   <Resource Group Name>"}
{"text": "Show Arguments\nAzure CLI\n   --bmc-key-set-name   --name   -n                  [Required]  : The name of the  BMC key set.     --cluster-name                                [Required]  : The name of the  cluster.     --resource-group   -g                           [Required]  : Name of resource  group. You can                                                             configure the  default group using `az                                                             configure  -- defaults   group= <name> `."}
{"text": "Manage lifecycle of Bare Metal"}
{"text": "Machines\nArticle  07/10/2023\nThis article describes how to perform lifecycle management operations on Bare Metal Machines (BMM). These steps should be used for troubleshooting purposes to recover from failures or when taking maintenance actions. The commands to manage the lifecycle of the BMM include:\nPower off the BMM Start the BMM Restart the BMM Make the BMM unschedulable or schedulable Reimage the BMM Replace the BMM"}
{"text": "Prerequisites\n1. Install the latest version of the  appropriate CLI extensions 2. Get the name of the resource group for the BMM 3. Get the name of the bare metal machine that requires a lifecycle management operation\npoweredState\n  set to \nOn\n  and \nreadyState\n4. Ensure that the target bare metal machine \nset to \nTrue\na. This prerequisite is not applicable for the \nstart\n  command\n  Caution\nActions against management servers should not be run without consultation with Microsoft support personnel. Doing so could affect the integrity of the Operator Nexus Cluster."}
{"text": "Power off the BMM\nThis command will \npower-off\n  the specified \nbareMetalMachineName\n .\nAzure CLI\naz networkcloud baremetalmachine power-off  \    --name   "bareMetalMachineName"   \    --resource-group   "resourceGroupName""}
{"text": "Start the BMM\nThis command will \nstart\n  the specified \nbareMetalMachineName\n .\nAzure CLI\naz networkcloud baremetalmachine start  \    --name   "bareMetalMachineName"  \    --resource-group   "resourceGroupName""}
{"text": "Restart the BMM\nrestart\n  the specified \nbareMetalMachineName\n . This command will \nAzure CLI\naz networkcloud baremetalmachine restart  \    --name   "bareMetalMachineName"  \    --resource-group   "resourceGroupName""}
{"text": "Make a BMM unschedulable (cordon)\nYou can make a BMM unschedulable by executing the  cordon  command. On the execution of the \ncordon\n  command, Operator Nexus workloads aren't scheduled on the\ncordoned\n  BMM results BMM when cordon is set; any attempt to create a workload on a \nin the workload being set to \npending\n  state. Existing workloads continue to run. The\ncordon command supports an \nevacuate\n  parameter with the default \nFalse\n  value. On\ncordon\n  command, with the value \nTrue\n  for the \nevacuate\n  parameter, the executing the \nworkloads that are running on the BMM are \nstopped\n  and the BMM is set to \npending\nstate.\nAzure CLI\naz networkcloud baremetalmachine cordon  \    --evacuate   "True"  \\n   --name   "bareMetalMachineName"  \    --resource-group   "resourceGroupName"\nThe \nevacuate "True"\n  removes workloads from that node while \nevacuate "False"\n  only\nprevents the scheduling of new workloads."}
{"text": "Make a BMM schedulable (uncordon)\nschedulable\n  (usable) by executing the  You can make a BMM  uncordon  command. All\nworkloads in a \npending\n  state on the BMM are \nrestarted\n  when the BMM is \nuncordoned\n .\nAzure CLI\naz networkcloud baremetalmachine uncordon  \    --name   "bareMetalMachineName"  \    --resource-group   "resourceGroupName""}
{"text": "Reimage a BMM\nYou can restore the runtime version on a BMM by executing \nreimage\n  command. This\nprocess  redeploys  the runtime image on the target BMM and executes the steps to rejoin the cluster with the same identifiers. This action doesn't impact the tenant workload files on this BMM. As a best practice, make sure the BMM's workloads are\nevacuate "True"\n , prior to executing the drained using the  cordon  command, with \nreimage\n  command.\nAzure CLI\naz networkcloud baremetalmachine reimage  \    -name   "bareMetalMachineName"   \    --resource-group   "resourceGroupName""}
{"text": "Replace BMM\nUse \nReplace BMM\n  command when a server has encountered hardware issues requiring a\ncomplete or partial hardware replacement. After replacement of components such as motherboard or NIC replacement, the MAC address of BMM will change, however the IDrac IP address and hostname will remain the same.\n  Warning\nRunning more than one baremetalmachine replace command at the same time will leave servers in a nonworking state. Make sure one replace has fully completed before starting another one. In a future release, we plan to either add the ability to replace multiple servers at once or have the command return an error when attempting to do so.\nAzure CLI\naz networkcloud baremetalmachine replace  \    --name   "bareMetalMachineName"  \    --resource-group   "resourceGroupName"  \    --bmc-credentials   password= "{password}"   username= "{user}"  \    --bmc-mac-address   "00:00:4f:00:57:ad"  \    --boot-mac-address   "00:00:4e:00:58:af"  \    --machine-name   "name"  \    --serial-number   "BM1219XXX""}
{"text": "Troubleshoot BMM issues using the"}
{"text": "az"}
{"text": "networkcloud baremetalmachine run-read-"}
{"text": "command\nArticle  05/01/2023\nThere may be situations where a user needs to investigate & resolve issues with an on- premises BMM. Operator Nexus provides the \naz networkcloud baremetalmachine run-\nread-command\n so users can run a curated list of read only commands to get information\nfrom a BMM.\nThe command execution produces an output file containing the results that can be found in the Cluster Manager's Storage account."}
{"text": "Prerequisites\n1. Install the latest version of the  appropriate CLI extensions 2. Ensure that the target BMM must have its \npoweredState\n set to \nOn\n and have its\nreadyState\n set to \nTrue\n3. Get the Resource group name that you created for \nCluster\n resource"}
{"text": "Executing a run-read command\nThe run-read command executes a read-only command on the specified BMM.\nThe current list of supported commands are:\ntraceroute\nping\narp\ntcpdump\nbrctl show\ndmidecode\nhost\nip link show\nip address show\nip maddress show\nip route show\njournalctl\nkubectl logs\nkubectl describe\nkubectl get\nkubectl api-resources\nkubectl api-versions\nuname\nuptime\nfdisk -l\nhostname\nifconfig -a\nifconfig -s\nmount\nss\nulimit -a\nThe command syntax is:\nAzure CLI\naz networkcloud baremetalmachine run-read-command  --name   "<machine-name>"        --limit-time-seconds   <timeout>  \       --commands   '[{"command":"<command1>"},{"command":" <command2>","arguments":["<arg1>","<arg2>"]}]'  \       --resource-group   "<resourceGroupName>"  \       --subscription   "<subscription>"   \nThese commands don't require \narguments\n:\nfdisk -l\nhostname\nifconfig -a\nifconfig -s\nmount\nss\nulimit -a\nAll other inputs are required.\nMultiple commands can be provided in json format to \n--commands\n option.\nFor a command with multiple arguments, provide as a list to \narguments\n parameter. See\nAzure CLI Shorthand  for instructions on constructing the \n--commands\n structure.\nThese commands can be long running so the recommendation is to set \n--limit-time-\nseconds\n to at least 600 seconds (10 minutes). Running multiple extracts might take\nlonger that 10 minutes.\nThis command runs synchronously. If you wish to skip waiting for the command to complete, specify the \n--no-wait --debug\n options. For more information, see  how to\ntrack asynchronous operations .\nWhen an optional argument \n--output-directory\n is provided, the output result is\ndownloaded and extracted to the local directory."}
{"text": "This example executes the"}
{"text": "hostname"}
{"text": "command and a"}
{"text": "ping"}
{"text": "command.\nAzure CLI\naz networkcloud baremetalmachine run-read-command  --name   "bareMetalMachineName"  \       --limit-time-seconds  60 \       --commands   '[{"command":"hostname"],"arguments":["198.51.102.1","- c","3"]},{"command":"ping"}]'  \       --resource-group   "resourceGroupName"  \       --subscription   "<subscription>"   \nIn the response, an HTTP status code of 202 is returned as the operation is performed asynchronously."}
{"text": "Checking command status and viewing output\nSample output looks something as below. It prints the top 4K characters of the result to the screen for convenience and provides a short-lived link to the storage blob containing the command execution result. You can use the link to download the zipped output file (tar.gz).\nOutput\n  ====Action Command Output====    + hostname    rack1compute01    + ping 198.51.102.1 -c 3    PING 198.51.102.1 (198.51.102.1) 56(84) bytes of data. \n  --- 198.51.102.1 ping statistics ---    3 packets transmitted, 0 received, 100% packet loss, time 2049ms \n  ================================    Script execution result can be found in storage account:    https://<storage_account_name>.blob.core.windows.net/bmm-run-command- output/a8e0a5fe-3279-46a8-b995-51f2f98a18dd-action-bmmrunreadcmd.tar.gz? se=2023-04-14T06%3A37%3A00Z&sig=XXX&sp=r&spr=https&sr=b&st=2023-04- 14T02%3A37%3A00Z&sv=2019-12-12"}
{"text": "How to view the output of an"}
{"text": "az networkcloud"}
{"text": "baremetalmachine run-read-command"}
{"text": "in the"}
{"text": "Cluster Manager Storage account\nThis guide walks you through accessing the output file that is created in the Cluster Manager Storage account when an \naz networkcloud baremetalmachine run-read-command\nis executed on a server. The name of the file is identified in the \naz rest\n status output.\n1. Open the Cluster Manager Managed Resource Group for the Cluster where the server is housed and then select the  Storage account .\n2. In the Storage account details, select  Storage browser  from the navigation menu on the left side.\n3. In the Storage browser details, select on  Blob containers .\n4. Select the baremetal-run-command-output blob container.\n5. Select the output file from the run-read command. The file name can be identified from the \naz rest --method get\n command. Additionally, the  Last modified\ntimestamp aligns with when the command was executed.\n6. You can manage & download the output file from the  Overview  pop-out."}
{"text": "Troubleshoot bare metal machine issues"}
{"text": "using the"}
{"text": "az networkcloud"}
{"text": "baremetalmachine run-data-extract"}
{"text": "command\nArticle  05/31/2023\nThere may be situations where a user needs to investigate and resolve issues with an on-premises bare metal machine. Azure Operator Nexus provides a prescribed set of data extract commands via \naz networkcloud baremetalmachine run-data-extract\n. These\ncommands enable users to get diagnostic data from a bare metal machine.\nThe command produces an output file containing the results of the data extract located in the Cluster Manager's Azure Storage Account."}
{"text": "Before you begin\nThis article assumes that you've installed the Azure command line interface and the\nnetworkcloud\n command line interface extension. For more information, see  How to\nInstall CLI Extensions . The target bare metal machine is on and has readyState set to True. The syntax for these commands is based on the 0.3.0+ version of the \naz\nnetworkcloud\n CLI."}
{"text": "Executing a run command\nThe run data extract command executes one or more predefined scripts to extract data from a bare metal machine.\nThe current list of supported commands are\nSupportAssist/TSR collection for Dell troubleshooting  Command Name: \nhardware-support-data-collection\n \nArguments: Type of logs requested\nSysInfo\n - System Information\nTTYLog\n - Storage TTYLog data\nDebug\n - debug logs\nThe command syntax is:\nAzure CLI\naz networkcloud baremetalmachine run-data-extract  --name   "<machine-name>"   \     --resource-group   "<resource-group>"  \     --subscription   "<subscription>"  \     --commands   '[{"arguments":["<arg1>","<arg2>"],"command":"<command1>"}]'   \     --limit-time-seconds   <timeout>  \nSpecify multiple commands using json format in \n--commands\n option. Each \ncommand\nspecifies command and arguments. For a command with multiple arguments, provide as a list to the \narguments\n parameter. See  Azure CLI Shorthand  for instructions on\nconstructing the \n--commands\n structure.\nThese commands can be long running so the recommendation is to set \n--limit-time-\nseconds\n to at least 600 seconds (10 minutes). The \nDebug\n option or running multiple\nextracts might take longer than 10 minutes.\nThis example executes the \nhardware-support-data-collection\n command and get\nSysInfo\n and \nTTYLog\n logs from the Dell Server.\nAzure CLI\naz networkcloud baremetalmachine run-data-extract  --name   "bareMetalMachineName"  \     --resource-group   "resourceGroupName"  \     --subscription   "subscription"  \     --commands   '[{"arguments":["SysInfo", "TTYLog"],"command":"hardware- support-data-collection"}]'  \     --limit-time-seconds  600 \nIn the response, the operation performs asynchronously and returns an HTTP status code of 202. See the  Viewing the output  section for details on how to track command completion and view the output file."}
{"text": "Viewing the output\nSample output looks something like this. Note the provided link to the tar.gz zipped file from the command execution. The tar.gz file name identifies the file in the Storage Account of the Cluster Manager resource group. You can also use the link to directly access the output zip file. The tar.gz file also contains the zipped extract command file outputs in \nhardware-support-data-<timestamp>.zip\n. Download the output file from the\nstorage blob to a local directory by specifying the directory path in the optional argument \n--output-directory\n.\nAzure CLI\n====Action Command O utput= ===  Executing hardware -support-data-collection  command  Getting following hardware support logs: SysInfo,TTYLog  Job JID_814372800396 is running, waiting for it to complete ...  Job JID_814372800396 Completed.  ----------------------------  JOB  -------------------------   [Job ID=JID_814372800396]  Job N ame= SupportAssist Collection S tatus= Completed  Scheduled Start T ime= [Not  Applicable]   Expiration T ime= [Not  Applicable]   Actual Start T ime= [Thu, 13 Apr 2023 20 :54:40]   Actual Completion T ime= [Thu, 13 Apr 2023 20 :59:51]   M essage= [SRV088: The SupportAssist Collection Operation is completed  successfully.]   Percent  Complete=[100]   ----------------------------------------------------------   Deleting Job JID_814372800396  Collection successfully exported to /hostfs/tmp/runcommand/hardware -support- data- 2023 - 04 - 13T21:00:01.zip \n================================  Script execution result can be found in storage account:  https://cm2p9bctvhxnst.blob.core.windows.net/bmm -run-command- output /dd84df50 - 7b02 - 4d10 -a 2be - 46782cbf4eef -action-bmmdataextcmd .tar. gz? se= 2023 - 04 - 14T01%3A00%3A15Z andsig= ZJcsNoBzvOkUNL0IQ3XGtbJSaZxYqmtd%2BM6rmxDFqXE%3D andsp =randspr=httpsandsr=bandst= 2023 - 04 - 13T21%3A00%3A15Z andsv= 2019 - 12 - 12"}
{"text": "Azure Operator Nexus frequently asked"}
{"text": "questions (FAQ)\nArticle  06/30/2023\nThe following sections covers some of the frequently asked questions for Azure Operator Nexus:"}
{"text": "Platform - General"}
{"text": "What services does Azure Operator Nexus provide?\nAzure Operator Nexus is a managed hybrid cloud platform that supports carrier-grade network workloads. Here, the management plane lives in Azure and the control plane and user plane gets deployed on operators' premises or in Azure. It simplifies provisioning of new network services and optimizes deployment of network functions and applications on premises. The end customer can deploy containerized applications (on an on premises Nexus AKS cluster) or a virtualized workload to run these network functions. Customer gets out of the box integration with many Azure services such as Azure Monitor, Azure Container Registry, and Azure Kubernetes Services."}
{"text": "How do I interact with Operator Nexus instance?\nYou can interact with Operator Nexus like any other Azure services using AZ CLI, API, ARM template, or portal. You can alternatively use BICEP templates."}
{"text": "Does customer need to deploy any resources in their"}
{"text": "subscription to deploy Azure Operator Nexus instances?\nYes, there are some resources that customer needs to create in the respective region under their Azure subscriptions. Some of these include creation of a pair of Network Fabric Controller and Cluster Manager resource, Log Analytics Workspace, a storage account. For more details, please refer to  Azure Operator Nexus documentation ."}
{"text": "Does Azure Operator Nexus rely on connectivity with"}
{"text": "Azure? What happens when there's a disconnection?\nYes, you need an ExpressRoute connection for its connectivity back to Azure and for Orchestration, Management and Operation purposes. During disconnection, the workloads will continue to run as is but you may lose the capability to orchestrate any new resources."}
{"text": "Do I have to use the BOM (Bill of Material) specified by"}
{"text": "Microsoft?\nYes, to ensure carrier-grade performance and high degrees of automation, you'll need to use equipment specified as per one of our BOMs."}
{"text": "How should I plan for a resilient Operator Nexus"}
{"text": "instance? How does Operator Nexus handle disaster"}
{"text": "recovery?\nCustomers should design their services with Intra-rack redundancy, Inter-rack redundancy and globally load balancing across multiple instances. Also, for high availability, plan to spread your instances across multiple Azure regions."}
{"text": "How do updates work to on-premises and to Azure"}
{"text": "components?\nUpgrades to Operator Nexus are made in two phases - Management bundle upgrades and Runtime bundle upgrades. Management bundle upgrades deals with the upgrades of Controllers in Azure, Cluster Managers in customer subscription and on-premises instances. In on-premises instances, it includes the Kubernetes controllers responsible for maintaining the state of infra resources.\nUpdates of Management bundle may cause interruptions to provisioning activities but it doesn't impacts the customers running workloads. Customers don't control or drive these upgrades, but these upgrades are essential to provide customers with the options to update to new runtime-based upgrades within their on-premises instances.\nOn the other hand, Runtime bundle upgrades deals with the components that require updates to the OS (Operating System) and/or workload supporting components. The update of the runtime bundle is entirely under the control of the customer and APIs can be used to perform these updates. You might observe some workload impacts during this upgrade."}
{"text": "Is the storage appliance a must required device?\nFor near-edge SKUs, Storage appliance is a part of the Hardware infrastructure that Operator needs to procure."}
{"text": "Does Operator Nexus provide best practices or blueprints"}
{"text": "for deploying network functions on Operator Nexus?\nYes, Operator Nexus comes with Nexus Ready program. With this program, Microsoft is working with industry leading Network Function partners to validate that their network functions to ensure they can run on Nexus platform. We validate these network functions on regular intervals to ensure that they stay compliant with newer versions of Nexus. Operators can now get consistent and scalable deployment of multi-vendor network functions with the Nexus Ready program."}
{"text": "What data stays on premises and what is available in"}
{"text": "Azure?\nFrom Infrastructure perspective, the data is managed via Azure APIs. The telemetry from these layers gets collected and is visible under customer subscription. Customers can use Log Analytics Workspace, storage accounts or other Analytics services in Azure to look into the telemetry from Infra layers.\nFor tenant workloads, the images get stored in ACRs (Azure Container Registry) and once deployed. Microsoft provides an option to collect the telemetry from tenant workloads into Azure but Customers can choose alternative tooling they wish to collect telemetry data or to analyze it."}
{"text": "If an Azure region doesn't exist in my country, can I still"}
{"text": "use Operator Nexus?\nYes, all you need is ExpressRoute connectivity to an Azure region. ExpressRoute connectivity is available at many locations. For more information, see the  Geo-locations and  connectivity providers ."}
{"text": "Can I move my resources from one subscription to"}
{"text": "another?\nCurrently, we don't support resource moves. If you need to move resources, you can consider deleting the existing controllers and using the ARM template to create another one in another location."}
{"text": "How many instances can be associated to a cluster"}
{"text": "manager/fabric controller pair?\nThe number of Azure Operator Nexus instances, a single pair of Network Fabric Controller and Cluster Manager can manage depends on multiple factors. It can be influenced by factors like size of Operator Nexus instances, ExpressRoute circuit bandwidth, number and frequency of optional metrics collection, number of workloads running in Instance, destination for workload telemetry data collection and other factors.\nFor more information, see  limits & quotas ."}
{"text": "Compute"}
{"text": "Does Azure Operator Nexus support creation of Virtual"}
{"text": "Machines (VMs)?\nYes, Azure Operator Nexus provides the ability to create customized VMs for hosting VNFs within a telco network. The Azure Operator Nexus platform provides Azure CLI command to create a customized VM. To host a VNF on your VM, have it Azure Arc enrolled, and provide a way to SSH to it via the Azure CLI. You can use your image when youre creating Azure Operator Nexus virtual machines. Make sure that each image that you use to create your workload VMs is a containerized image in either qcow2 or raw disk format. Upload these images to Azure Container Registry."}
{"text": "How do I update a bare metal server?\nTo update a bare metal server within an Azure Operator Nexus instance, you can use the Azure APIs. Any update for bare metal server is part of runtime bundle upgrade. This upgrade requires the Nexus instance connectivity back to Azure."}
{"text": "What OS runs on the bare metal server? Can I bring my"}
{"text": "own?\nBare metal servers are deployed with Microsoft Azure Linux (previously called CBL Mariner OS), which is thoroughly tested and is compatible with required Azure agents. There are no plans to support any other OS offering for Bare metal servers."}
{"text": "How many servers does Operator Nexus support? How"}
{"text": "many racks?\nAn Operator Nexus instance can have up to eight compute racks and each rack hosting upto 16 servers. These compute servers are used for running actual tenant workloads."}
{"text": "Networks"}
{"text": "Does Operator Nexus support IPv6?\nYes, Azure Operator Nexus provides support for both IPV4 and IPV6 configuration across all layers of the stack."}
{"text": "What are the networking requirements for Azure"}
{"text": "Operator Nexus?\nHere are some of the network requirements for Azure Operator Nexus:\nCustomers need to work with Microsoft partners for setting up ExpressRoute connections, PE (Provider Edge) device supports 400G or 100G connections to CE (Customer Edge) device in Operator Nexus instance PE must have routes to ExpressRoutes IP address blocks defined for various services, VLANs for iDrac, PXE, Storage, OAM etc.\nFor more information, see  Network fabric controller  and  Network fabric ."}
{"text": "What is an isolation domain?\nIsolation domains enable Layer 2 or Layer 3 connectivity between workloads hosted across the Azure Operator Nexus instance and external networks. These constructs segment a network into authentication domains and enforces communication within required boundaries."}
{"text": "Does Operator Nexus support a single ToR (Top of Rack)"}
{"text": "device?\nFor near-edge, the fabric is designed based on high availability model and the reason you can't have just one ToR switch."}
{"text": "Is Network packet Broker (NPB) a hard requirement?\nFor near-edge SKUs, NPBs will be part of the BOM."}
{"text": "How do I configure the load balancing service in a Nexus"}
{"text": "cluster?\nYou can deploy load balancers with the Nexus AKS clusters. Ensure you reserve and dedicate a pool of IP addresses while creating DCN network."}
{"text": "Tenant workloads"}
{"text": "Can I bring my own K8S cluster?\nNexus platform offers customers an option to either create Nexus AKS clusters with multiple Kubernetes versions or Virtual Machines for running their workloads."}
{"text": "What VNFs (Virtualized Network Functions) and CNFs"}
{"text": "(Containerized Network Functions) are certified on the"}
{"text": "platform?\nValidation of VNF and CNF functions is an ongoing activity. We'll publish the list of certified VNFs and CNFs soon."}
{"text": "If a VNF or CNF isn't certified, can I still use it?\nIndeed, you can collaborate with the Nexus team to ensure there are no limitations that would prevent you from deploying these workloads."}
{"text": "How do I deploy a VM/Container?\nCustomers can use APIs to deploy VM and Nexus AKS clusters in an Operator Nexus instance. For a richer experience, Microsoft offers another service AOSM (Azure Operator Service Manager) which allows you to automate deployment of your Containerized (CNF) and Virtualized (VNF) Network Functions."}
{"text": "What storage classes can I use for my containers within"}
{"text": "AKS clusters?\nNexus AKS clusters have a support for Read Write Many (RWX) & Read Write Once (RWO) storage classes."}
{"text": "How can Operator Nexus VMs be made highly available?\nThe Operators can choose to deploy VMs across multiple bare metal machines and across racks to achieve high availability."}
{"text": "Observability"}
{"text": "How do I monitor my Azure Operator Nexus instance?\nAzure Operator Nexus provides customers to monitor the health of Nexus instance and its connected resources by collecting telemetry into customer subscription. Customers can collect and analyze all the logs in Azure Log Analytics Workspace/Azure Data Explorer where those can be additionally retained for longer durations."}
{"text": "What metrics are available for compute, networking,"}
{"text": "storage? Can I alert on them? Can I integrate with my"}
{"text": "own monitoring solution?\nA wide and curated set of metrics are delivered to customer from across the layers in the Azure Operator Nexus instance stack. These include the metrics essential for assessing the health based of compute, storage and networking incl. resource quotas and utilization. Customers can set appropriate alert rules based on the telemetry collected to ensure that notifications are triggered whenever the configured thresholds are met.\nFor more information, see the  list of metrics ."}
{"text": "Configuration options for PE-CE"}
{"text": "connectivity\nArticle  04/05/2023"}
{"text": "Introduction\nOperator Nexus is a two layer Clos type architecture with the CEs (Connection Endpoint) acting as the edge devices or boundary routers. All types of traffic (such as management traffic, mobile network control, and user plane traffic), to and from an Operator Nexus instance, will pass through the CE.\nOn your site, the CEs will be connected to your PEs (Provider Edge or P) routers. You can configure PE-CE connectivity in multiple ways.\nFollowing are the configuration areas."}
{"text": "Physical connection\nOperator Nexus is designed to reserve multiple ports for physical connectivity between CE and PE. These ports will be added to port channel. You don't have to connect all ports on day one. You can start with one port and add more ports on need basis."}
{"text": "Port channel\nPort channel is required for PE-CE connectivity. All the ports connecting PE to CE will be part of this port channel. You can start with one port and later add more ports to this port channel. Based on your design, you'll create subinterfaces from this port channel interface for different types of traffic."}
{"text": "VLANs\nAt least one subinterface is required between PE and CE. You can create multiple subinterfaces and assign them to respective VLANs. You should pick a VLAN number above 500."}
{"text": "IP addresses\nCE supports both IPv4 and IPv6 address. You can assign a /31 or /30 IPv4 on the subinterface between PE and CE. You can assign a /127 IPv6 address. Based on your BGP design, you can use only IPv6 for option A. However, for option B you shall configure IPv4."}
{"text": "Protocols\nOnly BGP is supported between PE and CE. You can use iBGP or eBGP between the PE and CE. You'll assign "Fabric ASN" and "Peer ASN" based on your design. All BGP peerings between PE and CE at a given site will use the same "Fabric ASN". To establish some sessions as iBGP and others as eBGP make changes at PE."}
{"text": "BGP\nYou can use standard BGP (option A). You can also use MP-BGP with inter-as Option 10B. In this case, you have to define option B parameters during your network fabric creation."}
{"text": "Prerequisites\n1. Decide how many ports you want to start with. 2. Find the right optics and cables based on your desired throughput and distance between PE and CE. For the CE, the optics must conform to provided bill of materials. 3. Choose the VLAN numbers for subinterfaces 4. Allocate IP addresses for the PE CE interfaces 5. Select the right BGP design for PE-CE connectivity\nFor MP-BGP make sure you configure matching route targets on both PE and CE.\nAzure CLI\naz nf fabric create  \  --resource-group   "example-rg"  \  --location   "eastus"  \  --resource-name   "example-nf"  \  --nf-sku   "123"  \  --nfc-id   "12333"  \  --nni-config   '{"layer3Configuration":{"primaryIpv4Prefix":"10.20.0.0/19",  "fabricAsn":10000, "peerAsn":10001, "vlanId": 20}, "layer2Configuration" :  {"portCount":4,"mtu":1500} }'  \  --managed-network-config   '{"ipv4Prefix":"10.1.0.0/19",  "managementVpnConfiguration":{"optionBProperties":{"importRouteTargets":\n["65531:2001","65532:2001"], "exportRouteTargets": ["65531:2001","65532:2001"]}}}'"}
{"text": "PE configuration steps\n1. Add selected interface(s) to port channel 2. Configure subinterfaces and assign corresponding VLANs 3. Assign IPv4 and/or IPv6 addresses to the interfaces 4. Configure BGP based on the design 5. For option B, configure route targets"}
{"text": "Test the integration\n1. Run \nshow lldp\n neighbor to verify physical connection\n2. Validate connectivity by ping test 3. Check the BGP neighbor status 4. Verify that you're exchanging routes with CE"}
{"text": "List of metrics collected in Azure Operator Nexus\nArticle  05/23/2023\nThis section provides the list of metrics collected from the different components.\nUndercloud Kubernetes\nkubernetes API server kubernetes Services coreDNS etcd calico-felix calico-typha containers\nBaremetal servers\nnode metrics\nVirtual Machine orchestrator\nkubevirt\nStorage Appliance\npure storage\nNetwork Fabric\nNetwork Devices Metrics"}
{"text": "Undercloud Kubernetes\nKubernetes API server\nMetric Category Unit Aggregation Description Dimensions Exportable Type via Diagnostic Settings?\napiserver_audit_requests_rejected_total Apiserver Count Average Counter of apiserver Cluster, Yes requests rejected due Node to an error in audit logging backend.\napiserver_client_certificate_expiration_seconds_sum Apiserver Second Sum Distribution of the Cluster, Yes remaining lifetime on Node the certificate used to authenticate a request.\napiserver_storage_data_key_generation_failures_total Apiserver Count Average Total number of failed Cluster, Yes data encryption Node key(DEK) generation operations.\napiserver_tls_handshake_errors_total Apiserver Count Average Number of requests Cluster, Yes dropped with 'TLS Node handshake error from' error\nKubernetes services\nMetric Category Unit Aggregation Description Dimensions Exportable Metric Category Unit Aggregation Description Dimensions Exportable Type via   Type via   Diagnostic Diagnostic Settings? Settings?\nkube_daemonset_status_current_number_scheduled Kube Count Average Number of Cluster Yes Daemonset Daemonsets scheduled\nkube_daemonset_status_desired_number_scheduled Kube Count Average Number of Cluster Yes Daemonset daemoset replicas desired\nkube_deployment_status_replicas_ready Kube Count Average Number of Cluster Yes Deployment deployment replicas present\nkube_deployment_status_replicas_available Kube Count Average Number of Cluster Yes Deployment deployment replicas available\nkube_job_status_active Kube job - Labels Average Number of Cluster, Job Yes Active actively running jobs\nkube_job_status_failed Kube job - Labels Average Number of failed Cluster, Job Yes Failed jobs\nkube_job_status_succeeded Kube job - Labels Average Number of Cluster, Job Yes Succeeded successful jobs\nkube_node_status_allocatable Node - Labels Average The amount of Cluster, Yes Allocatable resources Node, allocatable for Resource pods\nkube_node_status_capacity Node - Labels Average The total amount Cluster, Yes Capacity of resources Node, available for a Resource node\nkube_node_status_condition Kubenode Labels Average The condition of Cluster, Yes status a cluster node Node, Condition, Status\nkube_pod_container_resource_limits Pod Count Average The number of Cluster, Yes container - requested limit Node, Limits resource by a Resource, container. Pod\nkube_pod_container_resource_requests Pod Count Average The number of Cluster, Yes container - requested Node, Requests request resource Resource, by a container. Pod\nkube_pod_container_state_started Pod Second Average Start time in unix Cluster, Yes container - timestamp for a Node, state pod container Container\nkube_pod_container_status_last_terminated_reason Pod Labels Average Describes the last Cluster, Yes container - reason the Node, state container was in Container, terminated state Reason\nkube_pod_container_status_ready Container Labels Average Describes Cluster, Yes State whether the Node, containers Container readiness check succeeded\nMetric Category Unit Aggregation Description Dimensions Exportable Type via   Diagnostic Settings?\nkube_pod_container_status_restarts_total Container Count Average The number of Cluster, Yes State container restarts Node, per container Container\nkube_pod_container_status_running Container Labels Average Describes Cluster, Yes State whether the Node, container is Container currently in running state\nkube_pod_container_status_terminated Container Labels Average Describes Cluster, Yes State whether the Node, container is Container currently in terminated state\nkube_pod_container_status_terminated_reason Container Labels Average Describes the Cluster, Yes State reason the Node, container is Container, currently in Reason terminated state\nkube_pod_container_status_waiting Container Labels Average Describes Cluster, Yes State whether the Node, container is Container currently in waiting state\nkube_pod_container_status_waiting_reason Container Labels Average Describes the Cluster, Yes State reason the Node, container is Container, currently in Reason waiting state\nkube_pod_deletion_timestamp Pod Timestamp NA Unix deletion Cluster, Pod Yes Deletion timestamp Timestamp\nkube_pod_init_container_status_ready Init Labels Average Describes Cluster, Yes Container whether the init Node, State containers Container readiness check succeeded\nkube_pod_init_container_status_restarts_total Init Count Average The number of Cluster, Yes Container restarts for the Container State init container\nkube_pod_init_container_status_running Init Labels Average Describes Cluster, Yes Container whether the init Node, State container is Container currently in running state\nkube_pod_init_container_status_terminated Init Labels Average Describes Cluster, Yes Container whether the init Node, State container is Container currently in terminated state\nkube_pod_init_container_status_terminated_reason Init Labels Average Describes the Cluster, Yes Container reason the init Node, State container is Container, currently in Reason terminated state\nMetric Category Unit Aggregation Description Dimensions Exportable Type via   Diagnostic Settings?\nkube_pod_init_container_status_waiting Init Labels Average Describes Cluster, Yes Container whether the init Node, State container is Container currently in waiting state\nkube_pod_init_container_status_waiting_reason Init Labels Average Describes the Cluster, Yes Container reason the init Node, State container is Container, currently in Reason waiting state\nkube_pod_status_phase Pod Status Labels Average The pods current Cluster, Yes phase Node, Container, Phase\nkube_pod_status_ready Pod Status Count Average Describe whether Cluster, Pod Yes Ready the pod is ready to serve requests.\nkube_pod_status_reason Pod Status Labels Average The pod status Cluster, Yes Reason reasons Node, Container, Reason\nkube_statefulset_replicas Statefulset Count Average The number of Cluster, Yes # of replicas desired pods for Stateful Set a statefulset\nkube_statefulset_status_replicas Statefulset Count Average The number of Cluster, Yes replicas replicas per Stateful Set status statefulsets\ncontroller_runtime_reconcile_errors_total Kube Count Average Total number of Cluster, Yes Controller reconciliation Node, errors per Controller controller\ncontroller_runtime_reconcile_total Kube Count Average Total number of Cluster, Yes Controller reconciliation per Node, controller Controller\nkubelet_running_containers Containers - Labels Average Number of Cluster, Yes # of containers node, running currently running Container State\nkubelet_running_pods Pods - # of Count Average Number of pods Cluster, Yes running that have a Node running pod sandbox\nkubelet_runtime_operations_errors_total Kubelet Count Average Cumulative Cluster, Yes Runtime Op number of Node Errors runtime operation errors by operation type.\nkubelet_volume_stats_available_bytes Pods - Byte Average Number of Cluster, Yes Storage - available bytes in Node, Available the volume Persistent Volume Claim\nMetric Category Unit Aggregation Description Dimensions Exportable Type via   Diagnostic Settings?\nkubelet_volume_stats_capacity_bytes Pods - Byte Average Capacity in bytes Cluster, Yes Storage - of the volume Node, Capacity Persistent Volume Claim\nkubelet_volume_stats_used_bytes Pods - Byte Average Number of used Cluster, Yes Storage - bytes in the Node, Used volume Persistent Volume Claim\ncoreDNS\nMetric Category Unit Aggregation Description Dimensions Exportable Type via   Diagnostic Settings?\ncoredns_dns_requests_total DNS Requests Count Average total query count Cluster, Yes Node, Protocol\ncoredns_dns_responses_total DNS Count Average response per zone, rcode and Cluster, Yes response/errors plugin. Node, Rcode\ncoredns_health_request_failures_total DNS Health Count Average The number of times the internal Cluster, Yes Request health check loop failed to query Node Failures\ncoredns_panics_total DNS panic Count Average total number of panics Cluster, Yes Node\netcd\nMetric Category Unit Aggregation Description Dimensions Exportable Type via   Diagnostic Settings?\netcd_disk_backend_commit_duration_seconds_sum Etcd Disk Second Average The latency distributions Cluster, Pod Yes of commit called by backend.\netcd_disk_wal_fsync_duration_seconds_sum Etcd Disk Second Average The latency distributions Cluster, Pod Yes of fsync called by wal\netcd_server_is_leader Etcd Labels Average Whether node is leader Cluster, Pod Yes Server"}
{"text": "etcd_server_is_learner Etcd Labels Average Whether node is learner Cluster, Pod Yes Server\netcd_server_leader_changes_seen_total Etcd Count Average The number of leader Cluster, Pod, Yes Server changes seen. Tier\netcd_server_proposals_committed_total Etcd Count Average The total number of Cluster, Pod, Yes Server consensus proposals Tier committed.\netcd_server_proposals_applied_total Etcd Count Average The total number of Cluster, Pod, Yes Server consensus proposals Tier applied.\nMetric Category Unit Aggregation Description Dimensions Exportable Type via   Diagnostic Settings?\netcd_server_proposals_failed_total Etcd Count Average The total number of Cluster, Pod, Yes Server failed proposals seen. Tier\ncalico-felix\nMetric Category Unit Aggregation Description Dimensions Exportable Type via   Diagnostic Settings?\nfelix_ipsets_calico Felix Count Average Number of active Calico IP sets. Cluster, Yes Node\nfelix_cluster_num_host_endpoints Felix Count Average Total number of host endpoints Cluster, Yes cluster-wide. Node\nfelix_active_local_endpoints Felix Count Average Number of active endpoints on this Cluster, Yes host. Node\nfelix_cluster_num_hosts Felix Count Average Total number of Calico hosts in the Cluster, Yes cluster. Node\nfelix_cluster_num_workload_endpoints Felix Count Average Total number of workload endpoints Cluster, Yes cluster-wide. Node\nfelix_int_dataplane_failures Felix Count Average Number of times dataplane updates Cluster, Yes failed and will be retried. Node\nfelix_ipset_errors Felix Count Average Number of ipset command failures. Cluster, Yes Node\nfelix_iptables_restore_errors Felix Count Average Number of iptables-restore errors. Cluster, Yes Node\nfelix_iptables_save_errors Felix Count Average Number of iptables-save errors. Cluster, Yes Node\nfelix_resyncs_started Felix Count Average Number of times Felix has started Cluster, Yes resyncing with the datastore. Node\nfelix_resync_state Felix Count Average Current datastore state. Cluster, Yes Node\ncalico-typha\nMetric Category Unit Aggregation Description Dimensions Exportable via Type Diagnostic Settings?\ntypha_connections_accepted Typha Count Average Total number of connections accepted Cluster, Yes over time. Node\ntypha_connections_dropped Typha Count Average Total number of connections dropped Cluster, Yes due to rebalancing. Node\ntypha_ping_latency_count Typha Count Average Round-trip ping latency to client. Cluster, Yes Node\nKubernetes containers\nMetric Category Unit Aggregation Description Dimensions Exportable Metric Category Unit Aggregation Description Dimensions Exportable Type via   Type via   Diagnostic Diagnostic Settings? Settings?\ncontainer_fs_io_time_seconds_total Containers Second Average Cumulative count of Cluster, Node, Yes - seconds spent doing I/Os Pod+Container+Interface Filesystem\ncontainer_memory_failcnt Containers Count Average Number of memory usage Cluster, Node, Yes - Memory hits limits Pod+Container+Interface\ncontainer_memory_usage_bytes Containers Byte Average Current memory usage, Cluster, Node, Yes - Memory including all memory Pod+Container+Interface regardless of when it was accessed\ncontainer_tasks_state Containers Labels Average Number of tasks in given Cluster, Node, Yes - Task state Pod+Container+Interface, state State"}
{"text": "Baremetal servers\nnode metrics\nMetric Category Unit Aggregation Description Dimensions Exportable Type via   Diagnostic Settings?\nnode_boot_time_seconds Node - Boot Second Average Unix time of last boot Cluster, Yes time Node\nnode_cpu_seconds_total Node - CPU Second Average CPU usage Cluster, Yes Node, CPU, Mode\nnode_disk_read_time_seconds_total Node - Disk - Second Average Disk read time Cluster, Yes Read Time Node, Device\nnode_disk_reads_completed_total Node - Disk - Count Average Disk reads completed Cluster, Yes Read Node, Completed Device\nnode_disk_write_time_seconds_total Node - Disk - Second Average Disk write time Cluster, Yes Write Time Node, Device\nnode_disk_writes_completed_total Node - Disk - Count Average Disk writes completed Cluster, Yes Write Node, Completed Device\nnode_entropy_available_bits Node - Bits Average Available node entropy Cluster, Yes Entropy Node Available\nnode_filesystem_avail_bytes Node - Disk - Byte Average Available filesystem size Cluster, Yes Available (TBD) Node, Mountpoint\nnode_filesystem_free_bytes Node - Disk - Byte Average Free filesystem size Cluster, Yes Free (TBD) Node, Mountpoint\nnode_filesystem_size_bytes Node - Disk - Byte Average Filesystem size Cluster, Yes Size Node, Mountpoint\nMetric Category Unit Aggregation Description Dimensions Exportable Type via   Diagnostic Settings?\nnode_filesystem_files Node - Disk - Count Average Total number of permitted Cluster, Yes Files inodes Node, Mountpoint\nnode_filesystem_files_free Node - Disk - Count Average Total number of free inodes Cluster, Yes Files Free Node, Mountpoint\nnode_filesystem_device_error Node - Disk - Count Average indicates if there was a Cluster, Yes FS Device error problem getting information Node, for the filesystem Mountpoint\nnode_filesystem_readonly Node - Disk - Count Average indicates if the filesystem is Cluster, Yes Files Readonly readonly Node, Mountpoint\nnode_hwmon_temp_celsius Node - Celcius Average Hardware monitor for Cluster, Yes temperature temperature Node, Chip, (TBD) Sensor\nnode_hwmon_temp_max_celsius Node - Celcius Average Hardware monitor for Cluster, Yes temperature maximum temperature Node, Chip, (TBD) Sensor\nnode_load1 Node - Second Average 1m load average. Cluster, Yes Memory Node\nnode_load15 Node - Second Average 15m load average. Cluster, Yes Memory Node\nnode_load5 Node - Second Average 5m load average. Cluster, Yes Memory Node\nnode_memory_HardwareCorrupted_bytes Node - Byte Average Memory information field Cluster, Yes Memory HardwareCorrupted_bytes. Node\nnode_memory_MemAvailable_bytes Node - Byte Average Memory information field Cluster, Yes Memory MemAvailable_bytes. Node\nnode_memory_MemFree_bytes Node - Byte Average Memory information field Cluster, Yes Memory MemFree_bytes. Node\nnode_memory_MemTotal_bytes Node - Byte Average Memory information field Cluster, Yes Memory MemTotal_bytes. Node\nnode_memory_numa_HugePages_Free Node - Byte Average Free hugepages Cluster, Yes Memory Node. NUMA\nnode_memory_numa_HugePages_Total Node - Byte Average Total hugepages Cluster, Yes Memory Node. NUMA\nnode_memory_numa_MemFree Node - Byte Average Numa memory free Cluster, Yes Memory Node. NUMA\nnode_memory_numa_MemTotal Node - Byte Average Total Numa memory Cluster, Yes Memory Node. NUMA\nnode_memory_numa_MemUsed Node - Byte Average Numa memory used Cluster, Yes Memory Node. NUMA\nnode_memory_numa_Shmem Node - Byte Average Shared memory Cluster, Yes Memory Node\nMetric Category Unit Aggregation Description Dimensions Exportable Type via   Diagnostic Settings?\nnode_os_info Node - OS Info Labels Average OS details Cluster, Yes Node\nnode_network_carrier_changes_total Node Network Count Average carrier_changes_total value Cluster, Yes - Carrier of \n/sys/class/net/<iface>\n. node, changes Device\nnode_network_receive_packets_total NodeNetwork Count Average Network device statistic Cluster, Yes - receive receive_packets. node, packets Device\nnode_network_transmit_packets_total NodeNetwork Count Average Network device statistic Cluster, Yes - transmit transmit_packets. node, packets Device\nnode_network_up Node Network Labels Average Value is 1 if operstate is 'up', Cluster, Yes - Interface 0 otherwise. node, state Device\nnode_network_mtu_bytes Network Byte Average mtu_bytes value of Cluster, Yes Interface -\n/sys/class/net/<iface>\n. node, MTU Device\nnode_network_receive_errs_total Network Count Average Network device statistic Cluster, Yes Interface - receive_errs node, Error totals Device\nnode_network_receive_multicast_total Network Count Average Network device statistic Cluster, Yes Interface - receive_multicast. node, Multicast Device\nnode_network_speed_bytes Network Byte Average speed_bytes value of Cluster, Yes Interface -\n/sys/class/net/<iface>\n. node, Speed Device\nnode_network_transmit_errs_total Network Count Average Network device statistic Cluster, Yes Interface - transmit_errs. node, Error totals Device\nnode_timex_sync_status Node Timex Labels Average Is clock synchronized to a Cluster, Yes reliable server (1 = yes, 0 = Node no).\nnode_timex_maxerror_seconds Node Timex Second Average Maximum error in seconds. Cluster, Yes Node\nnode_timex_offset_seconds Node Timex Second Average Time offset in between local Cluster, Yes system and reference clock. Node\nnode_vmstat_oom_kill Node VM Stat Count Average /proc/vmstat information Cluster, Yes field oom_kill. Node\nnode_vmstat_pswpin Node VM Stat Count Average /proc/vmstat information Cluster, Yes field pswpin. Node\nnode_vmstat_pswpout Node VM Stat Count Average /proc/vmstat information Cluster, Yes field pswpout Node\nnode_dmi_info Node Bios Labels Average Node environment Cluster, Yes Information information Node\nnode_time_seconds Node - Time Second NA System time in seconds since Cluster, Yes epoch (1970) Node\nidrac_power_input_watts Node - Power Watt Average Power Input Cluster, Yes Node, PSU\nMetric Category Unit Aggregation Description Dimensions Exportable Type via   Diagnostic Settings?\nidrac_power_output_watts Node - Power Watt Average Power Output Cluster, Yes Node, PSU\nidrac_power_capacity_watts Node - Power Watt Average Power Capacity Cluster, Yes Node, PSU\nidrac_sensors_temperature Node - Celcius Average Idrac sensor Temperature Cluster, Yes Temperature Node, Name\nidrac_power_on Node - Power Labels Average Idrac Power On Status Cluster, Yes Node"}
{"text": "Virtual Machine orchestrator\nkubevirt\nMetric Category Unit Aggregation Description Dimensions Exportable Type via   Diagnostic Settings?\nkubevirt_info Host Labels NA Version information. Cluster, Yes Node\nkubevirt_virt_controller_leading Kubevirt Labels Average Indication for an Cluster, Pod Yes Controller operating virt- controller.\nkubevirt_virt_operator_ready Kubevirt Labels Average Indication for a virt Cluster, Pod Yes Operator operator being ready\nkubevirt_vmi_cpu_affinity VM-CPU Labels Average Details the cpu pinning Cluster, Yes map via boolean labels Node, VM in the form of vcpu_X_cpu_Y.\nkubevirt_vmi_memory_actual_balloon_bytes VM- Byte Average Current balloon size in Cluster, Yes Memory bytes. Node, VM\nkubevirt_vmi_memory_domain_total_bytes VM- Byte Average The amount of Cluster, Yes Memory memory in bytes Node, VM allocated to the domain. The memory value in domain xml file\nkubevirt_vmi_memory_swap_in_traffic_bytes_total VM- Byte Average The total amount of Cluster, Yes Memory data read from swap Node, VM space of the guest in bytes.\nkubevirt_vmi_memory_swap_out_traffic_bytes_total VM- Byte Average The total amount of Cluster, Yes Memory memory written out to Node, VM swap space of the guest in bytes.\nMetric Category Unit Aggregation Description Dimensions Exportable Type via   Diagnostic Settings?\nkubevirt_vmi_memory_available_bytes VM- Byte Average Amount of usable Cluster, Yes Memory memory as seen by the Node, VM domain. This value may not be accurate if a balloon driver is in use or if the guest OS does not initialize all assigned pages\nkubevirt_vmi_memory_unused_bytes VM- Byte Average The amount of Cluster, Yes Memory memory left Node, VM completely unused by the system. Memory that is available but used for reclaimable caches should NOT be reported as free\nkubevirt_vmi_network_receive_packets_total VM- Count Average Total network traffic Cluster, Yes Network received packets. Node, VM, Interface\nkubevirt_vmi_network_transmit_packets_total VM- Count Average Total network traffic Cluster, Yes Network transmitted packets. Node, VM, Interface\nkubevirt_vmi_network_transmit_packets_dropped_total VM- Count Average The total number of tx Cluster, Yes Network packets dropped on Node, VM, vNIC interfaces. Interface\nkubevirt_vmi_outdated_count VMI Count Average Indication for the total Cluster, Yes number of Node, VM, VirtualMachineInstance Phase workloads that are not running within the most up-to-date version of the virt- launcher environment.\nkubevirt_vmi_phase_count VMI Count Average Sum of VMIs per phase Cluster, Yes and node. Node, VM, Phase\nkubevirt_vmi_storage_iops_read_total VM- Count Average Total number of I/O Cluster, Yes Storage read operations. Node, VM, Drive\nkubevirt_vmi_storage_iops_write_total VM- Count Average Total number of I/O Cluster, Yes Storage write operations. Node, VM, Drive\nkubevirt_vmi_storage_read_times_ms_total VM- Mili Average Total time (ms) spent Cluster, Yes Storage Second on read operations. Node, VM, Drive\nkubevirt_vmi_storage_write_times_ms_total VM- Mili Average Total time (ms) spent Cluster, Yes Storage Second on write operations Node, VM, Drive\nkubevirt_virt_controller_ready Kubevirt Labels Average Indication for a virt- Cluster, Pod Yes Controller controller that is ready to take the lead."}
{"text": "Storage Appliances\npure storage\nMetric Category Unit Aggregation Description Dimensions Exportable Type via   Diagnostic Settings?\npurefa_hardware_component_health FlashArray Labels NA FlashArray Cluster, Appliance, Yes hardware Controller+Component+Index component health status\npurefa_hardware_power_volts FlashArray Volt Average FlashArray Cluster, Power Supply, Yes hardware Appliance power supply voltage\npurefa_volume_performance_throughput_bytes Volume Byte Average FlashArray Cluster, Volume, Dimension, Yes volume Appliance throughput\npurefa_volume_space_datareduction_ratio Volume Count Average FlashArray Cluster, Volume, Appliance Yes volumes data reduction ratio\npurefa_hardware_temperature_celsius FlashArray Celcius Average FlashArray Cluster, Controller, Sensor, Yes hardware Appliance temperature sensors\npurefa_alerts_total FlashArray Count Average Number of Cluster, Severity Yes alert events\npurefa_array_performance_iops FlashArray Count Average FlashArray Cluster, Dimension, Appliance Yes IOPS\npurefa_array_performance_qdepth FlashArray Count Average FlashArray Cluster, Appliance Yes queue depth\npurefa_info FlashArray Labels NA FlashArray Cluster, Array Yes host volumes connections\npurefa_volume_performance_latency_usec Volume MicroSecond Average FlashArray Cluster, Volume, Dimension, Yes volume IO Appliance latency\npurefa_volume_space_bytes Volume Byte Average FlashArray Cluster, Volume, Dimension, Yes allocated Appliance space\npurefa_volume_performance_iops Volume Count Average FlashArray Cluster, Volume, Dimension, Yes volume Appliance IOPS\npurefa_volume_space_size_bytes Volume Byte Average FlashArray Cluster, Volume, Appliance Yes volumes size\npurefa_array_performance_latency_usec FlashArray MicroSecond Average FlashArray Cluster, Dimension, Appliance Yes latency\npurefa_array_space_used_bytes FlashArray Byte Average FlashArray Cluster, Dimension, Appliance Yes overall used space\nMetric Category Unit Aggregation Description Dimensions Exportable Type via   Diagnostic Settings?\npurefa_array_performance_bandwidth_bytes FlashArray Byte Average FlashArray Cluster, Dimension, Appliance Yes bandwidth\npurefa_array_performance_avg_block_bytes FlashArray Byte Average FlashArray Cluster, Dimension, Appliance Yes avg block size\npurefa_array_space_datareduction_ratio FlashArray Count Average FlashArray Cluster, Appliance Yes overall data reduction\npurefa_array_space_capacity_bytes FlashArray Byte Average FlashArray Cluster, Appliance Yes overall space capacity\npurefa_array_space_provisioned_bytes FlashArray Byte Average FlashArray Cluster, Appliance Yes overall provisioned space\npurefa_host_space_datareduction_ratio Host Count Average FlashArray Cluster, Node, Appliance Yes host volumes data reduction ratio\npurefa_host_space_size_bytes Host Byte Average FlashArray Cluster, Node, Appliance Yes host volumes size\npurefa_host_performance_latency_usec Host MicroSecond Average FlashArray Cluster, Node, Dimension, Yes host IO Appliance latency\npurefa_host_performance_bandwidth_bytes Host Byte Average FlashArray Cluster, Node, Dimension, Yes host Appliance bandwidth\npurefa_host_space_bytes Host Byte Average FlashArray Cluster, Node, Dimension, Yes host Appliance volumes allocated space\npurefa_host_performance_iops Host Count Average FlashArray Cluster, Node, Dimension, Yes host IOPS Appliance"}
{"text": "Network Fabric Metrics\nNetwork Devices Metrics\nMetric Name Metric Category Unit Aggregation Description Dimensions Exportable Display Type via   Name Diagnostic Settings?\nCpuUtilizationMax Cpu Resource % Average Maximum CPU CPU Cores No* Utilization Utilization utilization of the Max device over a given interval\nMetric Name Metric Category Unit Aggregation Description Dimensions Exportable Display Type via   Name Diagnostic Settings?\nCpuUtilizationMin Cpu Resource % Average Minimum CPU CPU Cores No* Utilization Utilization utilization of the Min device over a given interval\nFanSpeed Fan Speed Resource RPM Average Running speed of the Fan number No* Utilization fan at any given point of time\nMemoryAvailable Memory Resource GiB Average The amount of NA No* Available Utilization memory available or allocated to the device at a given point in time\nMemoryUtilized Memory Resource GiB Average The amount of NA No* Utilized Utilization memory utilized by the device at a given point in time\nPowerSupplyInputCurrent Power Resource Amps Average The input current draw NA No* Supply Utilization of the power supply Input Current\nPowerSupplyInputVoltage Power Resource Volts Average The input voltage of NA No* Supply Utilization the power supply Input Voltage\nPowerSupplyMaximumPowerCapacity Power Resource Watts Average Maximum power NA No* Supply Utilization capacity of the power Maximum supply Power Capacity\nPowerSupplyOutputCurrent Power Resource Amps Average The output current NA No* Supply Utilization supplied by the power Output supply Current\nPowerSupplyOutputPower Power Resource Watts Average The output power NA No* Supply Utilization supplied by the power Output supply Power\nPowerSupplyOutputVoltage Power Resource Volts Average The output voltage NA No* Supply Utilization supplied the power Output supply Voltage\nBgpPeerStatus BGP Peer BGP Status Count Average Operational state of NA No* Status the BGP Peer represented in numerical form. 1-Idle, 2-Connect, 3-Active, 4- OpenSent, 5- OpenConfirm, 6- Established\nMetric Name Metric Category Unit Aggregation Description Dimensions Exportable Display Type via   Name Diagnostic Settings?\nInterfaceOperStatus Interface Interface Count Average Operational state of NA No* Operational Operational the Interface State State represented in numerical form. 0-Up, 1-Down, 2-Lower Layer Down, 3-Testing, 4- Unknown, 5-Dormant, 6-Not Present\nIfEthInCrcErrors Ethernet Interface Count Average The count of incoming Interface No* Interface In State CRC errors caused by name CRC Errors Counters several factors for an ethernet interface over a given interval of time\nIfEthInFragmentFrames Ethernet Interface Count Average The count of incoming Interface No* Interface In State fragmented frames for name Fragment Counters an ethernet interface Frames over a given interval of time\nIfEthInJabberFrames Ethernet Interface Count Average The count of incoming Interface No* Interface In State jabber frames. Jabber name Jabber Counters frames are typically Frames oversized frames with invalid CRC\nIfEthInMacControlFrames Ethernet Interface Count Average The count of incoming Interface No* Interface In State MAC layer control name MAC Counters frames for an ethernet Control interface over a given Frames interval of time\nIfEthInMacPauseFrames Ethernet Interface Count Average The count of incoming Interface No* Interface In State MAC layer pause name MAC Pause Counters frames for an ethernet Frames interface over a given interval of time\nIfEthInOversizeFrames Ethernet Interface Count Average The count of incoming Interface No* Interface In State oversized frames name Oversize Counters (larger than 1518 Frames octets) for an ethernet interface over a given interval of time\nIfEthOutMacControlFrames Ethernet Interface Count Average The count of outgoing Interface No* Interface State MAC layer control name Out MAC Counters frames for an ethernet Control interface over a given Frames interval of time\nIfEthOutMacPauseFrames Ethernet Interface Count Average Shows the count of Interface No* Interface State outgoing MAC layer name Out MAC Counters pause frames for an Pause ethernet interface over Frames a given interval of time\nIfInBroadcastPkts Interface In Interface Count Average The count of incoming Interface No* Broadcast State broadcast packets for name Pkts Counters an interface over a given interval of time\nIfInDiscards Interface In Interface Count Average The count of incoming Interface No* Discards State discarded packets for name Counters an interface over a given interval of time\nMetric Name Metric Category Unit Aggregation Description Dimensions Exportable Display Type via   Name Diagnostic Settings?\nIfInErrors Interface In Interface Count Average The count of incoming Interface No* Errors State packets with errors for name Counters an interface over a given interval of time\nIfInFcsErrors Interface In Interface Count Average The count of incoming Interface No* FCS Errors State packets with FCS name Counters (Frame Check Sequence) errors for an interface over a given interval of time\nIfInMulticastPkts Interface In Interface Count Average The count of incoming Interface No* Multicast State multicast packets for name Pkts Counters an interface over a given interval of time\nIfInOctets Interface In Interface Count Average The total number of Interface No* Octets State incoming octets name Counters received by an interface over a given interval of time\nIfInUnicastPkts Interface In Interface Count Average The count of incoming Interface No* Unicast Pkts State unicast packets for an name Counters interface over a given interval of time\nIfInPkts Interface In Interface Count Average The total number of Interface No* Pkts State incoming packets name Counters received by an interface over a given interval of time. Includes all packets - unicast, multicast, broadcast, bad packets, etc.\nIfOutBroadcastPkts Interface Interface Count Average The count of outgoing Interface No* Out State broadcast packets for name Broadcast Counters an interface over a Pkts given interval of time\nIfOutDiscards Interface Interface Count Average The count of outgoing Interface No* Out State discarded packets for name Discards Counters an interface over a given interval of time\nIfOutErrors Interface Interface Count Average The count of outgoing Interface No* Out Errors State packets with errors for name Counters an interface over a given interval of time\nIfOutMulticastPkts Interface Interface Count Average The count of outgoing Interface No* Out State multicast packets for name Multicast Counters an interface over a Pkts given interval of time\nIfOutOctets Interface Interface Count Average The total number of Interface No* Out Octets State outgoing octets sent name Counters from an interface over a given interval of time\nIfOutUnicastPkts Interface Interface Count Average The count of outgoing Interface No* Out Unicast State unicast packets for an name Pkts Counters interface over a given interval of time"}
{"text": "Metric Name Metric Category Unit Aggregation Description Dimensions Exportable Display Type via   Name Diagnostic Settings?\nIfOutPkts Interface Interface Count Average The total number of Interface No* Out Pkts State outgoing packets sent name Counters from an interface over a given interval of time. Includes all packets - unicast, multicast, broadcast, bad packets, etc.\nLacpErrors LACP Errors LACP State Count Average The count of LACPDU Interface No* Counters illegal packet errors name\nLacpInPkts LACP In LACP State Count Average The count of LACPDU Interface No* Pkts Counters packets received by an name interface over a given interval of time\nLacpOutPkts LACP Out LACP State Count Average The count of LACPDU Interface No* Pkts Counters packets sent by an name interface over a given interval of time\nLacpRxErrors LACP Rx LACP State Count Average The count of LACPDU Interface No* Errors Counters packets with errors name received by an interface over a given interval of time\nLacpTxErrors LACP Tx LACP State Count Average The count of LACPDU Interface No* Errors Counters packets with errors name transmitted by an interface over a given interval of time\nLacpUnknownErrors LACP LACP State Count Average The count of LACPDU Interface No* Unknown Counters packets with unknown name Errors errors over a given interval of time\nLldpFrameIn LLDP Frame LLDP State Count Average The count of LLDP Interface No* In Counters frames received by an name interface over a given interval of time\nLldpFrameOut LLDP Frame LLDP State Count Average The count of LLDP Interface No* Out Counters frames trasmitted from name an interface over a given interval of time\nLldpTlvUnknown LLDP Tlv LLDP State Count Average The count of LLDP Interface No* Unknown Counters frames received with name unknown TLV by an interface over a given interval of time\n*Network Devices Metrics streaming via Diagnostic Setting is a work in progress and will be enabled in an upcoming release."}
{"text": "Near-edge Compute\nArticle  05/24/2023\nAzure Operator Nexus offers a group of on-premises cloud solutions. One of the on- premises offering allows Telco operators to run the Network functions in a Near-edge environment. In near-edge environment (also known as 'instance'), the compute servers, also referred to as bare metal machines (BMMs), represents the physical machines in the rack, runs the CBL-Mariner operating system, and provides support for running high- performance workloads."}
{"text": "SKUs available\nThe Nexus offering is today built with the following compute nodes for near-edge instances (the nodes that run the actual customer workloads).\nSKU Description\nDell R750 Compute node for Near Edge"}
{"text": "Compute connectivity\nThis diagram shows the connectivity model followed by computes in the near-edge instances:\nFigure: Operator Nexus Compute connectivity"}
{"text": "Compute configurations\nOperator Nexus supports a range of geometries and configurations. This table specifies the resources available per Compute.\nProperty Specification/Description Property Specification/Description\nNumber of 96 vCPUs hyper-threading enabled per compute server vCPUs for Tenant usage\nNumber of 2 - 48 vCPUs with even number of vCPUs only. No cross-NUMA VMs vCPU available for workloads\nCPU Default pinning\nRAM for 448 GB (224 GB per NUMA) running tenant workload\nHuge All VMs are backed by 1-GB huge pages pages for Tenant workloads\nDisk Up to 3.5 TB per compute host (Ephemeral) per Compute\nData plane SR-IOV traffic path for workloads\nNumber of Max 32 vNICs (30 VFs available for tenant workloads per NUMA) SR-IOV VFs\nSR-IOV NIC Enabled on all 100G NIC ports VMs with virtual functions (VF) assigned out of support Mellanox supported VF link aggregation (VF LAG). The allocated VFs are from the same physical NIC and within the same NUMA boundary. NIC ports providing VF LAG are connected to two different TOR switches for redundancy. Support for Trunked VFs RSS with Hardware Queuing. Supporting multi-queue support on VMs.\nIPv4/IPv6 Dual stack IPv4/IPv6, IPv4, and IPv6 only virtual machines Support"}
{"text": "Near-edge Nexus storage appliance\nArticle  07/03/2023\nThe architecture of Azure Operator Nexus revolves around core components such as compute servers, storage appliances, and network fabric devices. A single Storage Appliance referred to as the "Nexus Storage Appliance," is attached to each near-edge Nexus instance. These appliances play a vital role as the dedicated and persistent storage solution for the tenant workloads hosted within the Nexus instance.\nWithin each Nexus storage appliance, multiple storage devices are grouped together to form a unified storage pool. This pool is then divided into multiple volumes, which are then presented to the compute servers and tenant workloads as persistent volumes."}
{"text": "SKUs available\nThis table lists the SKUs available for the storage appliance in Near-edge Nexus offering:\nSKU Description\nPure x70r3-91 Storage appliance model x70r3-91 provided by PURE Storage"}
{"text": "Storage connectivity\nThis diagram shows the connectivity model followed by storage appliance in the Near Edge offering:"}
{"text": "Storage limits\nThis table lists the characteristics for the storage appliance:\nProperty Specification/Description\nRaw storage capacity 91 TB\nUsable capacity 50 TB\nNumber of maximum IO operations supported per second   250K+ (4K)   (with 80/20 R/W ratio) 150K+ (16K)\nNumber of IO operations supported per volume per second 50K+\nMaximum IO latency supported 10 ms\nNominal failover time supported 10 s"}
{"text": "Nexus Limits and Quotas\nArticle  06/30/2023\nThis document provides an overview of the resource limits that apply to the components used in the Nexus solution, encompassing the resources created within Azure cloud and in on-premises instance. It outlines the specific limitations and restrictions that operators should be aware of when deploying and managing the Nexus instance across these environments.\nUnderstanding these resource limits is crucial for optimizing resource utilization and ensuring the smooth operation of the Nexus solution. It's also essential to be aware of any restrictions or constraints that may apply to the creation of these resources to ensure compliance and avoid any potential issues or disruptions."}
{"text": "Azure-specific requirements\nIn addition to the hardware and software resources available in the customer's on- premises instance, the Azure Operator Nexus also incorporates essential components that must be created within the Azure cloud environment. These components, such as the Network Fabric Controller and Cluster Manager, are integral to the overall functionality and management of the Azure Nexus Operator on-premises instance. These controllers are built utilizing a diverse range of Azure services. Prior to creating these resources in the Azure cloud environment, Operators should thoroughly review and confirm the specific limits and quotas that are in effect. It's crucial to ensure compliance with these limitations to enable successful deployment and operation of the Azure Operator Nexus solution.\nSome of these Azure services have adjustable limits. When a service doesn't have adjustable limits, the default and the maximum limits are the same. The limit can be raised above the default limit but not above the maximum limit. If you want to raise the limit or quota above the default limit, open an online customer support request.\nThe terms soft limit and hard limit often are used informally to describe the current, adjustable limit (soft limit) and the maximum limit (hard limit).\nSome of these limits also apply at Azure region level.\n  Note\nIts highly recommended that Operators create and use a separate Azure subscription for Azure Nexus Operator and not mix it up with other Azure use cases\nunder the same subscription."}
{"text": "Network Fabric\nThe creation of the Network Fabric related resources is subject to the following resource limits:\nResource Notes Type\nNetwork Today, its creation depends on underlying Azure components as mentioned in Fabric the supporting table under section "Other Azure Resources" Controllers\nNetwork Up to 20 Network Fabric resources per Network Fabric Controller [To be Fabrics updated]\nNetwork Up to BOM-specified Network devices per Network Fabric Devices\nNetwork Up to BOM-specified racks per Network Fabric Racks\nLayer 2 3500 isolation domains per Nexus instance Isolation domains\nLayer 3 200 isolation domains per Nexus instance Isolation domains\nRoute policies 400 route policies per Nexus instance\n  Note\nThe number of Nexus instances a pair of NFC + CM can handle has been set\nto 20 based on some theoretical study for ExpressRoute. These numbers will\nbe refined after more testing.\nSome of these limits are yet to be introduced and are not applied by default\ntoday."}
{"text": "Network Cloud\nThe creation of the Network Cloud specific resources is subject to the following resource limits:\nResource Type Notes\nCluster Manager 1:1 mapping with Network Fabric Controller\nCluster Up to 20 Nexus Cluster instances per Cluster Manager (within same region) [To be updated]\nRacks Up to BOM-specified Compute Racks per Nexus Cluster\nBare Metal Up to BOM-specified BareMetal machines per Rack Machines\nStorage Appliances Up to BOM-specified Storage appliances per Nexus Cluster instance\nNAKS Cluster Depends on selection of VM flavor and number of nodes per NAKS cluster\nLayer 2 Networks 3500 per Nexus instance\nLayer 3 Networks 200 per Nexus instance\nTrunked Networks 3500 per Nexus instance\nCloud Service 100 per Nexus instance Networks\n  Note\nThe number of Nexus instances a pair of NFC + CM can handle has been set\nto 20 based on some theoretical study for ExpressRoute. These numbers will\nbe refined post GA after some further testing.\nSome of these limits are yet to be introduced and are not applied by default\ntoday."}
{"text": "Other Azure resources\nThere are several Azure resources that are required to build up Network Fabric Controllers and Cluster Manager. The table here outlines the Azure services that Operators must ensure that they have adequate capacity available for creation for each Network Fabric Controller and Cluster Manager pair.\nResource Type # of vCPUs\nResource Type # of vCPUs\nVirtual Machine 32 (D4_v2), 120 (DS4_v2), 4 (D2s_v3)\n  Note\nThe number of vCPUs and the family SKUs required are subject to change.\nThe table here briefly mentions other Azure resources that are necessary. However, by utilizing a dedicated subscription for Azure Operator Nexus, operators can alleviate concerns about their quotas.\nResource Notes Type\nSubscription Subscription limits\nResource Resource Group Limits . There's a max limit for RG per subscription. Operators Group need to make appropriate consideration for how they want to manage Resource Groups for NAKS clusters vs Virtual machines per Nexus instance.\nVM Flavors Customer generally has VM flavor quota in each region within subscription. You need to ensure that you can still create VMs per the requirements.\nAKS Clusters AKS Limits\nVirtual Virtual Network Limits Networks\nManaged Managed Identity Limits Identity\nExpressRoute ExpressRoute Limits\nVirtual Virtual Network Gateway Limits Network Gateway\nAzure Key Key Vault Limits Vault\nStorage Storage Account Limits Account\nLoad Load Balancer Limits Balancers (Standard)\nResource Notes Type\nPublic IP Public IP Address Limits Address (Standard)\nAzure Azure Monitor Limits Monitor Metrics\nLog Analytics Log Analytics Workspace Limits Workspace"}
